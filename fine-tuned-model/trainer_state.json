{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 21000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014285714285714286,
      "grad_norm": 1.4983924627304077,
      "learning_rate": 0.0001999047619047619,
      "loss": 4.1581,
      "step": 10
    },
    {
      "epoch": 0.002857142857142857,
      "grad_norm": 1.1323121786117554,
      "learning_rate": 0.00019980952380952382,
      "loss": 3.9781,
      "step": 20
    },
    {
      "epoch": 0.004285714285714286,
      "grad_norm": 1.146572232246399,
      "learning_rate": 0.00019971428571428573,
      "loss": 3.8255,
      "step": 30
    },
    {
      "epoch": 0.005714285714285714,
      "grad_norm": 1.3211936950683594,
      "learning_rate": 0.00019961904761904765,
      "loss": 3.7087,
      "step": 40
    },
    {
      "epoch": 0.007142857142857143,
      "grad_norm": 1.2884223461151123,
      "learning_rate": 0.00019952380952380954,
      "loss": 3.8109,
      "step": 50
    },
    {
      "epoch": 0.008571428571428572,
      "grad_norm": 1.6596465110778809,
      "learning_rate": 0.00019942857142857143,
      "loss": 3.6986,
      "step": 60
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.9994251132011414,
      "learning_rate": 0.00019933333333333334,
      "loss": 3.6787,
      "step": 70
    },
    {
      "epoch": 0.011428571428571429,
      "grad_norm": 1.8134251832962036,
      "learning_rate": 0.00019923809523809523,
      "loss": 3.6138,
      "step": 80
    },
    {
      "epoch": 0.012857142857142857,
      "grad_norm": 1.31058931350708,
      "learning_rate": 0.00019914285714285715,
      "loss": 3.6169,
      "step": 90
    },
    {
      "epoch": 0.014285714285714285,
      "grad_norm": 1.95804762840271,
      "learning_rate": 0.00019904761904761907,
      "loss": 3.5662,
      "step": 100
    },
    {
      "epoch": 0.015714285714285715,
      "grad_norm": 1.2248982191085815,
      "learning_rate": 0.00019895238095238096,
      "loss": 3.6774,
      "step": 110
    },
    {
      "epoch": 0.017142857142857144,
      "grad_norm": 1.3252774477005005,
      "learning_rate": 0.00019885714285714287,
      "loss": 3.5215,
      "step": 120
    },
    {
      "epoch": 0.018571428571428572,
      "grad_norm": 1.3503892421722412,
      "learning_rate": 0.00019876190476190476,
      "loss": 3.6169,
      "step": 130
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.532609224319458,
      "learning_rate": 0.00019866666666666668,
      "loss": 3.58,
      "step": 140
    },
    {
      "epoch": 0.02142857142857143,
      "grad_norm": 1.541034460067749,
      "learning_rate": 0.0001985714285714286,
      "loss": 3.4998,
      "step": 150
    },
    {
      "epoch": 0.022857142857142857,
      "grad_norm": 1.1523513793945312,
      "learning_rate": 0.00019847619047619049,
      "loss": 3.5053,
      "step": 160
    },
    {
      "epoch": 0.024285714285714285,
      "grad_norm": 1.3069900274276733,
      "learning_rate": 0.0001983809523809524,
      "loss": 3.5481,
      "step": 170
    },
    {
      "epoch": 0.025714285714285714,
      "grad_norm": 1.489034652709961,
      "learning_rate": 0.0001982857142857143,
      "loss": 3.4894,
      "step": 180
    },
    {
      "epoch": 0.027142857142857142,
      "grad_norm": 1.69954514503479,
      "learning_rate": 0.00019819047619047618,
      "loss": 3.5246,
      "step": 190
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 1.554611086845398,
      "learning_rate": 0.0001980952380952381,
      "loss": 3.5962,
      "step": 200
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.4112781286239624,
      "learning_rate": 0.00019800000000000002,
      "loss": 3.3871,
      "step": 210
    },
    {
      "epoch": 0.03142857142857143,
      "grad_norm": 2.2222847938537598,
      "learning_rate": 0.0001979047619047619,
      "loss": 3.5064,
      "step": 220
    },
    {
      "epoch": 0.032857142857142856,
      "grad_norm": 1.0278863906860352,
      "learning_rate": 0.00019780952380952382,
      "loss": 3.568,
      "step": 230
    },
    {
      "epoch": 0.03428571428571429,
      "grad_norm": 1.0930423736572266,
      "learning_rate": 0.0001977142857142857,
      "loss": 3.495,
      "step": 240
    },
    {
      "epoch": 0.03571428571428571,
      "grad_norm": 1.0608162879943848,
      "learning_rate": 0.00019761904761904763,
      "loss": 3.4808,
      "step": 250
    },
    {
      "epoch": 0.037142857142857144,
      "grad_norm": 1.3672434091567993,
      "learning_rate": 0.00019752380952380954,
      "loss": 3.4369,
      "step": 260
    },
    {
      "epoch": 0.03857142857142857,
      "grad_norm": 1.1568177938461304,
      "learning_rate": 0.00019742857142857143,
      "loss": 3.4749,
      "step": 270
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.4902044534683228,
      "learning_rate": 0.00019733333333333335,
      "loss": 3.4351,
      "step": 280
    },
    {
      "epoch": 0.041428571428571426,
      "grad_norm": 1.4004359245300293,
      "learning_rate": 0.00019723809523809524,
      "loss": 3.3067,
      "step": 290
    },
    {
      "epoch": 0.04285714285714286,
      "grad_norm": 1.363895297050476,
      "learning_rate": 0.00019714285714285716,
      "loss": 3.3317,
      "step": 300
    },
    {
      "epoch": 0.04428571428571428,
      "grad_norm": 1.1422730684280396,
      "learning_rate": 0.00019704761904761905,
      "loss": 3.4588,
      "step": 310
    },
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 1.7974470853805542,
      "learning_rate": 0.00019695238095238096,
      "loss": 3.4523,
      "step": 320
    },
    {
      "epoch": 0.047142857142857146,
      "grad_norm": 1.2400020360946655,
      "learning_rate": 0.00019685714285714288,
      "loss": 3.3402,
      "step": 330
    },
    {
      "epoch": 0.04857142857142857,
      "grad_norm": 2.1551120281219482,
      "learning_rate": 0.00019676190476190477,
      "loss": 3.3023,
      "step": 340
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.3566440343856812,
      "learning_rate": 0.00019666666666666666,
      "loss": 3.3389,
      "step": 350
    },
    {
      "epoch": 0.05142857142857143,
      "grad_norm": 1.413482666015625,
      "learning_rate": 0.00019657142857142858,
      "loss": 3.4127,
      "step": 360
    },
    {
      "epoch": 0.05285714285714286,
      "grad_norm": 1.010622501373291,
      "learning_rate": 0.0001964761904761905,
      "loss": 3.425,
      "step": 370
    },
    {
      "epoch": 0.054285714285714284,
      "grad_norm": 1.5565388202667236,
      "learning_rate": 0.0001963809523809524,
      "loss": 3.299,
      "step": 380
    },
    {
      "epoch": 0.055714285714285716,
      "grad_norm": 1.223969578742981,
      "learning_rate": 0.0001962857142857143,
      "loss": 3.4016,
      "step": 390
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 1.3910579681396484,
      "learning_rate": 0.0001961904761904762,
      "loss": 3.3806,
      "step": 400
    },
    {
      "epoch": 0.05857142857142857,
      "grad_norm": 1.1221145391464233,
      "learning_rate": 0.0001960952380952381,
      "loss": 3.3756,
      "step": 410
    },
    {
      "epoch": 0.06,
      "grad_norm": 1.4339227676391602,
      "learning_rate": 0.000196,
      "loss": 3.2872,
      "step": 420
    },
    {
      "epoch": 0.06142857142857143,
      "grad_norm": 1.1993731260299683,
      "learning_rate": 0.00019590476190476194,
      "loss": 3.3538,
      "step": 430
    },
    {
      "epoch": 0.06285714285714286,
      "grad_norm": 1.3705238103866577,
      "learning_rate": 0.00019580952380952383,
      "loss": 3.3895,
      "step": 440
    },
    {
      "epoch": 0.06428571428571428,
      "grad_norm": 1.5579220056533813,
      "learning_rate": 0.00019571428571428572,
      "loss": 3.4308,
      "step": 450
    },
    {
      "epoch": 0.06571428571428571,
      "grad_norm": 1.8243029117584229,
      "learning_rate": 0.00019561904761904763,
      "loss": 3.3167,
      "step": 460
    },
    {
      "epoch": 0.06714285714285714,
      "grad_norm": 1.5074695348739624,
      "learning_rate": 0.00019552380952380952,
      "loss": 3.4094,
      "step": 470
    },
    {
      "epoch": 0.06857142857142857,
      "grad_norm": 1.3229783773422241,
      "learning_rate": 0.00019542857142857144,
      "loss": 3.4116,
      "step": 480
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.474564552307129,
      "learning_rate": 0.00019533333333333336,
      "loss": 3.4214,
      "step": 490
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 1.1595431566238403,
      "learning_rate": 0.00019523809523809525,
      "loss": 3.4078,
      "step": 500
    },
    {
      "epoch": 0.07285714285714286,
      "grad_norm": 1.4624974727630615,
      "learning_rate": 0.00019514285714285716,
      "loss": 3.3659,
      "step": 510
    },
    {
      "epoch": 0.07428571428571429,
      "grad_norm": 1.103583574295044,
      "learning_rate": 0.00019504761904761905,
      "loss": 3.1552,
      "step": 520
    },
    {
      "epoch": 0.07571428571428572,
      "grad_norm": 1.7005172967910767,
      "learning_rate": 0.00019495238095238094,
      "loss": 3.2683,
      "step": 530
    },
    {
      "epoch": 0.07714285714285714,
      "grad_norm": 1.3497061729431152,
      "learning_rate": 0.00019485714285714286,
      "loss": 3.3483,
      "step": 540
    },
    {
      "epoch": 0.07857142857142857,
      "grad_norm": 1.5451678037643433,
      "learning_rate": 0.00019476190476190477,
      "loss": 3.207,
      "step": 550
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.0823583602905273,
      "learning_rate": 0.0001946666666666667,
      "loss": 3.3523,
      "step": 560
    },
    {
      "epoch": 0.08142857142857143,
      "grad_norm": 1.2645511627197266,
      "learning_rate": 0.00019457142857142858,
      "loss": 3.3669,
      "step": 570
    },
    {
      "epoch": 0.08285714285714285,
      "grad_norm": 1.6077104806900024,
      "learning_rate": 0.00019447619047619047,
      "loss": 3.1757,
      "step": 580
    },
    {
      "epoch": 0.08428571428571428,
      "grad_norm": 1.415358543395996,
      "learning_rate": 0.0001943809523809524,
      "loss": 3.4391,
      "step": 590
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 1.3452039957046509,
      "learning_rate": 0.0001942857142857143,
      "loss": 3.2605,
      "step": 600
    },
    {
      "epoch": 0.08714285714285715,
      "grad_norm": 1.3196837902069092,
      "learning_rate": 0.0001941904761904762,
      "loss": 3.2907,
      "step": 610
    },
    {
      "epoch": 0.08857142857142856,
      "grad_norm": 1.3777798414230347,
      "learning_rate": 0.0001940952380952381,
      "loss": 3.3278,
      "step": 620
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.470229983329773,
      "learning_rate": 0.000194,
      "loss": 3.2548,
      "step": 630
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 1.4538902044296265,
      "learning_rate": 0.00019390476190476192,
      "loss": 3.2599,
      "step": 640
    },
    {
      "epoch": 0.09285714285714286,
      "grad_norm": 0.9813604950904846,
      "learning_rate": 0.0001938095238095238,
      "loss": 3.2983,
      "step": 650
    },
    {
      "epoch": 0.09428571428571429,
      "grad_norm": 1.2359857559204102,
      "learning_rate": 0.00019371428571428572,
      "loss": 3.2537,
      "step": 660
    },
    {
      "epoch": 0.09571428571428571,
      "grad_norm": 1.3248554468154907,
      "learning_rate": 0.00019361904761904764,
      "loss": 3.3297,
      "step": 670
    },
    {
      "epoch": 0.09714285714285714,
      "grad_norm": 1.1249363422393799,
      "learning_rate": 0.00019352380952380953,
      "loss": 3.1376,
      "step": 680
    },
    {
      "epoch": 0.09857142857142857,
      "grad_norm": 1.2844535112380981,
      "learning_rate": 0.00019342857142857144,
      "loss": 3.1623,
      "step": 690
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.1831355094909668,
      "learning_rate": 0.00019333333333333333,
      "loss": 3.2761,
      "step": 700
    },
    {
      "epoch": 0.10142857142857142,
      "grad_norm": 1.7732338905334473,
      "learning_rate": 0.00019323809523809525,
      "loss": 3.3048,
      "step": 710
    },
    {
      "epoch": 0.10285714285714286,
      "grad_norm": 1.1166273355484009,
      "learning_rate": 0.00019314285714285717,
      "loss": 3.2384,
      "step": 720
    },
    {
      "epoch": 0.10428571428571429,
      "grad_norm": 1.5987058877944946,
      "learning_rate": 0.00019304761904761906,
      "loss": 3.1977,
      "step": 730
    },
    {
      "epoch": 0.10571428571428572,
      "grad_norm": 2.048149585723877,
      "learning_rate": 0.00019295238095238095,
      "loss": 3.1661,
      "step": 740
    },
    {
      "epoch": 0.10714285714285714,
      "grad_norm": 1.5551847219467163,
      "learning_rate": 0.00019285714285714286,
      "loss": 3.237,
      "step": 750
    },
    {
      "epoch": 0.10857142857142857,
      "grad_norm": 1.1541688442230225,
      "learning_rate": 0.00019276190476190475,
      "loss": 3.2009,
      "step": 760
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.0736353397369385,
      "learning_rate": 0.0001926666666666667,
      "loss": 3.2189,
      "step": 770
    },
    {
      "epoch": 0.11142857142857143,
      "grad_norm": 1.0044302940368652,
      "learning_rate": 0.00019257142857142859,
      "loss": 3.1932,
      "step": 780
    },
    {
      "epoch": 0.11285714285714285,
      "grad_norm": 1.133523941040039,
      "learning_rate": 0.00019247619047619048,
      "loss": 3.1864,
      "step": 790
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 1.519695520401001,
      "learning_rate": 0.0001923809523809524,
      "loss": 3.1523,
      "step": 800
    },
    {
      "epoch": 0.11571428571428571,
      "grad_norm": 1.552282691001892,
      "learning_rate": 0.00019228571428571428,
      "loss": 3.2171,
      "step": 810
    },
    {
      "epoch": 0.11714285714285715,
      "grad_norm": 0.9780957698822021,
      "learning_rate": 0.0001921904761904762,
      "loss": 3.3103,
      "step": 820
    },
    {
      "epoch": 0.11857142857142858,
      "grad_norm": 1.5024980306625366,
      "learning_rate": 0.00019209523809523812,
      "loss": 3.2097,
      "step": 830
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.2891360521316528,
      "learning_rate": 0.000192,
      "loss": 3.2012,
      "step": 840
    },
    {
      "epoch": 0.12142857142857143,
      "grad_norm": 1.2248481512069702,
      "learning_rate": 0.00019190476190476192,
      "loss": 3.3707,
      "step": 850
    },
    {
      "epoch": 0.12285714285714286,
      "grad_norm": 1.002319097518921,
      "learning_rate": 0.0001918095238095238,
      "loss": 3.1188,
      "step": 860
    },
    {
      "epoch": 0.12428571428571429,
      "grad_norm": 1.0450245141983032,
      "learning_rate": 0.0001917142857142857,
      "loss": 3.3537,
      "step": 870
    },
    {
      "epoch": 0.12571428571428572,
      "grad_norm": 1.2574512958526611,
      "learning_rate": 0.00019161904761904764,
      "loss": 3.0648,
      "step": 880
    },
    {
      "epoch": 0.12714285714285714,
      "grad_norm": 1.1716904640197754,
      "learning_rate": 0.00019152380952380953,
      "loss": 3.2042,
      "step": 890
    },
    {
      "epoch": 0.12857142857142856,
      "grad_norm": 1.5111244916915894,
      "learning_rate": 0.00019142857142857145,
      "loss": 3.1473,
      "step": 900
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.9545099139213562,
      "learning_rate": 0.00019133333333333334,
      "loss": 3.2466,
      "step": 910
    },
    {
      "epoch": 0.13142857142857142,
      "grad_norm": 1.3831076622009277,
      "learning_rate": 0.00019123809523809523,
      "loss": 3.2126,
      "step": 920
    },
    {
      "epoch": 0.13285714285714287,
      "grad_norm": 1.1714491844177246,
      "learning_rate": 0.00019114285714285715,
      "loss": 3.1372,
      "step": 930
    },
    {
      "epoch": 0.13428571428571429,
      "grad_norm": 1.2371069192886353,
      "learning_rate": 0.00019104761904761906,
      "loss": 3.2103,
      "step": 940
    },
    {
      "epoch": 0.1357142857142857,
      "grad_norm": 1.3432955741882324,
      "learning_rate": 0.00019095238095238098,
      "loss": 3.1385,
      "step": 950
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 1.1104472875595093,
      "learning_rate": 0.00019085714285714287,
      "loss": 3.2643,
      "step": 960
    },
    {
      "epoch": 0.13857142857142857,
      "grad_norm": 1.5865600109100342,
      "learning_rate": 0.00019076190476190476,
      "loss": 3.1592,
      "step": 970
    },
    {
      "epoch": 0.14,
      "grad_norm": 1.3930187225341797,
      "learning_rate": 0.00019066666666666668,
      "loss": 3.1799,
      "step": 980
    },
    {
      "epoch": 0.14142857142857143,
      "grad_norm": 1.0562382936477661,
      "learning_rate": 0.0001905714285714286,
      "loss": 3.2886,
      "step": 990
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 2.343902349472046,
      "learning_rate": 0.00019047619047619048,
      "loss": 3.1522,
      "step": 1000
    },
    {
      "epoch": 0.1442857142857143,
      "grad_norm": 1.155512809753418,
      "learning_rate": 0.0001903809523809524,
      "loss": 3.0824,
      "step": 1010
    },
    {
      "epoch": 0.1457142857142857,
      "grad_norm": 1.2747867107391357,
      "learning_rate": 0.0001902857142857143,
      "loss": 3.303,
      "step": 1020
    },
    {
      "epoch": 0.14714285714285713,
      "grad_norm": 1.3526676893234253,
      "learning_rate": 0.0001901904761904762,
      "loss": 3.3015,
      "step": 1030
    },
    {
      "epoch": 0.14857142857142858,
      "grad_norm": 1.447543740272522,
      "learning_rate": 0.0001900952380952381,
      "loss": 3.2735,
      "step": 1040
    },
    {
      "epoch": 0.15,
      "grad_norm": 1.3019297122955322,
      "learning_rate": 0.00019,
      "loss": 3.1935,
      "step": 1050
    },
    {
      "epoch": 0.15142857142857144,
      "grad_norm": 1.4917845726013184,
      "learning_rate": 0.00018990476190476193,
      "loss": 3.1793,
      "step": 1060
    },
    {
      "epoch": 0.15285714285714286,
      "grad_norm": 1.093451976776123,
      "learning_rate": 0.00018980952380952382,
      "loss": 3.151,
      "step": 1070
    },
    {
      "epoch": 0.15428571428571428,
      "grad_norm": 1.2798054218292236,
      "learning_rate": 0.00018971428571428573,
      "loss": 3.2404,
      "step": 1080
    },
    {
      "epoch": 0.15571428571428572,
      "grad_norm": 1.4167590141296387,
      "learning_rate": 0.00018961904761904762,
      "loss": 3.1299,
      "step": 1090
    },
    {
      "epoch": 0.15714285714285714,
      "grad_norm": 1.2601255178451538,
      "learning_rate": 0.0001895238095238095,
      "loss": 3.0885,
      "step": 1100
    },
    {
      "epoch": 0.15857142857142856,
      "grad_norm": 1.5465375185012817,
      "learning_rate": 0.00018942857142857146,
      "loss": 3.2178,
      "step": 1110
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.158531904220581,
      "learning_rate": 0.00018933333333333335,
      "loss": 3.2466,
      "step": 1120
    },
    {
      "epoch": 0.16142857142857142,
      "grad_norm": 1.554938793182373,
      "learning_rate": 0.00018923809523809524,
      "loss": 3.067,
      "step": 1130
    },
    {
      "epoch": 0.16285714285714287,
      "grad_norm": 1.5052692890167236,
      "learning_rate": 0.00018914285714285715,
      "loss": 3.2719,
      "step": 1140
    },
    {
      "epoch": 0.16428571428571428,
      "grad_norm": 1.391133189201355,
      "learning_rate": 0.00018904761904761904,
      "loss": 3.225,
      "step": 1150
    },
    {
      "epoch": 0.1657142857142857,
      "grad_norm": 1.3222686052322388,
      "learning_rate": 0.00018895238095238096,
      "loss": 3.3034,
      "step": 1160
    },
    {
      "epoch": 0.16714285714285715,
      "grad_norm": 1.158549189567566,
      "learning_rate": 0.00018885714285714287,
      "loss": 3.0695,
      "step": 1170
    },
    {
      "epoch": 0.16857142857142857,
      "grad_norm": 1.1540676355361938,
      "learning_rate": 0.00018876190476190476,
      "loss": 3.1167,
      "step": 1180
    },
    {
      "epoch": 0.17,
      "grad_norm": 1.9009332656860352,
      "learning_rate": 0.00018866666666666668,
      "loss": 3.0986,
      "step": 1190
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 1.0508688688278198,
      "learning_rate": 0.00018857142857142857,
      "loss": 3.0836,
      "step": 1200
    },
    {
      "epoch": 0.17285714285714285,
      "grad_norm": 1.008833885192871,
      "learning_rate": 0.0001884761904761905,
      "loss": 3.1283,
      "step": 1210
    },
    {
      "epoch": 0.1742857142857143,
      "grad_norm": 1.189058780670166,
      "learning_rate": 0.0001883809523809524,
      "loss": 3.1023,
      "step": 1220
    },
    {
      "epoch": 0.1757142857142857,
      "grad_norm": 1.0695596933364868,
      "learning_rate": 0.0001882857142857143,
      "loss": 3.1282,
      "step": 1230
    },
    {
      "epoch": 0.17714285714285713,
      "grad_norm": 1.828204870223999,
      "learning_rate": 0.0001881904761904762,
      "loss": 3.2538,
      "step": 1240
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 2.383120059967041,
      "learning_rate": 0.0001880952380952381,
      "loss": 3.1813,
      "step": 1250
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.3480477333068848,
      "learning_rate": 0.000188,
      "loss": 3.1495,
      "step": 1260
    },
    {
      "epoch": 0.18142857142857144,
      "grad_norm": 1.1948038339614868,
      "learning_rate": 0.0001879047619047619,
      "loss": 3.1345,
      "step": 1270
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 1.1674816608428955,
      "learning_rate": 0.00018780952380952382,
      "loss": 3.2573,
      "step": 1280
    },
    {
      "epoch": 0.18428571428571427,
      "grad_norm": 1.5511754751205444,
      "learning_rate": 0.00018771428571428574,
      "loss": 3.0965,
      "step": 1290
    },
    {
      "epoch": 0.18571428571428572,
      "grad_norm": 1.050208330154419,
      "learning_rate": 0.00018761904761904763,
      "loss": 3.1806,
      "step": 1300
    },
    {
      "epoch": 0.18714285714285714,
      "grad_norm": 1.1817762851715088,
      "learning_rate": 0.00018752380952380952,
      "loss": 2.9909,
      "step": 1310
    },
    {
      "epoch": 0.18857142857142858,
      "grad_norm": 1.4312666654586792,
      "learning_rate": 0.00018742857142857143,
      "loss": 3.026,
      "step": 1320
    },
    {
      "epoch": 0.19,
      "grad_norm": 1.1751632690429688,
      "learning_rate": 0.00018733333333333335,
      "loss": 3.1635,
      "step": 1330
    },
    {
      "epoch": 0.19142857142857142,
      "grad_norm": 1.2608121633529663,
      "learning_rate": 0.00018723809523809527,
      "loss": 3.1281,
      "step": 1340
    },
    {
      "epoch": 0.19285714285714287,
      "grad_norm": 1.0813219547271729,
      "learning_rate": 0.00018714285714285716,
      "loss": 3.0412,
      "step": 1350
    },
    {
      "epoch": 0.19428571428571428,
      "grad_norm": 1.1632554531097412,
      "learning_rate": 0.00018704761904761905,
      "loss": 3.1252,
      "step": 1360
    },
    {
      "epoch": 0.1957142857142857,
      "grad_norm": 1.0032213926315308,
      "learning_rate": 0.00018695238095238096,
      "loss": 3.1203,
      "step": 1370
    },
    {
      "epoch": 0.19714285714285715,
      "grad_norm": 1.1790131330490112,
      "learning_rate": 0.00018685714285714285,
      "loss": 3.139,
      "step": 1380
    },
    {
      "epoch": 0.19857142857142857,
      "grad_norm": 1.3390066623687744,
      "learning_rate": 0.00018676190476190477,
      "loss": 3.0548,
      "step": 1390
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.1988083124160767,
      "learning_rate": 0.0001866666666666667,
      "loss": 3.2009,
      "step": 1400
    },
    {
      "epoch": 0.20142857142857143,
      "grad_norm": 1.114493489265442,
      "learning_rate": 0.00018657142857142858,
      "loss": 3.1185,
      "step": 1410
    },
    {
      "epoch": 0.20285714285714285,
      "grad_norm": 1.3900164365768433,
      "learning_rate": 0.0001864761904761905,
      "loss": 3.1679,
      "step": 1420
    },
    {
      "epoch": 0.2042857142857143,
      "grad_norm": 1.0828230381011963,
      "learning_rate": 0.00018638095238095238,
      "loss": 3.135,
      "step": 1430
    },
    {
      "epoch": 0.2057142857142857,
      "grad_norm": 1.3273791074752808,
      "learning_rate": 0.0001862857142857143,
      "loss": 3.0138,
      "step": 1440
    },
    {
      "epoch": 0.20714285714285716,
      "grad_norm": 1.0957329273223877,
      "learning_rate": 0.00018619047619047622,
      "loss": 3.1979,
      "step": 1450
    },
    {
      "epoch": 0.20857142857142857,
      "grad_norm": 1.2952967882156372,
      "learning_rate": 0.0001860952380952381,
      "loss": 3.0998,
      "step": 1460
    },
    {
      "epoch": 0.21,
      "grad_norm": 1.1378709077835083,
      "learning_rate": 0.00018600000000000002,
      "loss": 3.1032,
      "step": 1470
    },
    {
      "epoch": 0.21142857142857144,
      "grad_norm": 1.1047163009643555,
      "learning_rate": 0.0001859047619047619,
      "loss": 3.0695,
      "step": 1480
    },
    {
      "epoch": 0.21285714285714286,
      "grad_norm": 1.267943263053894,
      "learning_rate": 0.0001858095238095238,
      "loss": 3.1667,
      "step": 1490
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 1.1695410013198853,
      "learning_rate": 0.00018571428571428572,
      "loss": 3.0501,
      "step": 1500
    },
    {
      "epoch": 0.21571428571428572,
      "grad_norm": 1.2503228187561035,
      "learning_rate": 0.00018561904761904763,
      "loss": 3.133,
      "step": 1510
    },
    {
      "epoch": 0.21714285714285714,
      "grad_norm": 1.1366380453109741,
      "learning_rate": 0.00018552380952380952,
      "loss": 2.9539,
      "step": 1520
    },
    {
      "epoch": 0.21857142857142858,
      "grad_norm": 1.2323781251907349,
      "learning_rate": 0.00018542857142857144,
      "loss": 3.1242,
      "step": 1530
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.3281798362731934,
      "learning_rate": 0.00018533333333333333,
      "loss": 3.0751,
      "step": 1540
    },
    {
      "epoch": 0.22142857142857142,
      "grad_norm": 1.0881938934326172,
      "learning_rate": 0.00018523809523809525,
      "loss": 3.1845,
      "step": 1550
    },
    {
      "epoch": 0.22285714285714286,
      "grad_norm": 1.3579849004745483,
      "learning_rate": 0.00018514285714285716,
      "loss": 3.1815,
      "step": 1560
    },
    {
      "epoch": 0.22428571428571428,
      "grad_norm": 1.347367286682129,
      "learning_rate": 0.00018504761904761905,
      "loss": 3.1384,
      "step": 1570
    },
    {
      "epoch": 0.2257142857142857,
      "grad_norm": 1.1990578174591064,
      "learning_rate": 0.00018495238095238097,
      "loss": 3.1524,
      "step": 1580
    },
    {
      "epoch": 0.22714285714285715,
      "grad_norm": 1.443678617477417,
      "learning_rate": 0.00018485714285714286,
      "loss": 3.0685,
      "step": 1590
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 1.1710563898086548,
      "learning_rate": 0.00018476190476190478,
      "loss": 3.0463,
      "step": 1600
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.380922555923462,
      "learning_rate": 0.00018466666666666666,
      "loss": 3.0516,
      "step": 1610
    },
    {
      "epoch": 0.23142857142857143,
      "grad_norm": 1.7145493030548096,
      "learning_rate": 0.00018457142857142858,
      "loss": 3.0828,
      "step": 1620
    },
    {
      "epoch": 0.23285714285714285,
      "grad_norm": 1.1049519777297974,
      "learning_rate": 0.0001844761904761905,
      "loss": 3.019,
      "step": 1630
    },
    {
      "epoch": 0.2342857142857143,
      "grad_norm": 1.0851627588272095,
      "learning_rate": 0.0001843809523809524,
      "loss": 3.1136,
      "step": 1640
    },
    {
      "epoch": 0.2357142857142857,
      "grad_norm": 1.1233574151992798,
      "learning_rate": 0.00018428571428571428,
      "loss": 3.076,
      "step": 1650
    },
    {
      "epoch": 0.23714285714285716,
      "grad_norm": 1.0808911323547363,
      "learning_rate": 0.0001841904761904762,
      "loss": 2.9576,
      "step": 1660
    },
    {
      "epoch": 0.23857142857142857,
      "grad_norm": 1.2707968950271606,
      "learning_rate": 0.0001840952380952381,
      "loss": 3.0138,
      "step": 1670
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.1214516162872314,
      "learning_rate": 0.00018400000000000003,
      "loss": 3.095,
      "step": 1680
    },
    {
      "epoch": 0.24142857142857144,
      "grad_norm": 1.0446892976760864,
      "learning_rate": 0.00018390476190476192,
      "loss": 3.0695,
      "step": 1690
    },
    {
      "epoch": 0.24285714285714285,
      "grad_norm": 1.121018409729004,
      "learning_rate": 0.0001838095238095238,
      "loss": 3.0352,
      "step": 1700
    },
    {
      "epoch": 0.24428571428571427,
      "grad_norm": 1.0666254758834839,
      "learning_rate": 0.00018371428571428572,
      "loss": 3.2274,
      "step": 1710
    },
    {
      "epoch": 0.24571428571428572,
      "grad_norm": 1.6246395111083984,
      "learning_rate": 0.0001836190476190476,
      "loss": 2.9421,
      "step": 1720
    },
    {
      "epoch": 0.24714285714285714,
      "grad_norm": 1.1325023174285889,
      "learning_rate": 0.00018352380952380953,
      "loss": 3.1683,
      "step": 1730
    },
    {
      "epoch": 0.24857142857142858,
      "grad_norm": 1.2164806127548218,
      "learning_rate": 0.00018342857142857145,
      "loss": 3.1303,
      "step": 1740
    },
    {
      "epoch": 0.25,
      "grad_norm": 1.3588463068008423,
      "learning_rate": 0.00018333333333333334,
      "loss": 2.9755,
      "step": 1750
    },
    {
      "epoch": 0.25142857142857145,
      "grad_norm": 1.3233087062835693,
      "learning_rate": 0.00018323809523809525,
      "loss": 3.0382,
      "step": 1760
    },
    {
      "epoch": 0.25285714285714284,
      "grad_norm": 1.7100882530212402,
      "learning_rate": 0.00018314285714285714,
      "loss": 3.1137,
      "step": 1770
    },
    {
      "epoch": 0.2542857142857143,
      "grad_norm": 1.4868272542953491,
      "learning_rate": 0.00018304761904761906,
      "loss": 3.1186,
      "step": 1780
    },
    {
      "epoch": 0.2557142857142857,
      "grad_norm": 1.8618556261062622,
      "learning_rate": 0.00018295238095238097,
      "loss": 3.1034,
      "step": 1790
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 1.2372716665267944,
      "learning_rate": 0.00018285714285714286,
      "loss": 2.9764,
      "step": 1800
    },
    {
      "epoch": 0.25857142857142856,
      "grad_norm": 1.5019500255584717,
      "learning_rate": 0.00018276190476190478,
      "loss": 3.0241,
      "step": 1810
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.6583894491195679,
      "learning_rate": 0.00018266666666666667,
      "loss": 3.0017,
      "step": 1820
    },
    {
      "epoch": 0.26142857142857145,
      "grad_norm": 1.0217993259429932,
      "learning_rate": 0.00018257142857142856,
      "loss": 3.0707,
      "step": 1830
    },
    {
      "epoch": 0.26285714285714284,
      "grad_norm": 1.1508790254592896,
      "learning_rate": 0.00018247619047619048,
      "loss": 3.1078,
      "step": 1840
    },
    {
      "epoch": 0.2642857142857143,
      "grad_norm": 1.3867459297180176,
      "learning_rate": 0.0001823809523809524,
      "loss": 3.0356,
      "step": 1850
    },
    {
      "epoch": 0.26571428571428574,
      "grad_norm": 1.2177212238311768,
      "learning_rate": 0.0001822857142857143,
      "loss": 3.0298,
      "step": 1860
    },
    {
      "epoch": 0.2671428571428571,
      "grad_norm": 1.2057162523269653,
      "learning_rate": 0.0001821904761904762,
      "loss": 3.1428,
      "step": 1870
    },
    {
      "epoch": 0.26857142857142857,
      "grad_norm": 1.2564150094985962,
      "learning_rate": 0.0001820952380952381,
      "loss": 3.1699,
      "step": 1880
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.8056625127792358,
      "learning_rate": 0.000182,
      "loss": 3.0083,
      "step": 1890
    },
    {
      "epoch": 0.2714285714285714,
      "grad_norm": 1.2490108013153076,
      "learning_rate": 0.00018190476190476192,
      "loss": 3.0358,
      "step": 1900
    },
    {
      "epoch": 0.27285714285714285,
      "grad_norm": 1.1419408321380615,
      "learning_rate": 0.0001818095238095238,
      "loss": 2.9606,
      "step": 1910
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 1.3367540836334229,
      "learning_rate": 0.00018171428571428573,
      "loss": 3.0711,
      "step": 1920
    },
    {
      "epoch": 0.2757142857142857,
      "grad_norm": 1.1209535598754883,
      "learning_rate": 0.00018161904761904762,
      "loss": 3.1435,
      "step": 1930
    },
    {
      "epoch": 0.27714285714285714,
      "grad_norm": 1.235158085823059,
      "learning_rate": 0.00018152380952380953,
      "loss": 3.1214,
      "step": 1940
    },
    {
      "epoch": 0.2785714285714286,
      "grad_norm": 1.2444795370101929,
      "learning_rate": 0.00018142857142857142,
      "loss": 3.1016,
      "step": 1950
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.5163850784301758,
      "learning_rate": 0.00018133333333333334,
      "loss": 3.0159,
      "step": 1960
    },
    {
      "epoch": 0.2814285714285714,
      "grad_norm": 0.9961878657341003,
      "learning_rate": 0.00018123809523809526,
      "loss": 3.0373,
      "step": 1970
    },
    {
      "epoch": 0.28285714285714286,
      "grad_norm": 1.5406900644302368,
      "learning_rate": 0.00018114285714285715,
      "loss": 3.0173,
      "step": 1980
    },
    {
      "epoch": 0.2842857142857143,
      "grad_norm": 1.1292009353637695,
      "learning_rate": 0.00018104761904761906,
      "loss": 3.0375,
      "step": 1990
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 1.2309638261795044,
      "learning_rate": 0.00018095238095238095,
      "loss": 3.1977,
      "step": 2000
    },
    {
      "epoch": 0.28714285714285714,
      "grad_norm": 1.168584942817688,
      "learning_rate": 0.00018085714285714287,
      "loss": 3.1297,
      "step": 2010
    },
    {
      "epoch": 0.2885714285714286,
      "grad_norm": 1.2704136371612549,
      "learning_rate": 0.0001807619047619048,
      "loss": 3.0006,
      "step": 2020
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.0876812934875488,
      "learning_rate": 0.00018066666666666668,
      "loss": 3.0122,
      "step": 2030
    },
    {
      "epoch": 0.2914285714285714,
      "grad_norm": 1.141663670539856,
      "learning_rate": 0.00018057142857142857,
      "loss": 2.994,
      "step": 2040
    },
    {
      "epoch": 0.29285714285714287,
      "grad_norm": 1.4504716396331787,
      "learning_rate": 0.00018047619047619048,
      "loss": 3.1017,
      "step": 2050
    },
    {
      "epoch": 0.29428571428571426,
      "grad_norm": 1.2454519271850586,
      "learning_rate": 0.00018038095238095237,
      "loss": 3.0294,
      "step": 2060
    },
    {
      "epoch": 0.2957142857142857,
      "grad_norm": 1.807964563369751,
      "learning_rate": 0.00018028571428571432,
      "loss": 3.0018,
      "step": 2070
    },
    {
      "epoch": 0.29714285714285715,
      "grad_norm": 1.3349757194519043,
      "learning_rate": 0.0001801904761904762,
      "loss": 2.9923,
      "step": 2080
    },
    {
      "epoch": 0.2985714285714286,
      "grad_norm": 1.193081021308899,
      "learning_rate": 0.0001800952380952381,
      "loss": 3.1036,
      "step": 2090
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.8472270965576172,
      "learning_rate": 0.00018,
      "loss": 3.0156,
      "step": 2100
    },
    {
      "epoch": 0.30142857142857143,
      "grad_norm": 1.11702299118042,
      "learning_rate": 0.0001799047619047619,
      "loss": 3.1094,
      "step": 2110
    },
    {
      "epoch": 0.3028571428571429,
      "grad_norm": 1.1406586170196533,
      "learning_rate": 0.00017980952380952382,
      "loss": 2.9476,
      "step": 2120
    },
    {
      "epoch": 0.30428571428571427,
      "grad_norm": 1.103182315826416,
      "learning_rate": 0.00017971428571428573,
      "loss": 2.9709,
      "step": 2130
    },
    {
      "epoch": 0.3057142857142857,
      "grad_norm": 1.1178141832351685,
      "learning_rate": 0.00017961904761904762,
      "loss": 2.9227,
      "step": 2140
    },
    {
      "epoch": 0.30714285714285716,
      "grad_norm": 1.056574821472168,
      "learning_rate": 0.00017952380952380954,
      "loss": 3.0102,
      "step": 2150
    },
    {
      "epoch": 0.30857142857142855,
      "grad_norm": 1.313873529434204,
      "learning_rate": 0.00017942857142857143,
      "loss": 3.0737,
      "step": 2160
    },
    {
      "epoch": 0.31,
      "grad_norm": 1.2720390558242798,
      "learning_rate": 0.00017933333333333332,
      "loss": 2.9957,
      "step": 2170
    },
    {
      "epoch": 0.31142857142857144,
      "grad_norm": 1.009245753288269,
      "learning_rate": 0.00017923809523809524,
      "loss": 3.1111,
      "step": 2180
    },
    {
      "epoch": 0.31285714285714283,
      "grad_norm": 1.267271637916565,
      "learning_rate": 0.00017914285714285715,
      "loss": 2.9913,
      "step": 2190
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 1.895487904548645,
      "learning_rate": 0.00017904761904761907,
      "loss": 3.054,
      "step": 2200
    },
    {
      "epoch": 0.3157142857142857,
      "grad_norm": 1.0943766832351685,
      "learning_rate": 0.00017895238095238096,
      "loss": 2.8846,
      "step": 2210
    },
    {
      "epoch": 0.3171428571428571,
      "grad_norm": 1.1027823686599731,
      "learning_rate": 0.00017885714285714285,
      "loss": 3.1212,
      "step": 2220
    },
    {
      "epoch": 0.31857142857142856,
      "grad_norm": 1.327278971672058,
      "learning_rate": 0.00017876190476190477,
      "loss": 3.0881,
      "step": 2230
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.0582756996154785,
      "learning_rate": 0.00017866666666666668,
      "loss": 3.0558,
      "step": 2240
    },
    {
      "epoch": 0.32142857142857145,
      "grad_norm": 1.645138144493103,
      "learning_rate": 0.0001785714285714286,
      "loss": 2.945,
      "step": 2250
    },
    {
      "epoch": 0.32285714285714284,
      "grad_norm": 1.6151676177978516,
      "learning_rate": 0.0001784761904761905,
      "loss": 2.9793,
      "step": 2260
    },
    {
      "epoch": 0.3242857142857143,
      "grad_norm": 1.5291402339935303,
      "learning_rate": 0.00017838095238095238,
      "loss": 3.0174,
      "step": 2270
    },
    {
      "epoch": 0.32571428571428573,
      "grad_norm": 1.5910332202911377,
      "learning_rate": 0.0001782857142857143,
      "loss": 2.9351,
      "step": 2280
    },
    {
      "epoch": 0.3271428571428571,
      "grad_norm": 1.478082299232483,
      "learning_rate": 0.00017819047619047618,
      "loss": 2.9809,
      "step": 2290
    },
    {
      "epoch": 0.32857142857142857,
      "grad_norm": 1.6078115701675415,
      "learning_rate": 0.0001780952380952381,
      "loss": 3.045,
      "step": 2300
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.2654328346252441,
      "learning_rate": 0.00017800000000000002,
      "loss": 3.1256,
      "step": 2310
    },
    {
      "epoch": 0.3314285714285714,
      "grad_norm": 1.0998550653457642,
      "learning_rate": 0.0001779047619047619,
      "loss": 3.0714,
      "step": 2320
    },
    {
      "epoch": 0.33285714285714285,
      "grad_norm": 1.2772098779678345,
      "learning_rate": 0.00017780952380952382,
      "loss": 3.0686,
      "step": 2330
    },
    {
      "epoch": 0.3342857142857143,
      "grad_norm": 1.674763798713684,
      "learning_rate": 0.0001777142857142857,
      "loss": 3.0362,
      "step": 2340
    },
    {
      "epoch": 0.3357142857142857,
      "grad_norm": 1.1828621625900269,
      "learning_rate": 0.00017761904761904763,
      "loss": 2.9533,
      "step": 2350
    },
    {
      "epoch": 0.33714285714285713,
      "grad_norm": 1.1217182874679565,
      "learning_rate": 0.00017752380952380955,
      "loss": 3.1276,
      "step": 2360
    },
    {
      "epoch": 0.3385714285714286,
      "grad_norm": 1.5393381118774414,
      "learning_rate": 0.00017742857142857144,
      "loss": 2.9132,
      "step": 2370
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.273390293121338,
      "learning_rate": 0.00017733333333333335,
      "loss": 2.9734,
      "step": 2380
    },
    {
      "epoch": 0.3414285714285714,
      "grad_norm": 1.4689785242080688,
      "learning_rate": 0.00017723809523809524,
      "loss": 2.9725,
      "step": 2390
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 1.229038119316101,
      "learning_rate": 0.00017714285714285713,
      "loss": 2.9848,
      "step": 2400
    },
    {
      "epoch": 0.3442857142857143,
      "grad_norm": 1.393832802772522,
      "learning_rate": 0.00017704761904761907,
      "loss": 3.0519,
      "step": 2410
    },
    {
      "epoch": 0.3457142857142857,
      "grad_norm": 1.1247797012329102,
      "learning_rate": 0.00017695238095238096,
      "loss": 3.0429,
      "step": 2420
    },
    {
      "epoch": 0.34714285714285714,
      "grad_norm": 1.1177899837493896,
      "learning_rate": 0.00017685714285714285,
      "loss": 3.0028,
      "step": 2430
    },
    {
      "epoch": 0.3485714285714286,
      "grad_norm": 1.1800031661987305,
      "learning_rate": 0.00017676190476190477,
      "loss": 2.9061,
      "step": 2440
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.1534777879714966,
      "learning_rate": 0.00017666666666666666,
      "loss": 2.9634,
      "step": 2450
    },
    {
      "epoch": 0.3514285714285714,
      "grad_norm": 1.2850674390792847,
      "learning_rate": 0.00017657142857142858,
      "loss": 3.2501,
      "step": 2460
    },
    {
      "epoch": 0.35285714285714287,
      "grad_norm": 1.2681291103363037,
      "learning_rate": 0.0001764761904761905,
      "loss": 3.1119,
      "step": 2470
    },
    {
      "epoch": 0.35428571428571426,
      "grad_norm": 1.4194244146347046,
      "learning_rate": 0.00017638095238095238,
      "loss": 3.0434,
      "step": 2480
    },
    {
      "epoch": 0.3557142857142857,
      "grad_norm": 1.0549734830856323,
      "learning_rate": 0.0001762857142857143,
      "loss": 3.0485,
      "step": 2490
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 1.219031810760498,
      "learning_rate": 0.0001761904761904762,
      "loss": 2.8967,
      "step": 2500
    },
    {
      "epoch": 0.3585714285714286,
      "grad_norm": 1.0929713249206543,
      "learning_rate": 0.0001760952380952381,
      "loss": 2.8189,
      "step": 2510
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.1165134906768799,
      "learning_rate": 0.00017600000000000002,
      "loss": 2.9138,
      "step": 2520
    },
    {
      "epoch": 0.36142857142857143,
      "grad_norm": 1.0620659589767456,
      "learning_rate": 0.0001759047619047619,
      "loss": 2.882,
      "step": 2530
    },
    {
      "epoch": 0.3628571428571429,
      "grad_norm": 1.178249716758728,
      "learning_rate": 0.00017580952380952383,
      "loss": 3.0608,
      "step": 2540
    },
    {
      "epoch": 0.36428571428571427,
      "grad_norm": 1.3795634508132935,
      "learning_rate": 0.00017571428571428572,
      "loss": 2.9926,
      "step": 2550
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 1.2664064168930054,
      "learning_rate": 0.0001756190476190476,
      "loss": 3.0292,
      "step": 2560
    },
    {
      "epoch": 0.36714285714285716,
      "grad_norm": 1.466050148010254,
      "learning_rate": 0.00017552380952380952,
      "loss": 2.89,
      "step": 2570
    },
    {
      "epoch": 0.36857142857142855,
      "grad_norm": 1.535115122795105,
      "learning_rate": 0.00017542857142857144,
      "loss": 3.0316,
      "step": 2580
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.1006107330322266,
      "learning_rate": 0.00017533333333333336,
      "loss": 3.1163,
      "step": 2590
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 1.1774235963821411,
      "learning_rate": 0.00017523809523809525,
      "loss": 2.9339,
      "step": 2600
    },
    {
      "epoch": 0.37285714285714283,
      "grad_norm": 1.5540213584899902,
      "learning_rate": 0.00017514285714285714,
      "loss": 2.9406,
      "step": 2610
    },
    {
      "epoch": 0.3742857142857143,
      "grad_norm": 1.363396406173706,
      "learning_rate": 0.00017504761904761905,
      "loss": 3.0653,
      "step": 2620
    },
    {
      "epoch": 0.3757142857142857,
      "grad_norm": 1.5029579401016235,
      "learning_rate": 0.00017495238095238094,
      "loss": 3.0645,
      "step": 2630
    },
    {
      "epoch": 0.37714285714285717,
      "grad_norm": 1.569185733795166,
      "learning_rate": 0.0001748571428571429,
      "loss": 3.0443,
      "step": 2640
    },
    {
      "epoch": 0.37857142857142856,
      "grad_norm": 1.5255612134933472,
      "learning_rate": 0.00017476190476190478,
      "loss": 3.0803,
      "step": 2650
    },
    {
      "epoch": 0.38,
      "grad_norm": 2.4177610874176025,
      "learning_rate": 0.00017466666666666667,
      "loss": 2.9553,
      "step": 2660
    },
    {
      "epoch": 0.38142857142857145,
      "grad_norm": 1.15415358543396,
      "learning_rate": 0.00017457142857142858,
      "loss": 2.9649,
      "step": 2670
    },
    {
      "epoch": 0.38285714285714284,
      "grad_norm": 1.0968989133834839,
      "learning_rate": 0.00017447619047619047,
      "loss": 3.0175,
      "step": 2680
    },
    {
      "epoch": 0.3842857142857143,
      "grad_norm": 1.536792278289795,
      "learning_rate": 0.0001743809523809524,
      "loss": 3.0761,
      "step": 2690
    },
    {
      "epoch": 0.38571428571428573,
      "grad_norm": 2.3689284324645996,
      "learning_rate": 0.0001742857142857143,
      "loss": 3.0045,
      "step": 2700
    },
    {
      "epoch": 0.3871428571428571,
      "grad_norm": 1.6654419898986816,
      "learning_rate": 0.0001741904761904762,
      "loss": 2.9407,
      "step": 2710
    },
    {
      "epoch": 0.38857142857142857,
      "grad_norm": 1.2571231126785278,
      "learning_rate": 0.0001740952380952381,
      "loss": 3.0563,
      "step": 2720
    },
    {
      "epoch": 0.39,
      "grad_norm": 2.236668109893799,
      "learning_rate": 0.000174,
      "loss": 3.028,
      "step": 2730
    },
    {
      "epoch": 0.3914285714285714,
      "grad_norm": 1.177315592765808,
      "learning_rate": 0.0001739047619047619,
      "loss": 3.0234,
      "step": 2740
    },
    {
      "epoch": 0.39285714285714285,
      "grad_norm": 1.524924397468567,
      "learning_rate": 0.00017380952380952383,
      "loss": 2.9751,
      "step": 2750
    },
    {
      "epoch": 0.3942857142857143,
      "grad_norm": 1.7177644968032837,
      "learning_rate": 0.00017371428571428572,
      "loss": 2.824,
      "step": 2760
    },
    {
      "epoch": 0.39571428571428574,
      "grad_norm": 1.386795997619629,
      "learning_rate": 0.00017361904761904764,
      "loss": 2.9722,
      "step": 2770
    },
    {
      "epoch": 0.39714285714285713,
      "grad_norm": 2.414727210998535,
      "learning_rate": 0.00017352380952380953,
      "loss": 2.9812,
      "step": 2780
    },
    {
      "epoch": 0.3985714285714286,
      "grad_norm": 1.48186457157135,
      "learning_rate": 0.00017342857142857142,
      "loss": 2.9763,
      "step": 2790
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.4702320098876953,
      "learning_rate": 0.00017333333333333334,
      "loss": 2.9231,
      "step": 2800
    },
    {
      "epoch": 0.4014285714285714,
      "grad_norm": 1.9229055643081665,
      "learning_rate": 0.00017323809523809525,
      "loss": 2.8833,
      "step": 2810
    },
    {
      "epoch": 0.40285714285714286,
      "grad_norm": 1.2549779415130615,
      "learning_rate": 0.00017314285714285717,
      "loss": 3.1662,
      "step": 2820
    },
    {
      "epoch": 0.4042857142857143,
      "grad_norm": 1.5076801776885986,
      "learning_rate": 0.00017304761904761906,
      "loss": 3.0082,
      "step": 2830
    },
    {
      "epoch": 0.4057142857142857,
      "grad_norm": 1.6714630126953125,
      "learning_rate": 0.00017295238095238095,
      "loss": 2.9303,
      "step": 2840
    },
    {
      "epoch": 0.40714285714285714,
      "grad_norm": 1.519159197807312,
      "learning_rate": 0.00017285714285714287,
      "loss": 3.0527,
      "step": 2850
    },
    {
      "epoch": 0.4085714285714286,
      "grad_norm": 1.3347190618515015,
      "learning_rate": 0.00017276190476190478,
      "loss": 2.9815,
      "step": 2860
    },
    {
      "epoch": 0.41,
      "grad_norm": 2.8290059566497803,
      "learning_rate": 0.00017266666666666667,
      "loss": 2.9473,
      "step": 2870
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 2.2771499156951904,
      "learning_rate": 0.0001725714285714286,
      "loss": 2.8612,
      "step": 2880
    },
    {
      "epoch": 0.41285714285714287,
      "grad_norm": 1.1238549947738647,
      "learning_rate": 0.00017247619047619048,
      "loss": 3.0164,
      "step": 2890
    },
    {
      "epoch": 0.4142857142857143,
      "grad_norm": 1.3494374752044678,
      "learning_rate": 0.0001723809523809524,
      "loss": 3.015,
      "step": 2900
    },
    {
      "epoch": 0.4157142857142857,
      "grad_norm": 1.3717745542526245,
      "learning_rate": 0.00017228571428571428,
      "loss": 3.0155,
      "step": 2910
    },
    {
      "epoch": 0.41714285714285715,
      "grad_norm": 1.5391736030578613,
      "learning_rate": 0.0001721904761904762,
      "loss": 2.9958,
      "step": 2920
    },
    {
      "epoch": 0.4185714285714286,
      "grad_norm": 1.4009172916412354,
      "learning_rate": 0.00017209523809523812,
      "loss": 3.0603,
      "step": 2930
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.7665965557098389,
      "learning_rate": 0.000172,
      "loss": 2.995,
      "step": 2940
    },
    {
      "epoch": 0.42142857142857143,
      "grad_norm": 2.711808443069458,
      "learning_rate": 0.00017190476190476192,
      "loss": 2.9479,
      "step": 2950
    },
    {
      "epoch": 0.4228571428571429,
      "grad_norm": 1.2729992866516113,
      "learning_rate": 0.0001718095238095238,
      "loss": 2.9579,
      "step": 2960
    },
    {
      "epoch": 0.42428571428571427,
      "grad_norm": 1.8231201171875,
      "learning_rate": 0.00017171428571428573,
      "loss": 2.9988,
      "step": 2970
    },
    {
      "epoch": 0.4257142857142857,
      "grad_norm": 1.2252004146575928,
      "learning_rate": 0.00017161904761904765,
      "loss": 2.9469,
      "step": 2980
    },
    {
      "epoch": 0.42714285714285716,
      "grad_norm": 1.1906139850616455,
      "learning_rate": 0.00017152380952380954,
      "loss": 2.9782,
      "step": 2990
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 0.8451075553894043,
      "learning_rate": 0.00017142857142857143,
      "loss": 2.9761,
      "step": 3000
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.2241629362106323,
      "learning_rate": 0.00017133333333333334,
      "loss": 3.0392,
      "step": 3010
    },
    {
      "epoch": 0.43142857142857144,
      "grad_norm": 1.005982518196106,
      "learning_rate": 0.00017123809523809523,
      "loss": 3.0649,
      "step": 3020
    },
    {
      "epoch": 0.4328571428571429,
      "grad_norm": 1.3085235357284546,
      "learning_rate": 0.00017114285714285715,
      "loss": 2.847,
      "step": 3030
    },
    {
      "epoch": 0.4342857142857143,
      "grad_norm": 1.1788703203201294,
      "learning_rate": 0.00017104761904761906,
      "loss": 2.9615,
      "step": 3040
    },
    {
      "epoch": 0.4357142857142857,
      "grad_norm": 1.101954460144043,
      "learning_rate": 0.00017095238095238095,
      "loss": 2.9169,
      "step": 3050
    },
    {
      "epoch": 0.43714285714285717,
      "grad_norm": 1.0495189428329468,
      "learning_rate": 0.00017085714285714287,
      "loss": 2.9998,
      "step": 3060
    },
    {
      "epoch": 0.43857142857142856,
      "grad_norm": 1.3259392976760864,
      "learning_rate": 0.00017076190476190476,
      "loss": 2.934,
      "step": 3070
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.21047842502594,
      "learning_rate": 0.00017066666666666668,
      "loss": 2.9976,
      "step": 3080
    },
    {
      "epoch": 0.44142857142857145,
      "grad_norm": 1.395577073097229,
      "learning_rate": 0.0001705714285714286,
      "loss": 2.9051,
      "step": 3090
    },
    {
      "epoch": 0.44285714285714284,
      "grad_norm": 1.1908963918685913,
      "learning_rate": 0.00017047619047619048,
      "loss": 2.9081,
      "step": 3100
    },
    {
      "epoch": 0.4442857142857143,
      "grad_norm": 1.1739593744277954,
      "learning_rate": 0.0001703809523809524,
      "loss": 2.9476,
      "step": 3110
    },
    {
      "epoch": 0.44571428571428573,
      "grad_norm": 2.8225550651550293,
      "learning_rate": 0.0001702857142857143,
      "loss": 3.003,
      "step": 3120
    },
    {
      "epoch": 0.4471428571428571,
      "grad_norm": 1.0352247953414917,
      "learning_rate": 0.00017019047619047618,
      "loss": 2.9508,
      "step": 3130
    },
    {
      "epoch": 0.44857142857142857,
      "grad_norm": 1.5522984266281128,
      "learning_rate": 0.0001700952380952381,
      "loss": 2.8135,
      "step": 3140
    },
    {
      "epoch": 0.45,
      "grad_norm": 1.1786402463912964,
      "learning_rate": 0.00017,
      "loss": 2.8756,
      "step": 3150
    },
    {
      "epoch": 0.4514285714285714,
      "grad_norm": 1.0736510753631592,
      "learning_rate": 0.00016990476190476193,
      "loss": 2.8498,
      "step": 3160
    },
    {
      "epoch": 0.45285714285714285,
      "grad_norm": 1.2785876989364624,
      "learning_rate": 0.00016980952380952382,
      "loss": 2.7308,
      "step": 3170
    },
    {
      "epoch": 0.4542857142857143,
      "grad_norm": 2.112595796585083,
      "learning_rate": 0.0001697142857142857,
      "loss": 3.0345,
      "step": 3180
    },
    {
      "epoch": 0.45571428571428574,
      "grad_norm": 1.5264763832092285,
      "learning_rate": 0.00016961904761904762,
      "loss": 3.1867,
      "step": 3190
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 1.221042275428772,
      "learning_rate": 0.00016952380952380954,
      "loss": 3.0799,
      "step": 3200
    },
    {
      "epoch": 0.4585714285714286,
      "grad_norm": 1.027255654335022,
      "learning_rate": 0.00016942857142857146,
      "loss": 2.9672,
      "step": 3210
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.0890791416168213,
      "learning_rate": 0.00016933333333333335,
      "loss": 2.9782,
      "step": 3220
    },
    {
      "epoch": 0.4614285714285714,
      "grad_norm": 1.03936767578125,
      "learning_rate": 0.00016923809523809524,
      "loss": 2.8628,
      "step": 3230
    },
    {
      "epoch": 0.46285714285714286,
      "grad_norm": 1.2125740051269531,
      "learning_rate": 0.00016914285714285715,
      "loss": 2.923,
      "step": 3240
    },
    {
      "epoch": 0.4642857142857143,
      "grad_norm": 1.0493296384811401,
      "learning_rate": 0.00016904761904761904,
      "loss": 2.9764,
      "step": 3250
    },
    {
      "epoch": 0.4657142857142857,
      "grad_norm": 1.6725658178329468,
      "learning_rate": 0.00016895238095238096,
      "loss": 3.0076,
      "step": 3260
    },
    {
      "epoch": 0.46714285714285714,
      "grad_norm": 0.98200923204422,
      "learning_rate": 0.00016885714285714288,
      "loss": 2.871,
      "step": 3270
    },
    {
      "epoch": 0.4685714285714286,
      "grad_norm": 1.181710958480835,
      "learning_rate": 0.00016876190476190477,
      "loss": 2.915,
      "step": 3280
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.703783631324768,
      "learning_rate": 0.00016866666666666668,
      "loss": 3.0197,
      "step": 3290
    },
    {
      "epoch": 0.4714285714285714,
      "grad_norm": 1.3900889158248901,
      "learning_rate": 0.00016857142857142857,
      "loss": 2.9761,
      "step": 3300
    },
    {
      "epoch": 0.47285714285714286,
      "grad_norm": 1.1087580919265747,
      "learning_rate": 0.0001684761904761905,
      "loss": 3.0379,
      "step": 3310
    },
    {
      "epoch": 0.4742857142857143,
      "grad_norm": 1.1923514604568481,
      "learning_rate": 0.0001683809523809524,
      "loss": 3.0189,
      "step": 3320
    },
    {
      "epoch": 0.4757142857142857,
      "grad_norm": 1.1110925674438477,
      "learning_rate": 0.0001682857142857143,
      "loss": 3.0003,
      "step": 3330
    },
    {
      "epoch": 0.47714285714285715,
      "grad_norm": 1.0464447736740112,
      "learning_rate": 0.0001681904761904762,
      "loss": 3.0722,
      "step": 3340
    },
    {
      "epoch": 0.4785714285714286,
      "grad_norm": 1.3996155261993408,
      "learning_rate": 0.0001680952380952381,
      "loss": 3.0559,
      "step": 3350
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.346903920173645,
      "learning_rate": 0.000168,
      "loss": 3.1023,
      "step": 3360
    },
    {
      "epoch": 0.48142857142857143,
      "grad_norm": 1.0747967958450317,
      "learning_rate": 0.0001679047619047619,
      "loss": 2.9477,
      "step": 3370
    },
    {
      "epoch": 0.4828571428571429,
      "grad_norm": 1.2472575902938843,
      "learning_rate": 0.00016780952380952382,
      "loss": 2.9457,
      "step": 3380
    },
    {
      "epoch": 0.48428571428571426,
      "grad_norm": 1.759090781211853,
      "learning_rate": 0.0001677142857142857,
      "loss": 2.8567,
      "step": 3390
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 1.039373755455017,
      "learning_rate": 0.00016761904761904763,
      "loss": 3.0732,
      "step": 3400
    },
    {
      "epoch": 0.48714285714285716,
      "grad_norm": 1.5910675525665283,
      "learning_rate": 0.00016752380952380952,
      "loss": 2.8839,
      "step": 3410
    },
    {
      "epoch": 0.48857142857142855,
      "grad_norm": 0.9521124362945557,
      "learning_rate": 0.00016742857142857144,
      "loss": 3.0255,
      "step": 3420
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.0529160499572754,
      "learning_rate": 0.00016733333333333335,
      "loss": 3.0371,
      "step": 3430
    },
    {
      "epoch": 0.49142857142857144,
      "grad_norm": 1.1371692419052124,
      "learning_rate": 0.00016723809523809524,
      "loss": 2.8631,
      "step": 3440
    },
    {
      "epoch": 0.4928571428571429,
      "grad_norm": 1.3481889963150024,
      "learning_rate": 0.00016714285714285716,
      "loss": 2.8768,
      "step": 3450
    },
    {
      "epoch": 0.4942857142857143,
      "grad_norm": 0.9360548853874207,
      "learning_rate": 0.00016704761904761905,
      "loss": 2.9293,
      "step": 3460
    },
    {
      "epoch": 0.4957142857142857,
      "grad_norm": 1.1416304111480713,
      "learning_rate": 0.00016695238095238097,
      "loss": 3.1092,
      "step": 3470
    },
    {
      "epoch": 0.49714285714285716,
      "grad_norm": 1.3101353645324707,
      "learning_rate": 0.00016685714285714285,
      "loss": 3.0058,
      "step": 3480
    },
    {
      "epoch": 0.49857142857142855,
      "grad_norm": 1.2451385259628296,
      "learning_rate": 0.00016676190476190477,
      "loss": 3.0753,
      "step": 3490
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.4381942749023438,
      "learning_rate": 0.0001666666666666667,
      "loss": 2.9284,
      "step": 3500
    },
    {
      "epoch": 0.5014285714285714,
      "grad_norm": 1.063027262687683,
      "learning_rate": 0.00016657142857142858,
      "loss": 3.0493,
      "step": 3510
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 1.0724706649780273,
      "learning_rate": 0.00016647619047619047,
      "loss": 3.1177,
      "step": 3520
    },
    {
      "epoch": 0.5042857142857143,
      "grad_norm": 1.3620717525482178,
      "learning_rate": 0.00016638095238095238,
      "loss": 2.7949,
      "step": 3530
    },
    {
      "epoch": 0.5057142857142857,
      "grad_norm": 1.2165732383728027,
      "learning_rate": 0.0001662857142857143,
      "loss": 2.7738,
      "step": 3540
    },
    {
      "epoch": 0.5071428571428571,
      "grad_norm": 1.4571819305419922,
      "learning_rate": 0.00016619047619047622,
      "loss": 2.875,
      "step": 3550
    },
    {
      "epoch": 0.5085714285714286,
      "grad_norm": 1.49783456325531,
      "learning_rate": 0.0001660952380952381,
      "loss": 3.0605,
      "step": 3560
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.2627604007720947,
      "learning_rate": 0.000166,
      "loss": 2.9325,
      "step": 3570
    },
    {
      "epoch": 0.5114285714285715,
      "grad_norm": 1.142688274383545,
      "learning_rate": 0.0001659047619047619,
      "loss": 2.7838,
      "step": 3580
    },
    {
      "epoch": 0.5128571428571429,
      "grad_norm": 1.238222599029541,
      "learning_rate": 0.0001658095238095238,
      "loss": 2.8852,
      "step": 3590
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 1.2198418378829956,
      "learning_rate": 0.00016571428571428575,
      "loss": 2.9858,
      "step": 3600
    },
    {
      "epoch": 0.5157142857142857,
      "grad_norm": 0.9891067147254944,
      "learning_rate": 0.00016561904761904764,
      "loss": 2.8202,
      "step": 3610
    },
    {
      "epoch": 0.5171428571428571,
      "grad_norm": 1.2141497135162354,
      "learning_rate": 0.00016552380952380953,
      "loss": 3.0007,
      "step": 3620
    },
    {
      "epoch": 0.5185714285714286,
      "grad_norm": 0.9622252583503723,
      "learning_rate": 0.00016542857142857144,
      "loss": 2.9466,
      "step": 3630
    },
    {
      "epoch": 0.52,
      "grad_norm": 2.1123878955841064,
      "learning_rate": 0.00016533333333333333,
      "loss": 3.1029,
      "step": 3640
    },
    {
      "epoch": 0.5214285714285715,
      "grad_norm": 1.0441440343856812,
      "learning_rate": 0.00016523809523809525,
      "loss": 2.8663,
      "step": 3650
    },
    {
      "epoch": 0.5228571428571429,
      "grad_norm": 1.0175009965896606,
      "learning_rate": 0.00016514285714285716,
      "loss": 3.0337,
      "step": 3660
    },
    {
      "epoch": 0.5242857142857142,
      "grad_norm": 1.3214528560638428,
      "learning_rate": 0.00016504761904761905,
      "loss": 2.9949,
      "step": 3670
    },
    {
      "epoch": 0.5257142857142857,
      "grad_norm": 0.9851755499839783,
      "learning_rate": 0.00016495238095238097,
      "loss": 3.0264,
      "step": 3680
    },
    {
      "epoch": 0.5271428571428571,
      "grad_norm": 1.2984975576400757,
      "learning_rate": 0.00016485714285714286,
      "loss": 3.0499,
      "step": 3690
    },
    {
      "epoch": 0.5285714285714286,
      "grad_norm": 1.0188202857971191,
      "learning_rate": 0.00016476190476190475,
      "loss": 2.867,
      "step": 3700
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.9826651215553284,
      "learning_rate": 0.00016466666666666667,
      "loss": 2.8322,
      "step": 3710
    },
    {
      "epoch": 0.5314285714285715,
      "grad_norm": 1.4021344184875488,
      "learning_rate": 0.00016457142857142858,
      "loss": 3.091,
      "step": 3720
    },
    {
      "epoch": 0.5328571428571428,
      "grad_norm": 1.3847295045852661,
      "learning_rate": 0.0001644761904761905,
      "loss": 2.9711,
      "step": 3730
    },
    {
      "epoch": 0.5342857142857143,
      "grad_norm": 1.0077987909317017,
      "learning_rate": 0.0001643809523809524,
      "loss": 2.9825,
      "step": 3740
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 0.9349408745765686,
      "learning_rate": 0.00016428571428571428,
      "loss": 2.8951,
      "step": 3750
    },
    {
      "epoch": 0.5371428571428571,
      "grad_norm": 1.4218815565109253,
      "learning_rate": 0.0001641904761904762,
      "loss": 3.1412,
      "step": 3760
    },
    {
      "epoch": 0.5385714285714286,
      "grad_norm": 1.082007646560669,
      "learning_rate": 0.0001640952380952381,
      "loss": 2.8793,
      "step": 3770
    },
    {
      "epoch": 0.54,
      "grad_norm": 1.1697077751159668,
      "learning_rate": 0.000164,
      "loss": 2.9636,
      "step": 3780
    },
    {
      "epoch": 0.5414285714285715,
      "grad_norm": 1.3345166444778442,
      "learning_rate": 0.00016390476190476192,
      "loss": 2.9949,
      "step": 3790
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 1.1756031513214111,
      "learning_rate": 0.0001638095238095238,
      "loss": 2.9767,
      "step": 3800
    },
    {
      "epoch": 0.5442857142857143,
      "grad_norm": 1.3360884189605713,
      "learning_rate": 0.00016371428571428572,
      "loss": 2.9429,
      "step": 3810
    },
    {
      "epoch": 0.5457142857142857,
      "grad_norm": 1.411512851715088,
      "learning_rate": 0.00016361904761904761,
      "loss": 2.9697,
      "step": 3820
    },
    {
      "epoch": 0.5471428571428572,
      "grad_norm": 1.1478281021118164,
      "learning_rate": 0.00016352380952380953,
      "loss": 2.944,
      "step": 3830
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 1.601224660873413,
      "learning_rate": 0.00016342857142857145,
      "loss": 2.9267,
      "step": 3840
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.1450037956237793,
      "learning_rate": 0.00016333333333333334,
      "loss": 2.8454,
      "step": 3850
    },
    {
      "epoch": 0.5514285714285714,
      "grad_norm": 1.4351146221160889,
      "learning_rate": 0.00016323809523809525,
      "loss": 2.979,
      "step": 3860
    },
    {
      "epoch": 0.5528571428571428,
      "grad_norm": 1.082353949546814,
      "learning_rate": 0.00016314285714285714,
      "loss": 2.8779,
      "step": 3870
    },
    {
      "epoch": 0.5542857142857143,
      "grad_norm": 1.2482291460037231,
      "learning_rate": 0.00016304761904761906,
      "loss": 2.9584,
      "step": 3880
    },
    {
      "epoch": 0.5557142857142857,
      "grad_norm": 1.208840012550354,
      "learning_rate": 0.00016295238095238098,
      "loss": 2.9652,
      "step": 3890
    },
    {
      "epoch": 0.5571428571428572,
      "grad_norm": 1.1517280340194702,
      "learning_rate": 0.00016285714285714287,
      "loss": 2.8978,
      "step": 3900
    },
    {
      "epoch": 0.5585714285714286,
      "grad_norm": 1.9078267812728882,
      "learning_rate": 0.00016276190476190476,
      "loss": 3.0724,
      "step": 3910
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.0021816492080688,
      "learning_rate": 0.00016266666666666667,
      "loss": 2.8185,
      "step": 3920
    },
    {
      "epoch": 0.5614285714285714,
      "grad_norm": 1.1244380474090576,
      "learning_rate": 0.00016257142857142856,
      "loss": 2.9204,
      "step": 3930
    },
    {
      "epoch": 0.5628571428571428,
      "grad_norm": 1.2908987998962402,
      "learning_rate": 0.0001624761904761905,
      "loss": 2.9654,
      "step": 3940
    },
    {
      "epoch": 0.5642857142857143,
      "grad_norm": 1.1806200742721558,
      "learning_rate": 0.0001623809523809524,
      "loss": 2.9715,
      "step": 3950
    },
    {
      "epoch": 0.5657142857142857,
      "grad_norm": 1.513838291168213,
      "learning_rate": 0.00016228571428571428,
      "loss": 2.8294,
      "step": 3960
    },
    {
      "epoch": 0.5671428571428572,
      "grad_norm": 1.0763198137283325,
      "learning_rate": 0.0001621904761904762,
      "loss": 2.864,
      "step": 3970
    },
    {
      "epoch": 0.5685714285714286,
      "grad_norm": 1.2301993370056152,
      "learning_rate": 0.0001620952380952381,
      "loss": 3.0039,
      "step": 3980
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.2831672430038452,
      "learning_rate": 0.000162,
      "loss": 2.8214,
      "step": 3990
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.8731744885444641,
      "learning_rate": 0.00016190476190476192,
      "loss": 2.9638,
      "step": 4000
    },
    {
      "epoch": 0.5728571428571428,
      "grad_norm": 1.7070164680480957,
      "learning_rate": 0.00016180952380952381,
      "loss": 2.9332,
      "step": 4010
    },
    {
      "epoch": 0.5742857142857143,
      "grad_norm": 1.1122243404388428,
      "learning_rate": 0.00016171428571428573,
      "loss": 3.0175,
      "step": 4020
    },
    {
      "epoch": 0.5757142857142857,
      "grad_norm": 1.0998001098632812,
      "learning_rate": 0.00016161904761904762,
      "loss": 3.0428,
      "step": 4030
    },
    {
      "epoch": 0.5771428571428572,
      "grad_norm": 1.2888329029083252,
      "learning_rate": 0.0001615238095238095,
      "loss": 2.9249,
      "step": 4040
    },
    {
      "epoch": 0.5785714285714286,
      "grad_norm": 1.1957151889801025,
      "learning_rate": 0.00016142857142857145,
      "loss": 2.9059,
      "step": 4050
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.3070456981658936,
      "learning_rate": 0.00016133333333333334,
      "loss": 2.9571,
      "step": 4060
    },
    {
      "epoch": 0.5814285714285714,
      "grad_norm": 1.1914030313491821,
      "learning_rate": 0.00016123809523809526,
      "loss": 2.9793,
      "step": 4070
    },
    {
      "epoch": 0.5828571428571429,
      "grad_norm": 2.358452320098877,
      "learning_rate": 0.00016114285714285715,
      "loss": 2.8054,
      "step": 4080
    },
    {
      "epoch": 0.5842857142857143,
      "grad_norm": 0.8636426329612732,
      "learning_rate": 0.00016104761904761904,
      "loss": 2.9804,
      "step": 4090
    },
    {
      "epoch": 0.5857142857142857,
      "grad_norm": 2.646523952484131,
      "learning_rate": 0.00016095238095238096,
      "loss": 2.9169,
      "step": 4100
    },
    {
      "epoch": 0.5871428571428572,
      "grad_norm": 1.3095606565475464,
      "learning_rate": 0.00016085714285714287,
      "loss": 2.8842,
      "step": 4110
    },
    {
      "epoch": 0.5885714285714285,
      "grad_norm": 1.3759058713912964,
      "learning_rate": 0.0001607619047619048,
      "loss": 2.8166,
      "step": 4120
    },
    {
      "epoch": 0.59,
      "grad_norm": 1.3166534900665283,
      "learning_rate": 0.00016066666666666668,
      "loss": 2.9261,
      "step": 4130
    },
    {
      "epoch": 0.5914285714285714,
      "grad_norm": 1.785327672958374,
      "learning_rate": 0.00016057142857142857,
      "loss": 2.8601,
      "step": 4140
    },
    {
      "epoch": 0.5928571428571429,
      "grad_norm": 1.4672157764434814,
      "learning_rate": 0.00016047619047619048,
      "loss": 2.8804,
      "step": 4150
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 0.9584782123565674,
      "learning_rate": 0.0001603809523809524,
      "loss": 2.9464,
      "step": 4160
    },
    {
      "epoch": 0.5957142857142858,
      "grad_norm": 1.478431224822998,
      "learning_rate": 0.0001602857142857143,
      "loss": 2.855,
      "step": 4170
    },
    {
      "epoch": 0.5971428571428572,
      "grad_norm": 2.040835380554199,
      "learning_rate": 0.0001601904761904762,
      "loss": 2.8606,
      "step": 4180
    },
    {
      "epoch": 0.5985714285714285,
      "grad_norm": 1.2789795398712158,
      "learning_rate": 0.0001600952380952381,
      "loss": 2.929,
      "step": 4190
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.1434615850448608,
      "learning_rate": 0.00016,
      "loss": 2.8688,
      "step": 4200
    },
    {
      "epoch": 0.6014285714285714,
      "grad_norm": 1.212323546409607,
      "learning_rate": 0.0001599047619047619,
      "loss": 2.9327,
      "step": 4210
    },
    {
      "epoch": 0.6028571428571429,
      "grad_norm": 1.0896995067596436,
      "learning_rate": 0.00015980952380952382,
      "loss": 2.8001,
      "step": 4220
    },
    {
      "epoch": 0.6042857142857143,
      "grad_norm": 1.023606538772583,
      "learning_rate": 0.00015971428571428574,
      "loss": 2.8202,
      "step": 4230
    },
    {
      "epoch": 0.6057142857142858,
      "grad_norm": 1.1988428831100464,
      "learning_rate": 0.00015961904761904763,
      "loss": 2.8596,
      "step": 4240
    },
    {
      "epoch": 0.6071428571428571,
      "grad_norm": 1.4331257343292236,
      "learning_rate": 0.00015952380952380954,
      "loss": 2.8311,
      "step": 4250
    },
    {
      "epoch": 0.6085714285714285,
      "grad_norm": 1.3789180517196655,
      "learning_rate": 0.00015942857142857143,
      "loss": 2.9761,
      "step": 4260
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.3673211336135864,
      "learning_rate": 0.00015933333333333332,
      "loss": 2.9336,
      "step": 4270
    },
    {
      "epoch": 0.6114285714285714,
      "grad_norm": 1.4701640605926514,
      "learning_rate": 0.00015923809523809526,
      "loss": 2.8979,
      "step": 4280
    },
    {
      "epoch": 0.6128571428571429,
      "grad_norm": 1.154563307762146,
      "learning_rate": 0.00015914285714285715,
      "loss": 2.8643,
      "step": 4290
    },
    {
      "epoch": 0.6142857142857143,
      "grad_norm": 1.9574389457702637,
      "learning_rate": 0.00015904761904761904,
      "loss": 2.8444,
      "step": 4300
    },
    {
      "epoch": 0.6157142857142858,
      "grad_norm": 1.417551875114441,
      "learning_rate": 0.00015895238095238096,
      "loss": 2.8462,
      "step": 4310
    },
    {
      "epoch": 0.6171428571428571,
      "grad_norm": 1.545649528503418,
      "learning_rate": 0.00015885714285714285,
      "loss": 2.9342,
      "step": 4320
    },
    {
      "epoch": 0.6185714285714285,
      "grad_norm": 1.231529951095581,
      "learning_rate": 0.00015876190476190477,
      "loss": 2.9919,
      "step": 4330
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.4401962757110596,
      "learning_rate": 0.00015866666666666668,
      "loss": 2.9237,
      "step": 4340
    },
    {
      "epoch": 0.6214285714285714,
      "grad_norm": 1.4196922779083252,
      "learning_rate": 0.00015857142857142857,
      "loss": 2.9429,
      "step": 4350
    },
    {
      "epoch": 0.6228571428571429,
      "grad_norm": 1.0628836154937744,
      "learning_rate": 0.0001584761904761905,
      "loss": 2.8146,
      "step": 4360
    },
    {
      "epoch": 0.6242857142857143,
      "grad_norm": 1.1419011354446411,
      "learning_rate": 0.00015838095238095238,
      "loss": 2.9219,
      "step": 4370
    },
    {
      "epoch": 0.6257142857142857,
      "grad_norm": 0.9000279307365417,
      "learning_rate": 0.0001582857142857143,
      "loss": 2.7338,
      "step": 4380
    },
    {
      "epoch": 0.6271428571428571,
      "grad_norm": 1.1975246667861938,
      "learning_rate": 0.0001581904761904762,
      "loss": 2.9056,
      "step": 4390
    },
    {
      "epoch": 0.6285714285714286,
      "grad_norm": 1.6012599468231201,
      "learning_rate": 0.0001580952380952381,
      "loss": 3.0063,
      "step": 4400
    },
    {
      "epoch": 0.63,
      "grad_norm": 2.2173094749450684,
      "learning_rate": 0.00015800000000000002,
      "loss": 2.986,
      "step": 4410
    },
    {
      "epoch": 0.6314285714285715,
      "grad_norm": 1.0768929719924927,
      "learning_rate": 0.0001579047619047619,
      "loss": 2.8192,
      "step": 4420
    },
    {
      "epoch": 0.6328571428571429,
      "grad_norm": 1.3099957704544067,
      "learning_rate": 0.0001578095238095238,
      "loss": 2.9011,
      "step": 4430
    },
    {
      "epoch": 0.6342857142857142,
      "grad_norm": 1.4626697301864624,
      "learning_rate": 0.00015771428571428571,
      "loss": 2.7897,
      "step": 4440
    },
    {
      "epoch": 0.6357142857142857,
      "grad_norm": 1.3778258562088013,
      "learning_rate": 0.00015761904761904763,
      "loss": 2.9921,
      "step": 4450
    },
    {
      "epoch": 0.6371428571428571,
      "grad_norm": 1.0281063318252563,
      "learning_rate": 0.00015752380952380955,
      "loss": 2.9524,
      "step": 4460
    },
    {
      "epoch": 0.6385714285714286,
      "grad_norm": 1.1727145910263062,
      "learning_rate": 0.00015742857142857144,
      "loss": 2.8576,
      "step": 4470
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.1836315393447876,
      "learning_rate": 0.00015733333333333333,
      "loss": 2.8985,
      "step": 4480
    },
    {
      "epoch": 0.6414285714285715,
      "grad_norm": 1.433681607246399,
      "learning_rate": 0.00015723809523809524,
      "loss": 2.8253,
      "step": 4490
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 1.5127960443496704,
      "learning_rate": 0.00015714285714285716,
      "loss": 2.7489,
      "step": 4500
    },
    {
      "epoch": 0.6442857142857142,
      "grad_norm": 1.1188901662826538,
      "learning_rate": 0.00015704761904761908,
      "loss": 2.7533,
      "step": 4510
    },
    {
      "epoch": 0.6457142857142857,
      "grad_norm": 1.1357500553131104,
      "learning_rate": 0.00015695238095238097,
      "loss": 2.7842,
      "step": 4520
    },
    {
      "epoch": 0.6471428571428571,
      "grad_norm": 1.3412576913833618,
      "learning_rate": 0.00015685714285714286,
      "loss": 2.8975,
      "step": 4530
    },
    {
      "epoch": 0.6485714285714286,
      "grad_norm": 1.6693739891052246,
      "learning_rate": 0.00015676190476190477,
      "loss": 2.839,
      "step": 4540
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.3906464576721191,
      "learning_rate": 0.00015666666666666666,
      "loss": 2.9537,
      "step": 4550
    },
    {
      "epoch": 0.6514285714285715,
      "grad_norm": 1.408422827720642,
      "learning_rate": 0.00015657142857142858,
      "loss": 2.7375,
      "step": 4560
    },
    {
      "epoch": 0.6528571428571428,
      "grad_norm": 1.2042334079742432,
      "learning_rate": 0.0001564761904761905,
      "loss": 2.9642,
      "step": 4570
    },
    {
      "epoch": 0.6542857142857142,
      "grad_norm": 1.1794090270996094,
      "learning_rate": 0.00015638095238095238,
      "loss": 2.9785,
      "step": 4580
    },
    {
      "epoch": 0.6557142857142857,
      "grad_norm": 1.2204859256744385,
      "learning_rate": 0.0001562857142857143,
      "loss": 2.9186,
      "step": 4590
    },
    {
      "epoch": 0.6571428571428571,
      "grad_norm": 1.6874983310699463,
      "learning_rate": 0.0001561904761904762,
      "loss": 2.936,
      "step": 4600
    },
    {
      "epoch": 0.6585714285714286,
      "grad_norm": 1.2452783584594727,
      "learning_rate": 0.0001560952380952381,
      "loss": 2.9099,
      "step": 4610
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.126340389251709,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.8282,
      "step": 4620
    },
    {
      "epoch": 0.6614285714285715,
      "grad_norm": 1.2163456678390503,
      "learning_rate": 0.00015590476190476191,
      "loss": 2.9442,
      "step": 4630
    },
    {
      "epoch": 0.6628571428571428,
      "grad_norm": 1.1883692741394043,
      "learning_rate": 0.00015580952380952383,
      "loss": 2.8842,
      "step": 4640
    },
    {
      "epoch": 0.6642857142857143,
      "grad_norm": 1.2349680662155151,
      "learning_rate": 0.00015571428571428572,
      "loss": 2.8257,
      "step": 4650
    },
    {
      "epoch": 0.6657142857142857,
      "grad_norm": 1.2292201519012451,
      "learning_rate": 0.0001556190476190476,
      "loss": 3.0262,
      "step": 4660
    },
    {
      "epoch": 0.6671428571428571,
      "grad_norm": 1.9280787706375122,
      "learning_rate": 0.00015552380952380953,
      "loss": 2.8274,
      "step": 4670
    },
    {
      "epoch": 0.6685714285714286,
      "grad_norm": 1.073299527168274,
      "learning_rate": 0.00015542857142857144,
      "loss": 2.8676,
      "step": 4680
    },
    {
      "epoch": 0.67,
      "grad_norm": 1.071341872215271,
      "learning_rate": 0.00015533333333333333,
      "loss": 2.6621,
      "step": 4690
    },
    {
      "epoch": 0.6714285714285714,
      "grad_norm": 1.019899845123291,
      "learning_rate": 0.00015523809523809525,
      "loss": 2.9397,
      "step": 4700
    },
    {
      "epoch": 0.6728571428571428,
      "grad_norm": 1.5399922132492065,
      "learning_rate": 0.00015514285714285714,
      "loss": 2.8198,
      "step": 4710
    },
    {
      "epoch": 0.6742857142857143,
      "grad_norm": 1.181408166885376,
      "learning_rate": 0.00015504761904761906,
      "loss": 2.9428,
      "step": 4720
    },
    {
      "epoch": 0.6757142857142857,
      "grad_norm": 0.8832783699035645,
      "learning_rate": 0.00015495238095238097,
      "loss": 2.9394,
      "step": 4730
    },
    {
      "epoch": 0.6771428571428572,
      "grad_norm": 1.0656006336212158,
      "learning_rate": 0.00015485714285714286,
      "loss": 2.8148,
      "step": 4740
    },
    {
      "epoch": 0.6785714285714286,
      "grad_norm": 1.1774165630340576,
      "learning_rate": 0.00015476190476190478,
      "loss": 2.8028,
      "step": 4750
    },
    {
      "epoch": 0.68,
      "grad_norm": 1.5494258403778076,
      "learning_rate": 0.00015466666666666667,
      "loss": 2.7034,
      "step": 4760
    },
    {
      "epoch": 0.6814285714285714,
      "grad_norm": 1.3916149139404297,
      "learning_rate": 0.00015457142857142858,
      "loss": 2.8819,
      "step": 4770
    },
    {
      "epoch": 0.6828571428571428,
      "grad_norm": 0.9135054349899292,
      "learning_rate": 0.00015447619047619047,
      "loss": 2.8777,
      "step": 4780
    },
    {
      "epoch": 0.6842857142857143,
      "grad_norm": 1.5729210376739502,
      "learning_rate": 0.0001543809523809524,
      "loss": 2.9085,
      "step": 4790
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 0.9697902798652649,
      "learning_rate": 0.0001542857142857143,
      "loss": 2.8303,
      "step": 4800
    },
    {
      "epoch": 0.6871428571428572,
      "grad_norm": 1.2869104146957397,
      "learning_rate": 0.0001541904761904762,
      "loss": 3.0554,
      "step": 4810
    },
    {
      "epoch": 0.6885714285714286,
      "grad_norm": 1.1893627643585205,
      "learning_rate": 0.00015409523809523809,
      "loss": 3.0291,
      "step": 4820
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.201623797416687,
      "learning_rate": 0.000154,
      "loss": 2.8697,
      "step": 4830
    },
    {
      "epoch": 0.6914285714285714,
      "grad_norm": 1.268239974975586,
      "learning_rate": 0.00015390476190476192,
      "loss": 2.8807,
      "step": 4840
    },
    {
      "epoch": 0.6928571428571428,
      "grad_norm": 1.3012274503707886,
      "learning_rate": 0.00015380952380952384,
      "loss": 2.9566,
      "step": 4850
    },
    {
      "epoch": 0.6942857142857143,
      "grad_norm": 1.1316688060760498,
      "learning_rate": 0.00015371428571428573,
      "loss": 2.7204,
      "step": 4860
    },
    {
      "epoch": 0.6957142857142857,
      "grad_norm": 1.0844961404800415,
      "learning_rate": 0.00015361904761904762,
      "loss": 2.7895,
      "step": 4870
    },
    {
      "epoch": 0.6971428571428572,
      "grad_norm": 1.442114233970642,
      "learning_rate": 0.00015352380952380953,
      "loss": 2.9067,
      "step": 4880
    },
    {
      "epoch": 0.6985714285714286,
      "grad_norm": 1.1046265363693237,
      "learning_rate": 0.00015342857142857142,
      "loss": 2.7871,
      "step": 4890
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.321476936340332,
      "learning_rate": 0.00015333333333333334,
      "loss": 2.8261,
      "step": 4900
    },
    {
      "epoch": 0.7014285714285714,
      "grad_norm": 1.0466043949127197,
      "learning_rate": 0.00015323809523809525,
      "loss": 2.9945,
      "step": 4910
    },
    {
      "epoch": 0.7028571428571428,
      "grad_norm": 1.091812014579773,
      "learning_rate": 0.00015314285714285714,
      "loss": 2.8732,
      "step": 4920
    },
    {
      "epoch": 0.7042857142857143,
      "grad_norm": 1.6829196214675903,
      "learning_rate": 0.00015304761904761906,
      "loss": 2.9344,
      "step": 4930
    },
    {
      "epoch": 0.7057142857142857,
      "grad_norm": 1.325514316558838,
      "learning_rate": 0.00015295238095238095,
      "loss": 2.8446,
      "step": 4940
    },
    {
      "epoch": 0.7071428571428572,
      "grad_norm": 1.3468875885009766,
      "learning_rate": 0.00015285714285714287,
      "loss": 2.8476,
      "step": 4950
    },
    {
      "epoch": 0.7085714285714285,
      "grad_norm": 1.0404860973358154,
      "learning_rate": 0.00015276190476190478,
      "loss": 2.9349,
      "step": 4960
    },
    {
      "epoch": 0.71,
      "grad_norm": 1.0751681327819824,
      "learning_rate": 0.00015266666666666667,
      "loss": 2.8031,
      "step": 4970
    },
    {
      "epoch": 0.7114285714285714,
      "grad_norm": 1.4784313440322876,
      "learning_rate": 0.0001525714285714286,
      "loss": 3.0647,
      "step": 4980
    },
    {
      "epoch": 0.7128571428571429,
      "grad_norm": 0.8767918944358826,
      "learning_rate": 0.00015247619047619048,
      "loss": 2.9152,
      "step": 4990
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 5.454151630401611,
      "learning_rate": 0.00015238095238095237,
      "loss": 2.8248,
      "step": 5000
    },
    {
      "epoch": 0.7157142857142857,
      "grad_norm": 1.2614823579788208,
      "learning_rate": 0.00015228571428571429,
      "loss": 2.7987,
      "step": 5010
    },
    {
      "epoch": 0.7171428571428572,
      "grad_norm": 1.4999580383300781,
      "learning_rate": 0.0001521904761904762,
      "loss": 2.9537,
      "step": 5020
    },
    {
      "epoch": 0.7185714285714285,
      "grad_norm": 1.8654495477676392,
      "learning_rate": 0.00015209523809523812,
      "loss": 2.843,
      "step": 5030
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.7926710844039917,
      "learning_rate": 0.000152,
      "loss": 2.8668,
      "step": 5040
    },
    {
      "epoch": 0.7214285714285714,
      "grad_norm": 1.0009523630142212,
      "learning_rate": 0.0001519047619047619,
      "loss": 2.9982,
      "step": 5050
    },
    {
      "epoch": 0.7228571428571429,
      "grad_norm": 1.695811152458191,
      "learning_rate": 0.00015180952380952381,
      "loss": 2.8093,
      "step": 5060
    },
    {
      "epoch": 0.7242857142857143,
      "grad_norm": 1.25956392288208,
      "learning_rate": 0.00015171428571428573,
      "loss": 2.8087,
      "step": 5070
    },
    {
      "epoch": 0.7257142857142858,
      "grad_norm": 1.2851524353027344,
      "learning_rate": 0.00015161904761904762,
      "loss": 2.8358,
      "step": 5080
    },
    {
      "epoch": 0.7271428571428571,
      "grad_norm": 0.9720562696456909,
      "learning_rate": 0.00015152380952380954,
      "loss": 2.9275,
      "step": 5090
    },
    {
      "epoch": 0.7285714285714285,
      "grad_norm": 1.5098562240600586,
      "learning_rate": 0.00015142857142857143,
      "loss": 2.8767,
      "step": 5100
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.3269810676574707,
      "learning_rate": 0.00015133333333333334,
      "loss": 2.7644,
      "step": 5110
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 1.2011617422103882,
      "learning_rate": 0.00015123809523809523,
      "loss": 2.9186,
      "step": 5120
    },
    {
      "epoch": 0.7328571428571429,
      "grad_norm": 1.6258280277252197,
      "learning_rate": 0.00015114285714285715,
      "loss": 2.981,
      "step": 5130
    },
    {
      "epoch": 0.7342857142857143,
      "grad_norm": 1.9425921440124512,
      "learning_rate": 0.00015104761904761907,
      "loss": 2.7904,
      "step": 5140
    },
    {
      "epoch": 0.7357142857142858,
      "grad_norm": 0.9769445061683655,
      "learning_rate": 0.00015095238095238096,
      "loss": 3.0008,
      "step": 5150
    },
    {
      "epoch": 0.7371428571428571,
      "grad_norm": 1.2598364353179932,
      "learning_rate": 0.00015085714285714287,
      "loss": 2.8204,
      "step": 5160
    },
    {
      "epoch": 0.7385714285714285,
      "grad_norm": 1.0953192710876465,
      "learning_rate": 0.00015076190476190476,
      "loss": 2.786,
      "step": 5170
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.4250993728637695,
      "learning_rate": 0.00015066666666666668,
      "loss": 2.8363,
      "step": 5180
    },
    {
      "epoch": 0.7414285714285714,
      "grad_norm": 1.854576826095581,
      "learning_rate": 0.0001505714285714286,
      "loss": 2.8636,
      "step": 5190
    },
    {
      "epoch": 0.7428571428571429,
      "grad_norm": 1.2971478700637817,
      "learning_rate": 0.00015047619047619048,
      "loss": 2.8006,
      "step": 5200
    },
    {
      "epoch": 0.7442857142857143,
      "grad_norm": 1.3969488143920898,
      "learning_rate": 0.00015038095238095237,
      "loss": 2.8361,
      "step": 5210
    },
    {
      "epoch": 0.7457142857142857,
      "grad_norm": 1.398514747619629,
      "learning_rate": 0.0001502857142857143,
      "loss": 2.9835,
      "step": 5220
    },
    {
      "epoch": 0.7471428571428571,
      "grad_norm": 1.1264220476150513,
      "learning_rate": 0.00015019047619047618,
      "loss": 2.8003,
      "step": 5230
    },
    {
      "epoch": 0.7485714285714286,
      "grad_norm": 1.2860594987869263,
      "learning_rate": 0.00015009523809523812,
      "loss": 2.8315,
      "step": 5240
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.2075374126434326,
      "learning_rate": 0.00015000000000000001,
      "loss": 2.6743,
      "step": 5250
    },
    {
      "epoch": 0.7514285714285714,
      "grad_norm": 1.244480013847351,
      "learning_rate": 0.0001499047619047619,
      "loss": 2.8685,
      "step": 5260
    },
    {
      "epoch": 0.7528571428571429,
      "grad_norm": 1.1576811075210571,
      "learning_rate": 0.00014980952380952382,
      "loss": 2.8741,
      "step": 5270
    },
    {
      "epoch": 0.7542857142857143,
      "grad_norm": 1.5004576444625854,
      "learning_rate": 0.0001497142857142857,
      "loss": 2.8523,
      "step": 5280
    },
    {
      "epoch": 0.7557142857142857,
      "grad_norm": 1.2467247247695923,
      "learning_rate": 0.00014961904761904763,
      "loss": 2.9524,
      "step": 5290
    },
    {
      "epoch": 0.7571428571428571,
      "grad_norm": 1.364702820777893,
      "learning_rate": 0.00014952380952380954,
      "loss": 2.7189,
      "step": 5300
    },
    {
      "epoch": 0.7585714285714286,
      "grad_norm": 1.450811743736267,
      "learning_rate": 0.00014942857142857143,
      "loss": 2.9537,
      "step": 5310
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.0670298337936401,
      "learning_rate": 0.00014933333333333335,
      "loss": 2.8443,
      "step": 5320
    },
    {
      "epoch": 0.7614285714285715,
      "grad_norm": 2.0558128356933594,
      "learning_rate": 0.00014923809523809524,
      "loss": 2.792,
      "step": 5330
    },
    {
      "epoch": 0.7628571428571429,
      "grad_norm": 1.3265329599380493,
      "learning_rate": 0.00014914285714285713,
      "loss": 2.8058,
      "step": 5340
    },
    {
      "epoch": 0.7642857142857142,
      "grad_norm": 1.115767240524292,
      "learning_rate": 0.00014904761904761904,
      "loss": 2.899,
      "step": 5350
    },
    {
      "epoch": 0.7657142857142857,
      "grad_norm": 0.9377184510231018,
      "learning_rate": 0.00014895238095238096,
      "loss": 2.8344,
      "step": 5360
    },
    {
      "epoch": 0.7671428571428571,
      "grad_norm": 0.8576200008392334,
      "learning_rate": 0.00014885714285714288,
      "loss": 2.7939,
      "step": 5370
    },
    {
      "epoch": 0.7685714285714286,
      "grad_norm": 1.2521308660507202,
      "learning_rate": 0.00014876190476190477,
      "loss": 2.9016,
      "step": 5380
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.4487996101379395,
      "learning_rate": 0.00014866666666666666,
      "loss": 2.9736,
      "step": 5390
    },
    {
      "epoch": 0.7714285714285715,
      "grad_norm": 1.2659835815429688,
      "learning_rate": 0.00014857142857142857,
      "loss": 2.6249,
      "step": 5400
    },
    {
      "epoch": 0.7728571428571429,
      "grad_norm": 1.2956522703170776,
      "learning_rate": 0.0001484761904761905,
      "loss": 2.8238,
      "step": 5410
    },
    {
      "epoch": 0.7742857142857142,
      "grad_norm": 1.0214787721633911,
      "learning_rate": 0.0001483809523809524,
      "loss": 2.8481,
      "step": 5420
    },
    {
      "epoch": 0.7757142857142857,
      "grad_norm": 1.1120387315750122,
      "learning_rate": 0.0001482857142857143,
      "loss": 2.956,
      "step": 5430
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 1.0682028532028198,
      "learning_rate": 0.00014819047619047619,
      "loss": 2.9835,
      "step": 5440
    },
    {
      "epoch": 0.7785714285714286,
      "grad_norm": 0.9259040355682373,
      "learning_rate": 0.0001480952380952381,
      "loss": 2.6877,
      "step": 5450
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.2169959545135498,
      "learning_rate": 0.000148,
      "loss": 2.7049,
      "step": 5460
    },
    {
      "epoch": 0.7814285714285715,
      "grad_norm": 1.668939471244812,
      "learning_rate": 0.0001479047619047619,
      "loss": 2.8297,
      "step": 5470
    },
    {
      "epoch": 0.7828571428571428,
      "grad_norm": 1.1917798519134521,
      "learning_rate": 0.00014780952380952383,
      "loss": 2.7526,
      "step": 5480
    },
    {
      "epoch": 0.7842857142857143,
      "grad_norm": 1.1445261240005493,
      "learning_rate": 0.00014771428571428572,
      "loss": 2.8053,
      "step": 5490
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 1.4990959167480469,
      "learning_rate": 0.00014761904761904763,
      "loss": 3.0661,
      "step": 5500
    },
    {
      "epoch": 0.7871428571428571,
      "grad_norm": 1.2993427515029907,
      "learning_rate": 0.00014752380952380952,
      "loss": 2.809,
      "step": 5510
    },
    {
      "epoch": 0.7885714285714286,
      "grad_norm": 1.4557321071624756,
      "learning_rate": 0.00014742857142857144,
      "loss": 2.7315,
      "step": 5520
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.1163777112960815,
      "learning_rate": 0.00014733333333333335,
      "loss": 2.8577,
      "step": 5530
    },
    {
      "epoch": 0.7914285714285715,
      "grad_norm": 1.0268421173095703,
      "learning_rate": 0.00014723809523809524,
      "loss": 2.9112,
      "step": 5540
    },
    {
      "epoch": 0.7928571428571428,
      "grad_norm": 1.609740138053894,
      "learning_rate": 0.00014714285714285716,
      "loss": 2.6685,
      "step": 5550
    },
    {
      "epoch": 0.7942857142857143,
      "grad_norm": 1.0736021995544434,
      "learning_rate": 0.00014704761904761905,
      "loss": 2.8974,
      "step": 5560
    },
    {
      "epoch": 0.7957142857142857,
      "grad_norm": 1.1691110134124756,
      "learning_rate": 0.00014695238095238094,
      "loss": 2.7746,
      "step": 5570
    },
    {
      "epoch": 0.7971428571428572,
      "grad_norm": 1.2595528364181519,
      "learning_rate": 0.00014685714285714288,
      "loss": 2.758,
      "step": 5580
    },
    {
      "epoch": 0.7985714285714286,
      "grad_norm": 1.3865941762924194,
      "learning_rate": 0.00014676190476190477,
      "loss": 2.8501,
      "step": 5590
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1283589601516724,
      "learning_rate": 0.00014666666666666666,
      "loss": 2.8323,
      "step": 5600
    },
    {
      "epoch": 0.8014285714285714,
      "grad_norm": 1.2952278852462769,
      "learning_rate": 0.00014657142857142858,
      "loss": 2.8449,
      "step": 5610
    },
    {
      "epoch": 0.8028571428571428,
      "grad_norm": 1.4245134592056274,
      "learning_rate": 0.00014647619047619047,
      "loss": 2.8872,
      "step": 5620
    },
    {
      "epoch": 0.8042857142857143,
      "grad_norm": 1.4196048974990845,
      "learning_rate": 0.00014638095238095239,
      "loss": 2.7902,
      "step": 5630
    },
    {
      "epoch": 0.8057142857142857,
      "grad_norm": 1.3239487409591675,
      "learning_rate": 0.0001462857142857143,
      "loss": 2.9644,
      "step": 5640
    },
    {
      "epoch": 0.8071428571428572,
      "grad_norm": 1.3416141271591187,
      "learning_rate": 0.0001461904761904762,
      "loss": 2.8945,
      "step": 5650
    },
    {
      "epoch": 0.8085714285714286,
      "grad_norm": 1.3881601095199585,
      "learning_rate": 0.0001460952380952381,
      "loss": 2.7406,
      "step": 5660
    },
    {
      "epoch": 0.81,
      "grad_norm": 1.0667405128479004,
      "learning_rate": 0.000146,
      "loss": 2.8217,
      "step": 5670
    },
    {
      "epoch": 0.8114285714285714,
      "grad_norm": 1.424841284751892,
      "learning_rate": 0.00014590476190476191,
      "loss": 2.9414,
      "step": 5680
    },
    {
      "epoch": 0.8128571428571428,
      "grad_norm": 1.2123140096664429,
      "learning_rate": 0.00014580952380952383,
      "loss": 2.6649,
      "step": 5690
    },
    {
      "epoch": 0.8142857142857143,
      "grad_norm": 1.2684205770492554,
      "learning_rate": 0.00014571428571428572,
      "loss": 2.7681,
      "step": 5700
    },
    {
      "epoch": 0.8157142857142857,
      "grad_norm": 1.2469518184661865,
      "learning_rate": 0.00014561904761904764,
      "loss": 2.7807,
      "step": 5710
    },
    {
      "epoch": 0.8171428571428572,
      "grad_norm": 1.4458662271499634,
      "learning_rate": 0.00014552380952380953,
      "loss": 2.8181,
      "step": 5720
    },
    {
      "epoch": 0.8185714285714286,
      "grad_norm": 1.2208213806152344,
      "learning_rate": 0.00014542857142857142,
      "loss": 2.8565,
      "step": 5730
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.1547638177871704,
      "learning_rate": 0.00014533333333333333,
      "loss": 2.9243,
      "step": 5740
    },
    {
      "epoch": 0.8214285714285714,
      "grad_norm": 1.2191791534423828,
      "learning_rate": 0.00014523809523809525,
      "loss": 2.9023,
      "step": 5750
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 1.5605907440185547,
      "learning_rate": 0.00014514285714285717,
      "loss": 2.7932,
      "step": 5760
    },
    {
      "epoch": 0.8242857142857143,
      "grad_norm": 1.2177969217300415,
      "learning_rate": 0.00014504761904761906,
      "loss": 2.9561,
      "step": 5770
    },
    {
      "epoch": 0.8257142857142857,
      "grad_norm": 1.75364351272583,
      "learning_rate": 0.00014495238095238095,
      "loss": 2.8863,
      "step": 5780
    },
    {
      "epoch": 0.8271428571428572,
      "grad_norm": 1.4849399328231812,
      "learning_rate": 0.00014485714285714286,
      "loss": 2.8612,
      "step": 5790
    },
    {
      "epoch": 0.8285714285714286,
      "grad_norm": 1.4054611921310425,
      "learning_rate": 0.00014476190476190475,
      "loss": 2.7942,
      "step": 5800
    },
    {
      "epoch": 0.83,
      "grad_norm": 1.3374720811843872,
      "learning_rate": 0.0001446666666666667,
      "loss": 2.866,
      "step": 5810
    },
    {
      "epoch": 0.8314285714285714,
      "grad_norm": 0.9926617741584778,
      "learning_rate": 0.00014457142857142859,
      "loss": 2.9589,
      "step": 5820
    },
    {
      "epoch": 0.8328571428571429,
      "grad_norm": 1.366747498512268,
      "learning_rate": 0.00014447619047619047,
      "loss": 2.724,
      "step": 5830
    },
    {
      "epoch": 0.8342857142857143,
      "grad_norm": 1.7366769313812256,
      "learning_rate": 0.0001443809523809524,
      "loss": 2.7509,
      "step": 5840
    },
    {
      "epoch": 0.8357142857142857,
      "grad_norm": 1.181329607963562,
      "learning_rate": 0.00014428571428571428,
      "loss": 2.8757,
      "step": 5850
    },
    {
      "epoch": 0.8371428571428572,
      "grad_norm": 1.4321492910385132,
      "learning_rate": 0.0001441904761904762,
      "loss": 2.8888,
      "step": 5860
    },
    {
      "epoch": 0.8385714285714285,
      "grad_norm": 1.33622145652771,
      "learning_rate": 0.00014409523809523811,
      "loss": 2.941,
      "step": 5870
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.445504069328308,
      "learning_rate": 0.000144,
      "loss": 2.8737,
      "step": 5880
    },
    {
      "epoch": 0.8414285714285714,
      "grad_norm": 1.0961320400238037,
      "learning_rate": 0.00014390476190476192,
      "loss": 2.7305,
      "step": 5890
    },
    {
      "epoch": 0.8428571428571429,
      "grad_norm": 1.1415250301361084,
      "learning_rate": 0.0001438095238095238,
      "loss": 2.8134,
      "step": 5900
    },
    {
      "epoch": 0.8442857142857143,
      "grad_norm": 1.3441964387893677,
      "learning_rate": 0.0001437142857142857,
      "loss": 2.8858,
      "step": 5910
    },
    {
      "epoch": 0.8457142857142858,
      "grad_norm": 1.635422706604004,
      "learning_rate": 0.00014361904761904764,
      "loss": 2.8762,
      "step": 5920
    },
    {
      "epoch": 0.8471428571428572,
      "grad_norm": 1.4055299758911133,
      "learning_rate": 0.00014352380952380953,
      "loss": 2.8067,
      "step": 5930
    },
    {
      "epoch": 0.8485714285714285,
      "grad_norm": 1.2633771896362305,
      "learning_rate": 0.00014342857142857145,
      "loss": 2.7799,
      "step": 5940
    },
    {
      "epoch": 0.85,
      "grad_norm": 1.0727112293243408,
      "learning_rate": 0.00014333333333333334,
      "loss": 2.8433,
      "step": 5950
    },
    {
      "epoch": 0.8514285714285714,
      "grad_norm": 1.2155529260635376,
      "learning_rate": 0.00014323809523809523,
      "loss": 2.8299,
      "step": 5960
    },
    {
      "epoch": 0.8528571428571429,
      "grad_norm": 1.610908031463623,
      "learning_rate": 0.00014314285714285715,
      "loss": 2.7786,
      "step": 5970
    },
    {
      "epoch": 0.8542857142857143,
      "grad_norm": 1.3101252317428589,
      "learning_rate": 0.00014304761904761906,
      "loss": 2.8793,
      "step": 5980
    },
    {
      "epoch": 0.8557142857142858,
      "grad_norm": 1.4224251508712769,
      "learning_rate": 0.00014295238095238095,
      "loss": 2.763,
      "step": 5990
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 1.481335163116455,
      "learning_rate": 0.00014285714285714287,
      "loss": 2.7439,
      "step": 6000
    },
    {
      "epoch": 0.8585714285714285,
      "grad_norm": 1.1901417970657349,
      "learning_rate": 0.00014276190476190476,
      "loss": 2.8433,
      "step": 6010
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.0966956615447998,
      "learning_rate": 0.00014266666666666667,
      "loss": 2.7966,
      "step": 6020
    },
    {
      "epoch": 0.8614285714285714,
      "grad_norm": 1.0225729942321777,
      "learning_rate": 0.0001425714285714286,
      "loss": 2.8018,
      "step": 6030
    },
    {
      "epoch": 0.8628571428571429,
      "grad_norm": 0.9936888217926025,
      "learning_rate": 0.00014247619047619048,
      "loss": 2.6785,
      "step": 6040
    },
    {
      "epoch": 0.8642857142857143,
      "grad_norm": 1.2509649991989136,
      "learning_rate": 0.0001423809523809524,
      "loss": 2.8774,
      "step": 6050
    },
    {
      "epoch": 0.8657142857142858,
      "grad_norm": 1.0474791526794434,
      "learning_rate": 0.00014228571428571429,
      "loss": 2.7805,
      "step": 6060
    },
    {
      "epoch": 0.8671428571428571,
      "grad_norm": 1.1222772598266602,
      "learning_rate": 0.0001421904761904762,
      "loss": 2.9513,
      "step": 6070
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 1.1130006313323975,
      "learning_rate": 0.0001420952380952381,
      "loss": 2.8623,
      "step": 6080
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.097428560256958,
      "learning_rate": 0.000142,
      "loss": 2.7151,
      "step": 6090
    },
    {
      "epoch": 0.8714285714285714,
      "grad_norm": 1.1543492078781128,
      "learning_rate": 0.00014190476190476193,
      "loss": 2.6987,
      "step": 6100
    },
    {
      "epoch": 0.8728571428571429,
      "grad_norm": 1.1674929857254028,
      "learning_rate": 0.00014180952380952382,
      "loss": 2.8141,
      "step": 6110
    },
    {
      "epoch": 0.8742857142857143,
      "grad_norm": 1.2648875713348389,
      "learning_rate": 0.0001417142857142857,
      "loss": 2.8952,
      "step": 6120
    },
    {
      "epoch": 0.8757142857142857,
      "grad_norm": 1.4871153831481934,
      "learning_rate": 0.00014161904761904762,
      "loss": 2.8447,
      "step": 6130
    },
    {
      "epoch": 0.8771428571428571,
      "grad_norm": 1.2240465879440308,
      "learning_rate": 0.00014152380952380954,
      "loss": 3.0039,
      "step": 6140
    },
    {
      "epoch": 0.8785714285714286,
      "grad_norm": 1.4502092599868774,
      "learning_rate": 0.00014142857142857145,
      "loss": 2.883,
      "step": 6150
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.2952581644058228,
      "learning_rate": 0.00014133333333333334,
      "loss": 2.8372,
      "step": 6160
    },
    {
      "epoch": 0.8814285714285715,
      "grad_norm": 1.5253276824951172,
      "learning_rate": 0.00014123809523809523,
      "loss": 2.8886,
      "step": 6170
    },
    {
      "epoch": 0.8828571428571429,
      "grad_norm": 1.1510529518127441,
      "learning_rate": 0.00014114285714285715,
      "loss": 2.8624,
      "step": 6180
    },
    {
      "epoch": 0.8842857142857142,
      "grad_norm": 1.142781138420105,
      "learning_rate": 0.00014104761904761904,
      "loss": 2.8236,
      "step": 6190
    },
    {
      "epoch": 0.8857142857142857,
      "grad_norm": 1.5409331321716309,
      "learning_rate": 0.00014095238095238096,
      "loss": 2.8963,
      "step": 6200
    },
    {
      "epoch": 0.8871428571428571,
      "grad_norm": 1.1564768552780151,
      "learning_rate": 0.00014085714285714287,
      "loss": 2.627,
      "step": 6210
    },
    {
      "epoch": 0.8885714285714286,
      "grad_norm": 0.949082612991333,
      "learning_rate": 0.00014076190476190476,
      "loss": 2.8072,
      "step": 6220
    },
    {
      "epoch": 0.89,
      "grad_norm": 1.3013957738876343,
      "learning_rate": 0.00014066666666666668,
      "loss": 2.8145,
      "step": 6230
    },
    {
      "epoch": 0.8914285714285715,
      "grad_norm": 1.1022909879684448,
      "learning_rate": 0.00014057142857142857,
      "loss": 2.8267,
      "step": 6240
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 1.434772253036499,
      "learning_rate": 0.00014047619047619049,
      "loss": 2.8393,
      "step": 6250
    },
    {
      "epoch": 0.8942857142857142,
      "grad_norm": 1.187008023262024,
      "learning_rate": 0.0001403809523809524,
      "loss": 2.7036,
      "step": 6260
    },
    {
      "epoch": 0.8957142857142857,
      "grad_norm": 1.2946559190750122,
      "learning_rate": 0.0001402857142857143,
      "loss": 2.6611,
      "step": 6270
    },
    {
      "epoch": 0.8971428571428571,
      "grad_norm": 1.4302656650543213,
      "learning_rate": 0.0001401904761904762,
      "loss": 2.7295,
      "step": 6280
    },
    {
      "epoch": 0.8985714285714286,
      "grad_norm": 1.4181841611862183,
      "learning_rate": 0.0001400952380952381,
      "loss": 2.8806,
      "step": 6290
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.0960652828216553,
      "learning_rate": 0.00014,
      "loss": 2.9613,
      "step": 6300
    },
    {
      "epoch": 0.9014285714285715,
      "grad_norm": 1.1182255744934082,
      "learning_rate": 0.0001399047619047619,
      "loss": 2.8459,
      "step": 6310
    },
    {
      "epoch": 0.9028571428571428,
      "grad_norm": 1.301388144493103,
      "learning_rate": 0.00013980952380952382,
      "loss": 2.7515,
      "step": 6320
    },
    {
      "epoch": 0.9042857142857142,
      "grad_norm": 1.4294263124465942,
      "learning_rate": 0.00013971428571428574,
      "loss": 2.8207,
      "step": 6330
    },
    {
      "epoch": 0.9057142857142857,
      "grad_norm": 1.062596082687378,
      "learning_rate": 0.00013961904761904763,
      "loss": 2.9703,
      "step": 6340
    },
    {
      "epoch": 0.9071428571428571,
      "grad_norm": 1.4856069087982178,
      "learning_rate": 0.00013952380952380952,
      "loss": 3.0152,
      "step": 6350
    },
    {
      "epoch": 0.9085714285714286,
      "grad_norm": 1.4696435928344727,
      "learning_rate": 0.00013942857142857143,
      "loss": 2.8677,
      "step": 6360
    },
    {
      "epoch": 0.91,
      "grad_norm": 1.2890799045562744,
      "learning_rate": 0.00013933333333333335,
      "loss": 2.7477,
      "step": 6370
    },
    {
      "epoch": 0.9114285714285715,
      "grad_norm": 1.1824378967285156,
      "learning_rate": 0.00013923809523809524,
      "loss": 2.8212,
      "step": 6380
    },
    {
      "epoch": 0.9128571428571428,
      "grad_norm": 1.191508173942566,
      "learning_rate": 0.00013914285714285716,
      "loss": 2.6686,
      "step": 6390
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 1.754761815071106,
      "learning_rate": 0.00013904761904761905,
      "loss": 2.8004,
      "step": 6400
    },
    {
      "epoch": 0.9157142857142857,
      "grad_norm": 1.2688628435134888,
      "learning_rate": 0.00013895238095238096,
      "loss": 2.7132,
      "step": 6410
    },
    {
      "epoch": 0.9171428571428571,
      "grad_norm": 1.269467830657959,
      "learning_rate": 0.00013885714285714285,
      "loss": 2.8354,
      "step": 6420
    },
    {
      "epoch": 0.9185714285714286,
      "grad_norm": 1.5930880308151245,
      "learning_rate": 0.00013876190476190477,
      "loss": 2.9161,
      "step": 6430
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.0760284662246704,
      "learning_rate": 0.00013866666666666669,
      "loss": 2.9081,
      "step": 6440
    },
    {
      "epoch": 0.9214285714285714,
      "grad_norm": 1.5754148960113525,
      "learning_rate": 0.00013857142857142857,
      "loss": 2.8036,
      "step": 6450
    },
    {
      "epoch": 0.9228571428571428,
      "grad_norm": 1.2305349111557007,
      "learning_rate": 0.0001384761904761905,
      "loss": 2.8551,
      "step": 6460
    },
    {
      "epoch": 0.9242857142857143,
      "grad_norm": 1.1736735105514526,
      "learning_rate": 0.00013838095238095238,
      "loss": 2.8764,
      "step": 6470
    },
    {
      "epoch": 0.9257142857142857,
      "grad_norm": 1.6659575700759888,
      "learning_rate": 0.0001382857142857143,
      "loss": 2.8012,
      "step": 6480
    },
    {
      "epoch": 0.9271428571428572,
      "grad_norm": 1.1029011011123657,
      "learning_rate": 0.00013819047619047621,
      "loss": 2.8559,
      "step": 6490
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 0.9259306192398071,
      "learning_rate": 0.0001380952380952381,
      "loss": 2.8868,
      "step": 6500
    },
    {
      "epoch": 0.93,
      "grad_norm": 1.3493329286575317,
      "learning_rate": 0.000138,
      "loss": 2.7252,
      "step": 6510
    },
    {
      "epoch": 0.9314285714285714,
      "grad_norm": 1.1626653671264648,
      "learning_rate": 0.0001379047619047619,
      "loss": 2.9449,
      "step": 6520
    },
    {
      "epoch": 0.9328571428571428,
      "grad_norm": 1.2825437784194946,
      "learning_rate": 0.0001378095238095238,
      "loss": 2.702,
      "step": 6530
    },
    {
      "epoch": 0.9342857142857143,
      "grad_norm": 1.3350776433944702,
      "learning_rate": 0.00013771428571428572,
      "loss": 2.8271,
      "step": 6540
    },
    {
      "epoch": 0.9357142857142857,
      "grad_norm": 1.0967780351638794,
      "learning_rate": 0.00013761904761904763,
      "loss": 2.9317,
      "step": 6550
    },
    {
      "epoch": 0.9371428571428572,
      "grad_norm": 1.0078481435775757,
      "learning_rate": 0.00013752380952380952,
      "loss": 2.8947,
      "step": 6560
    },
    {
      "epoch": 0.9385714285714286,
      "grad_norm": 1.4899468421936035,
      "learning_rate": 0.00013742857142857144,
      "loss": 2.9727,
      "step": 6570
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.0908693075180054,
      "learning_rate": 0.00013733333333333333,
      "loss": 2.7644,
      "step": 6580
    },
    {
      "epoch": 0.9414285714285714,
      "grad_norm": 1.1023846864700317,
      "learning_rate": 0.00013723809523809525,
      "loss": 2.6338,
      "step": 6590
    },
    {
      "epoch": 0.9428571428571428,
      "grad_norm": 3.216613531112671,
      "learning_rate": 0.00013714285714285716,
      "loss": 2.8153,
      "step": 6600
    },
    {
      "epoch": 0.9442857142857143,
      "grad_norm": 1.0018752813339233,
      "learning_rate": 0.00013704761904761905,
      "loss": 2.8056,
      "step": 6610
    },
    {
      "epoch": 0.9457142857142857,
      "grad_norm": 1.2958424091339111,
      "learning_rate": 0.00013695238095238097,
      "loss": 2.8796,
      "step": 6620
    },
    {
      "epoch": 0.9471428571428572,
      "grad_norm": 1.0137919187545776,
      "learning_rate": 0.00013685714285714286,
      "loss": 2.8584,
      "step": 6630
    },
    {
      "epoch": 0.9485714285714286,
      "grad_norm": 1.4135098457336426,
      "learning_rate": 0.00013676190476190475,
      "loss": 2.7707,
      "step": 6640
    },
    {
      "epoch": 0.95,
      "grad_norm": 1.3266407251358032,
      "learning_rate": 0.00013666666666666666,
      "loss": 2.7863,
      "step": 6650
    },
    {
      "epoch": 0.9514285714285714,
      "grad_norm": 1.1715264320373535,
      "learning_rate": 0.00013657142857142858,
      "loss": 2.6918,
      "step": 6660
    },
    {
      "epoch": 0.9528571428571428,
      "grad_norm": 1.5324592590332031,
      "learning_rate": 0.0001364761904761905,
      "loss": 2.8843,
      "step": 6670
    },
    {
      "epoch": 0.9542857142857143,
      "grad_norm": 1.2074185609817505,
      "learning_rate": 0.0001363809523809524,
      "loss": 2.9598,
      "step": 6680
    },
    {
      "epoch": 0.9557142857142857,
      "grad_norm": 1.253141164779663,
      "learning_rate": 0.00013628571428571428,
      "loss": 2.9493,
      "step": 6690
    },
    {
      "epoch": 0.9571428571428572,
      "grad_norm": 3.295403003692627,
      "learning_rate": 0.0001361904761904762,
      "loss": 2.8463,
      "step": 6700
    },
    {
      "epoch": 0.9585714285714285,
      "grad_norm": 1.0773382186889648,
      "learning_rate": 0.0001360952380952381,
      "loss": 2.6984,
      "step": 6710
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.459294319152832,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.7281,
      "step": 6720
    },
    {
      "epoch": 0.9614285714285714,
      "grad_norm": 1.6932427883148193,
      "learning_rate": 0.00013590476190476192,
      "loss": 2.8519,
      "step": 6730
    },
    {
      "epoch": 0.9628571428571429,
      "grad_norm": 1.2256361246109009,
      "learning_rate": 0.0001358095238095238,
      "loss": 2.9157,
      "step": 6740
    },
    {
      "epoch": 0.9642857142857143,
      "grad_norm": 1.4856635332107544,
      "learning_rate": 0.00013571428571428572,
      "loss": 2.669,
      "step": 6750
    },
    {
      "epoch": 0.9657142857142857,
      "grad_norm": 1.010105013847351,
      "learning_rate": 0.0001356190476190476,
      "loss": 2.7669,
      "step": 6760
    },
    {
      "epoch": 0.9671428571428572,
      "grad_norm": 1.0739253759384155,
      "learning_rate": 0.00013552380952380953,
      "loss": 2.9179,
      "step": 6770
    },
    {
      "epoch": 0.9685714285714285,
      "grad_norm": 0.9050179123878479,
      "learning_rate": 0.00013542857142857144,
      "loss": 2.8366,
      "step": 6780
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.43197500705719,
      "learning_rate": 0.00013533333333333333,
      "loss": 2.7148,
      "step": 6790
    },
    {
      "epoch": 0.9714285714285714,
      "grad_norm": 1.1641030311584473,
      "learning_rate": 0.00013523809523809525,
      "loss": 2.6766,
      "step": 6800
    },
    {
      "epoch": 0.9728571428571429,
      "grad_norm": 1.2229371070861816,
      "learning_rate": 0.00013514285714285714,
      "loss": 2.8872,
      "step": 6810
    },
    {
      "epoch": 0.9742857142857143,
      "grad_norm": 1.1534150838851929,
      "learning_rate": 0.00013504761904761906,
      "loss": 2.6501,
      "step": 6820
    },
    {
      "epoch": 0.9757142857142858,
      "grad_norm": 0.9191401600837708,
      "learning_rate": 0.00013495238095238097,
      "loss": 2.8203,
      "step": 6830
    },
    {
      "epoch": 0.9771428571428571,
      "grad_norm": 1.4768162965774536,
      "learning_rate": 0.00013485714285714286,
      "loss": 2.877,
      "step": 6840
    },
    {
      "epoch": 0.9785714285714285,
      "grad_norm": 0.9741953611373901,
      "learning_rate": 0.00013476190476190478,
      "loss": 2.7831,
      "step": 6850
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.1143403053283691,
      "learning_rate": 0.00013466666666666667,
      "loss": 2.9048,
      "step": 6860
    },
    {
      "epoch": 0.9814285714285714,
      "grad_norm": 1.0846911668777466,
      "learning_rate": 0.00013457142857142856,
      "loss": 2.8144,
      "step": 6870
    },
    {
      "epoch": 0.9828571428571429,
      "grad_norm": 1.1118342876434326,
      "learning_rate": 0.00013447619047619048,
      "loss": 2.6762,
      "step": 6880
    },
    {
      "epoch": 0.9842857142857143,
      "grad_norm": 1.2779923677444458,
      "learning_rate": 0.0001343809523809524,
      "loss": 2.7356,
      "step": 6890
    },
    {
      "epoch": 0.9857142857142858,
      "grad_norm": 1.6424239873886108,
      "learning_rate": 0.00013428571428571428,
      "loss": 2.8371,
      "step": 6900
    },
    {
      "epoch": 0.9871428571428571,
      "grad_norm": 1.4341351985931396,
      "learning_rate": 0.0001341904761904762,
      "loss": 2.819,
      "step": 6910
    },
    {
      "epoch": 0.9885714285714285,
      "grad_norm": 1.2274163961410522,
      "learning_rate": 0.0001340952380952381,
      "loss": 2.9184,
      "step": 6920
    },
    {
      "epoch": 0.99,
      "grad_norm": 1.2656395435333252,
      "learning_rate": 0.000134,
      "loss": 2.8587,
      "step": 6930
    },
    {
      "epoch": 0.9914285714285714,
      "grad_norm": 1.104356050491333,
      "learning_rate": 0.00013390476190476192,
      "loss": 2.8257,
      "step": 6940
    },
    {
      "epoch": 0.9928571428571429,
      "grad_norm": 0.9355513453483582,
      "learning_rate": 0.0001338095238095238,
      "loss": 2.7989,
      "step": 6950
    },
    {
      "epoch": 0.9942857142857143,
      "grad_norm": 1.568335771560669,
      "learning_rate": 0.00013371428571428573,
      "loss": 2.7678,
      "step": 6960
    },
    {
      "epoch": 0.9957142857142857,
      "grad_norm": 1.0829200744628906,
      "learning_rate": 0.00013361904761904762,
      "loss": 2.8675,
      "step": 6970
    },
    {
      "epoch": 0.9971428571428571,
      "grad_norm": 1.0553884506225586,
      "learning_rate": 0.00013352380952380953,
      "loss": 2.8838,
      "step": 6980
    },
    {
      "epoch": 0.9985714285714286,
      "grad_norm": 0.94920814037323,
      "learning_rate": 0.00013342857142857142,
      "loss": 2.7832,
      "step": 6990
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.1885173320770264,
      "learning_rate": 0.00013333333333333334,
      "loss": 2.6992,
      "step": 7000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.604564905166626,
      "eval_rouge1": 0.13237920161818373,
      "eval_rouge2": 0.03239719316536935,
      "eval_rougeL": 0.10122404110357011,
      "eval_rougeLsum": 0.12687580806923762,
      "eval_runtime": 1728.5121,
      "eval_samples_per_second": 13.885,
      "eval_steps_per_second": 3.471,
      "step": 7000
    },
    {
      "epoch": 1.0014285714285713,
      "grad_norm": 1.0225909948349,
      "learning_rate": 0.00013323809523809526,
      "loss": 2.8127,
      "step": 7010
    },
    {
      "epoch": 1.002857142857143,
      "grad_norm": 1.1320831775665283,
      "learning_rate": 0.00013314285714285715,
      "loss": 2.7596,
      "step": 7020
    },
    {
      "epoch": 1.0042857142857142,
      "grad_norm": 1.4246623516082764,
      "learning_rate": 0.00013304761904761904,
      "loss": 2.7667,
      "step": 7030
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 1.0482349395751953,
      "learning_rate": 0.00013295238095238095,
      "loss": 2.9428,
      "step": 7040
    },
    {
      "epoch": 1.0071428571428571,
      "grad_norm": 1.3110545873641968,
      "learning_rate": 0.00013285714285714287,
      "loss": 2.7825,
      "step": 7050
    },
    {
      "epoch": 1.0085714285714287,
      "grad_norm": 1.1535680294036865,
      "learning_rate": 0.00013276190476190479,
      "loss": 2.8895,
      "step": 7060
    },
    {
      "epoch": 1.01,
      "grad_norm": 1.1833161115646362,
      "learning_rate": 0.00013266666666666667,
      "loss": 2.8306,
      "step": 7070
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 1.2520745992660522,
      "learning_rate": 0.00013257142857142856,
      "loss": 2.7024,
      "step": 7080
    },
    {
      "epoch": 1.012857142857143,
      "grad_norm": 1.371665120124817,
      "learning_rate": 0.00013247619047619048,
      "loss": 2.7527,
      "step": 7090
    },
    {
      "epoch": 1.0142857142857142,
      "grad_norm": 1.3524760007858276,
      "learning_rate": 0.00013238095238095237,
      "loss": 2.676,
      "step": 7100
    },
    {
      "epoch": 1.0157142857142858,
      "grad_norm": 1.3382784128189087,
      "learning_rate": 0.00013228571428571431,
      "loss": 2.8011,
      "step": 7110
    },
    {
      "epoch": 1.0171428571428571,
      "grad_norm": 1.3938060998916626,
      "learning_rate": 0.0001321904761904762,
      "loss": 2.6698,
      "step": 7120
    },
    {
      "epoch": 1.0185714285714287,
      "grad_norm": 1.021743893623352,
      "learning_rate": 0.0001320952380952381,
      "loss": 2.8071,
      "step": 7130
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.1322286128997803,
      "learning_rate": 0.000132,
      "loss": 2.9161,
      "step": 7140
    },
    {
      "epoch": 1.0214285714285714,
      "grad_norm": 1.2151732444763184,
      "learning_rate": 0.0001319047619047619,
      "loss": 2.7947,
      "step": 7150
    },
    {
      "epoch": 1.022857142857143,
      "grad_norm": 1.1710301637649536,
      "learning_rate": 0.00013180952380952382,
      "loss": 2.8231,
      "step": 7160
    },
    {
      "epoch": 1.0242857142857142,
      "grad_norm": 1.2761696577072144,
      "learning_rate": 0.00013171428571428573,
      "loss": 2.8158,
      "step": 7170
    },
    {
      "epoch": 1.0257142857142858,
      "grad_norm": 1.046455979347229,
      "learning_rate": 0.00013161904761904762,
      "loss": 2.9028,
      "step": 7180
    },
    {
      "epoch": 1.0271428571428571,
      "grad_norm": 1.0873985290527344,
      "learning_rate": 0.00013152380952380954,
      "loss": 2.8408,
      "step": 7190
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 1.069657564163208,
      "learning_rate": 0.00013142857142857143,
      "loss": 2.7732,
      "step": 7200
    },
    {
      "epoch": 1.03,
      "grad_norm": 1.0478553771972656,
      "learning_rate": 0.00013133333333333332,
      "loss": 2.8999,
      "step": 7210
    },
    {
      "epoch": 1.0314285714285714,
      "grad_norm": 1.1073585748672485,
      "learning_rate": 0.00013123809523809526,
      "loss": 2.9107,
      "step": 7220
    },
    {
      "epoch": 1.032857142857143,
      "grad_norm": 1.6067869663238525,
      "learning_rate": 0.00013114285714285715,
      "loss": 2.6005,
      "step": 7230
    },
    {
      "epoch": 1.0342857142857143,
      "grad_norm": 1.8832536935806274,
      "learning_rate": 0.00013104761904761907,
      "loss": 2.9207,
      "step": 7240
    },
    {
      "epoch": 1.0357142857142858,
      "grad_norm": 1.3911871910095215,
      "learning_rate": 0.00013095238095238096,
      "loss": 2.5319,
      "step": 7250
    },
    {
      "epoch": 1.0371428571428571,
      "grad_norm": 1.1062452793121338,
      "learning_rate": 0.00013085714285714285,
      "loss": 2.8199,
      "step": 7260
    },
    {
      "epoch": 1.0385714285714285,
      "grad_norm": 1.167969822883606,
      "learning_rate": 0.00013076190476190476,
      "loss": 2.7994,
      "step": 7270
    },
    {
      "epoch": 1.04,
      "grad_norm": 1.2696810960769653,
      "learning_rate": 0.00013066666666666668,
      "loss": 2.7834,
      "step": 7280
    },
    {
      "epoch": 1.0414285714285714,
      "grad_norm": 1.5488206148147583,
      "learning_rate": 0.00013057142857142857,
      "loss": 2.9163,
      "step": 7290
    },
    {
      "epoch": 1.042857142857143,
      "grad_norm": 0.9573619961738586,
      "learning_rate": 0.0001304761904761905,
      "loss": 2.8057,
      "step": 7300
    },
    {
      "epoch": 1.0442857142857143,
      "grad_norm": 1.0431493520736694,
      "learning_rate": 0.00013038095238095238,
      "loss": 2.8314,
      "step": 7310
    },
    {
      "epoch": 1.0457142857142858,
      "grad_norm": 1.3818529844284058,
      "learning_rate": 0.0001302857142857143,
      "loss": 2.8331,
      "step": 7320
    },
    {
      "epoch": 1.0471428571428572,
      "grad_norm": 1.2304730415344238,
      "learning_rate": 0.0001301904761904762,
      "loss": 2.8408,
      "step": 7330
    },
    {
      "epoch": 1.0485714285714285,
      "grad_norm": 1.3572335243225098,
      "learning_rate": 0.0001300952380952381,
      "loss": 2.6901,
      "step": 7340
    },
    {
      "epoch": 1.05,
      "grad_norm": 1.4940792322158813,
      "learning_rate": 0.00013000000000000002,
      "loss": 2.7793,
      "step": 7350
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 1.5520615577697754,
      "learning_rate": 0.0001299047619047619,
      "loss": 2.7066,
      "step": 7360
    },
    {
      "epoch": 1.052857142857143,
      "grad_norm": 1.6301665306091309,
      "learning_rate": 0.00012980952380952382,
      "loss": 2.8122,
      "step": 7370
    },
    {
      "epoch": 1.0542857142857143,
      "grad_norm": 1.0244295597076416,
      "learning_rate": 0.0001297142857142857,
      "loss": 2.948,
      "step": 7380
    },
    {
      "epoch": 1.0557142857142856,
      "grad_norm": 0.960395336151123,
      "learning_rate": 0.00012961904761904763,
      "loss": 2.8639,
      "step": 7390
    },
    {
      "epoch": 1.0571428571428572,
      "grad_norm": 0.8716686964035034,
      "learning_rate": 0.00012952380952380954,
      "loss": 2.683,
      "step": 7400
    },
    {
      "epoch": 1.0585714285714285,
      "grad_norm": 1.265343189239502,
      "learning_rate": 0.00012942857142857143,
      "loss": 2.849,
      "step": 7410
    },
    {
      "epoch": 1.06,
      "grad_norm": 1.191397786140442,
      "learning_rate": 0.00012933333333333332,
      "loss": 2.8325,
      "step": 7420
    },
    {
      "epoch": 1.0614285714285714,
      "grad_norm": 1.4034442901611328,
      "learning_rate": 0.00012923809523809524,
      "loss": 2.6459,
      "step": 7430
    },
    {
      "epoch": 1.062857142857143,
      "grad_norm": 1.3674472570419312,
      "learning_rate": 0.00012914285714285713,
      "loss": 2.7506,
      "step": 7440
    },
    {
      "epoch": 1.0642857142857143,
      "grad_norm": 1.1243760585784912,
      "learning_rate": 0.00012904761904761907,
      "loss": 2.8642,
      "step": 7450
    },
    {
      "epoch": 1.0657142857142856,
      "grad_norm": 1.0530822277069092,
      "learning_rate": 0.00012895238095238096,
      "loss": 2.706,
      "step": 7460
    },
    {
      "epoch": 1.0671428571428572,
      "grad_norm": 1.3100217580795288,
      "learning_rate": 0.00012885714285714285,
      "loss": 2.6222,
      "step": 7470
    },
    {
      "epoch": 1.0685714285714285,
      "grad_norm": 1.5588761568069458,
      "learning_rate": 0.00012876190476190477,
      "loss": 2.7901,
      "step": 7480
    },
    {
      "epoch": 1.07,
      "grad_norm": 1.0689729452133179,
      "learning_rate": 0.00012866666666666666,
      "loss": 2.7267,
      "step": 7490
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.8042076230049133,
      "learning_rate": 0.00012857142857142858,
      "loss": 2.7788,
      "step": 7500
    },
    {
      "epoch": 1.072857142857143,
      "grad_norm": 0.97856605052948,
      "learning_rate": 0.0001284761904761905,
      "loss": 2.9789,
      "step": 7510
    },
    {
      "epoch": 1.0742857142857143,
      "grad_norm": 1.2614974975585938,
      "learning_rate": 0.00012838095238095238,
      "loss": 2.841,
      "step": 7520
    },
    {
      "epoch": 1.0757142857142856,
      "grad_norm": 1.2289990186691284,
      "learning_rate": 0.0001282857142857143,
      "loss": 2.6618,
      "step": 7530
    },
    {
      "epoch": 1.0771428571428572,
      "grad_norm": 1.2967530488967896,
      "learning_rate": 0.0001281904761904762,
      "loss": 2.7155,
      "step": 7540
    },
    {
      "epoch": 1.0785714285714285,
      "grad_norm": 1.303149700164795,
      "learning_rate": 0.00012809523809523808,
      "loss": 2.8229,
      "step": 7550
    },
    {
      "epoch": 1.08,
      "grad_norm": 1.1271448135375977,
      "learning_rate": 0.00012800000000000002,
      "loss": 2.5897,
      "step": 7560
    },
    {
      "epoch": 1.0814285714285714,
      "grad_norm": 1.139805793762207,
      "learning_rate": 0.0001279047619047619,
      "loss": 2.7682,
      "step": 7570
    },
    {
      "epoch": 1.0828571428571427,
      "grad_norm": 0.9420895576477051,
      "learning_rate": 0.00012780952380952383,
      "loss": 2.9181,
      "step": 7580
    },
    {
      "epoch": 1.0842857142857143,
      "grad_norm": 1.3325061798095703,
      "learning_rate": 0.00012771428571428572,
      "loss": 2.7299,
      "step": 7590
    },
    {
      "epoch": 1.0857142857142856,
      "grad_norm": 2.0020925998687744,
      "learning_rate": 0.0001276190476190476,
      "loss": 2.6689,
      "step": 7600
    },
    {
      "epoch": 1.0871428571428572,
      "grad_norm": 1.727530598640442,
      "learning_rate": 0.00012752380952380952,
      "loss": 2.7182,
      "step": 7610
    },
    {
      "epoch": 1.0885714285714285,
      "grad_norm": 1.2957675457000732,
      "learning_rate": 0.00012742857142857144,
      "loss": 2.8404,
      "step": 7620
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.8593711256980896,
      "learning_rate": 0.00012733333333333336,
      "loss": 2.8838,
      "step": 7630
    },
    {
      "epoch": 1.0914285714285714,
      "grad_norm": 1.1202551126480103,
      "learning_rate": 0.00012723809523809525,
      "loss": 2.9288,
      "step": 7640
    },
    {
      "epoch": 1.092857142857143,
      "grad_norm": 1.5027987957000732,
      "learning_rate": 0.00012714285714285714,
      "loss": 2.7198,
      "step": 7650
    },
    {
      "epoch": 1.0942857142857143,
      "grad_norm": 1.0696080923080444,
      "learning_rate": 0.00012704761904761905,
      "loss": 2.8085,
      "step": 7660
    },
    {
      "epoch": 1.0957142857142856,
      "grad_norm": 1.4526814222335815,
      "learning_rate": 0.00012695238095238097,
      "loss": 2.8708,
      "step": 7670
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 1.0823801755905151,
      "learning_rate": 0.00012685714285714286,
      "loss": 2.9009,
      "step": 7680
    },
    {
      "epoch": 1.0985714285714285,
      "grad_norm": 1.2032753229141235,
      "learning_rate": 0.00012676190476190478,
      "loss": 2.708,
      "step": 7690
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.0304722785949707,
      "learning_rate": 0.00012666666666666666,
      "loss": 2.7015,
      "step": 7700
    },
    {
      "epoch": 1.1014285714285714,
      "grad_norm": 1.3193362951278687,
      "learning_rate": 0.00012657142857142858,
      "loss": 2.8321,
      "step": 7710
    },
    {
      "epoch": 1.1028571428571428,
      "grad_norm": 1.201795220375061,
      "learning_rate": 0.00012647619047619047,
      "loss": 2.7866,
      "step": 7720
    },
    {
      "epoch": 1.1042857142857143,
      "grad_norm": 1.3745537996292114,
      "learning_rate": 0.0001263809523809524,
      "loss": 2.6259,
      "step": 7730
    },
    {
      "epoch": 1.1057142857142856,
      "grad_norm": 1.3285839557647705,
      "learning_rate": 0.0001262857142857143,
      "loss": 2.7956,
      "step": 7740
    },
    {
      "epoch": 1.1071428571428572,
      "grad_norm": 1.096465826034546,
      "learning_rate": 0.0001261904761904762,
      "loss": 3.0064,
      "step": 7750
    },
    {
      "epoch": 1.1085714285714285,
      "grad_norm": 1.185726284980774,
      "learning_rate": 0.0001260952380952381,
      "loss": 2.7796,
      "step": 7760
    },
    {
      "epoch": 1.11,
      "grad_norm": 1.3020681142807007,
      "learning_rate": 0.000126,
      "loss": 2.4141,
      "step": 7770
    },
    {
      "epoch": 1.1114285714285714,
      "grad_norm": 1.057751178741455,
      "learning_rate": 0.00012590476190476192,
      "loss": 2.8377,
      "step": 7780
    },
    {
      "epoch": 1.1128571428571428,
      "grad_norm": 1.112301230430603,
      "learning_rate": 0.00012580952380952383,
      "loss": 2.7986,
      "step": 7790
    },
    {
      "epoch": 1.1142857142857143,
      "grad_norm": 1.0066158771514893,
      "learning_rate": 0.00012571428571428572,
      "loss": 2.8915,
      "step": 7800
    },
    {
      "epoch": 1.1157142857142857,
      "grad_norm": 1.8650459051132202,
      "learning_rate": 0.0001256190476190476,
      "loss": 2.7385,
      "step": 7810
    },
    {
      "epoch": 1.1171428571428572,
      "grad_norm": 1.0960347652435303,
      "learning_rate": 0.00012552380952380953,
      "loss": 2.6854,
      "step": 7820
    },
    {
      "epoch": 1.1185714285714285,
      "grad_norm": 1.1279646158218384,
      "learning_rate": 0.00012542857142857142,
      "loss": 2.6508,
      "step": 7830
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.417576789855957,
      "learning_rate": 0.00012533333333333334,
      "loss": 2.7287,
      "step": 7840
    },
    {
      "epoch": 1.1214285714285714,
      "grad_norm": 0.9976977705955505,
      "learning_rate": 0.00012523809523809525,
      "loss": 2.5909,
      "step": 7850
    },
    {
      "epoch": 1.1228571428571428,
      "grad_norm": 1.1553317308425903,
      "learning_rate": 0.00012514285714285714,
      "loss": 2.7719,
      "step": 7860
    },
    {
      "epoch": 1.1242857142857143,
      "grad_norm": 1.4186429977416992,
      "learning_rate": 0.00012504761904761906,
      "loss": 2.779,
      "step": 7870
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 1.109336495399475,
      "learning_rate": 0.00012495238095238095,
      "loss": 2.8111,
      "step": 7880
    },
    {
      "epoch": 1.1271428571428572,
      "grad_norm": 1.5405056476593018,
      "learning_rate": 0.00012485714285714286,
      "loss": 2.6905,
      "step": 7890
    },
    {
      "epoch": 1.1285714285714286,
      "grad_norm": 0.931769073009491,
      "learning_rate": 0.00012476190476190478,
      "loss": 2.8513,
      "step": 7900
    },
    {
      "epoch": 1.13,
      "grad_norm": 0.9898472428321838,
      "learning_rate": 0.00012466666666666667,
      "loss": 2.775,
      "step": 7910
    },
    {
      "epoch": 1.1314285714285715,
      "grad_norm": 1.218036413192749,
      "learning_rate": 0.0001245714285714286,
      "loss": 2.8281,
      "step": 7920
    },
    {
      "epoch": 1.1328571428571428,
      "grad_norm": 1.1057376861572266,
      "learning_rate": 0.00012447619047619048,
      "loss": 2.6519,
      "step": 7930
    },
    {
      "epoch": 1.1342857142857143,
      "grad_norm": 1.2900975942611694,
      "learning_rate": 0.0001243809523809524,
      "loss": 2.6666,
      "step": 7940
    },
    {
      "epoch": 1.1357142857142857,
      "grad_norm": 1.1282235383987427,
      "learning_rate": 0.00012428571428571428,
      "loss": 2.5744,
      "step": 7950
    },
    {
      "epoch": 1.1371428571428572,
      "grad_norm": 1.1019147634506226,
      "learning_rate": 0.0001241904761904762,
      "loss": 2.6769,
      "step": 7960
    },
    {
      "epoch": 1.1385714285714286,
      "grad_norm": 1.3521604537963867,
      "learning_rate": 0.00012409523809523812,
      "loss": 2.7735,
      "step": 7970
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 1.2291597127914429,
      "learning_rate": 0.000124,
      "loss": 2.8564,
      "step": 7980
    },
    {
      "epoch": 1.1414285714285715,
      "grad_norm": 1.3768595457077026,
      "learning_rate": 0.0001239047619047619,
      "loss": 2.6425,
      "step": 7990
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.959959864616394,
      "learning_rate": 0.0001238095238095238,
      "loss": 2.8159,
      "step": 8000
    },
    {
      "epoch": 1.1442857142857144,
      "grad_norm": 1.4632397890090942,
      "learning_rate": 0.00012371428571428573,
      "loss": 2.6936,
      "step": 8010
    },
    {
      "epoch": 1.1457142857142857,
      "grad_norm": 1.1266400814056396,
      "learning_rate": 0.00012361904761904764,
      "loss": 2.6031,
      "step": 8020
    },
    {
      "epoch": 1.1471428571428572,
      "grad_norm": 1.3470046520233154,
      "learning_rate": 0.00012352380952380953,
      "loss": 2.6654,
      "step": 8030
    },
    {
      "epoch": 1.1485714285714286,
      "grad_norm": 1.3869198560714722,
      "learning_rate": 0.00012342857142857142,
      "loss": 2.6719,
      "step": 8040
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.1691111326217651,
      "learning_rate": 0.00012333333333333334,
      "loss": 2.663,
      "step": 8050
    },
    {
      "epoch": 1.1514285714285715,
      "grad_norm": 1.2484897375106812,
      "learning_rate": 0.00012323809523809523,
      "loss": 2.7221,
      "step": 8060
    },
    {
      "epoch": 1.1528571428571428,
      "grad_norm": 1.0218676328659058,
      "learning_rate": 0.00012314285714285715,
      "loss": 2.7551,
      "step": 8070
    },
    {
      "epoch": 1.1542857142857144,
      "grad_norm": 1.8242110013961792,
      "learning_rate": 0.00012304761904761906,
      "loss": 2.5696,
      "step": 8080
    },
    {
      "epoch": 1.1557142857142857,
      "grad_norm": 1.1653621196746826,
      "learning_rate": 0.00012295238095238095,
      "loss": 2.7106,
      "step": 8090
    },
    {
      "epoch": 1.157142857142857,
      "grad_norm": 1.127732276916504,
      "learning_rate": 0.00012285714285714287,
      "loss": 2.9415,
      "step": 8100
    },
    {
      "epoch": 1.1585714285714286,
      "grad_norm": 1.2444509267807007,
      "learning_rate": 0.00012276190476190476,
      "loss": 2.8671,
      "step": 8110
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.0927414894104004,
      "learning_rate": 0.00012266666666666668,
      "loss": 2.7335,
      "step": 8120
    },
    {
      "epoch": 1.1614285714285715,
      "grad_norm": 1.082085132598877,
      "learning_rate": 0.0001225714285714286,
      "loss": 2.7412,
      "step": 8130
    },
    {
      "epoch": 1.1628571428571428,
      "grad_norm": 0.885232150554657,
      "learning_rate": 0.00012247619047619048,
      "loss": 2.8585,
      "step": 8140
    },
    {
      "epoch": 1.1642857142857144,
      "grad_norm": 1.4978821277618408,
      "learning_rate": 0.0001223809523809524,
      "loss": 2.6745,
      "step": 8150
    },
    {
      "epoch": 1.1657142857142857,
      "grad_norm": 1.4682449102401733,
      "learning_rate": 0.0001222857142857143,
      "loss": 2.7133,
      "step": 8160
    },
    {
      "epoch": 1.1671428571428573,
      "grad_norm": 1.1267900466918945,
      "learning_rate": 0.00012219047619047618,
      "loss": 2.7843,
      "step": 8170
    },
    {
      "epoch": 1.1685714285714286,
      "grad_norm": 1.0411019325256348,
      "learning_rate": 0.0001220952380952381,
      "loss": 2.704,
      "step": 8180
    },
    {
      "epoch": 1.17,
      "grad_norm": 1.2861781120300293,
      "learning_rate": 0.000122,
      "loss": 2.736,
      "step": 8190
    },
    {
      "epoch": 1.1714285714285715,
      "grad_norm": 1.1449137926101685,
      "learning_rate": 0.00012190476190476193,
      "loss": 2.9061,
      "step": 8200
    },
    {
      "epoch": 1.1728571428571428,
      "grad_norm": 1.083117961883545,
      "learning_rate": 0.00012180952380952382,
      "loss": 2.7618,
      "step": 8210
    },
    {
      "epoch": 1.1742857142857144,
      "grad_norm": 1.1257333755493164,
      "learning_rate": 0.00012171428571428572,
      "loss": 2.671,
      "step": 8220
    },
    {
      "epoch": 1.1757142857142857,
      "grad_norm": 1.1998791694641113,
      "learning_rate": 0.00012161904761904764,
      "loss": 2.6499,
      "step": 8230
    },
    {
      "epoch": 1.177142857142857,
      "grad_norm": 1.0382436513900757,
      "learning_rate": 0.00012152380952380953,
      "loss": 2.7305,
      "step": 8240
    },
    {
      "epoch": 1.1785714285714286,
      "grad_norm": 1.1847470998764038,
      "learning_rate": 0.00012142857142857143,
      "loss": 2.7418,
      "step": 8250
    },
    {
      "epoch": 1.18,
      "grad_norm": 1.0471559762954712,
      "learning_rate": 0.00012133333333333335,
      "loss": 2.699,
      "step": 8260
    },
    {
      "epoch": 1.1814285714285715,
      "grad_norm": 2.072016477584839,
      "learning_rate": 0.00012123809523809524,
      "loss": 2.5757,
      "step": 8270
    },
    {
      "epoch": 1.1828571428571428,
      "grad_norm": 1.1340585947036743,
      "learning_rate": 0.00012114285714285715,
      "loss": 2.8242,
      "step": 8280
    },
    {
      "epoch": 1.1842857142857142,
      "grad_norm": 1.3188180923461914,
      "learning_rate": 0.00012104761904761906,
      "loss": 2.8071,
      "step": 8290
    },
    {
      "epoch": 1.1857142857142857,
      "grad_norm": 1.279204249382019,
      "learning_rate": 0.00012095238095238095,
      "loss": 2.8014,
      "step": 8300
    },
    {
      "epoch": 1.187142857142857,
      "grad_norm": 1.2235186100006104,
      "learning_rate": 0.00012085714285714288,
      "loss": 2.8009,
      "step": 8310
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 1.2653872966766357,
      "learning_rate": 0.00012076190476190476,
      "loss": 2.5087,
      "step": 8320
    },
    {
      "epoch": 1.19,
      "grad_norm": 1.3173449039459229,
      "learning_rate": 0.00012066666666666668,
      "loss": 2.6539,
      "step": 8330
    },
    {
      "epoch": 1.1914285714285715,
      "grad_norm": 1.321158766746521,
      "learning_rate": 0.00012057142857142858,
      "loss": 2.7073,
      "step": 8340
    },
    {
      "epoch": 1.1928571428571428,
      "grad_norm": 1.2502976655960083,
      "learning_rate": 0.00012047619047619047,
      "loss": 2.6434,
      "step": 8350
    },
    {
      "epoch": 1.1942857142857144,
      "grad_norm": 1.0516630411148071,
      "learning_rate": 0.00012038095238095239,
      "loss": 2.6811,
      "step": 8360
    },
    {
      "epoch": 1.1957142857142857,
      "grad_norm": 1.4415132999420166,
      "learning_rate": 0.0001202857142857143,
      "loss": 2.6613,
      "step": 8370
    },
    {
      "epoch": 1.197142857142857,
      "grad_norm": 1.1286289691925049,
      "learning_rate": 0.00012019047619047618,
      "loss": 2.654,
      "step": 8380
    },
    {
      "epoch": 1.1985714285714286,
      "grad_norm": 1.1171343326568604,
      "learning_rate": 0.0001200952380952381,
      "loss": 2.6124,
      "step": 8390
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.9754251837730408,
      "learning_rate": 0.00012,
      "loss": 2.692,
      "step": 8400
    },
    {
      "epoch": 1.2014285714285715,
      "grad_norm": 1.197298526763916,
      "learning_rate": 0.00011990476190476192,
      "loss": 2.7817,
      "step": 8410
    },
    {
      "epoch": 1.2028571428571428,
      "grad_norm": 1.2955012321472168,
      "learning_rate": 0.00011980952380952381,
      "loss": 2.777,
      "step": 8420
    },
    {
      "epoch": 1.2042857142857142,
      "grad_norm": 1.3510457277297974,
      "learning_rate": 0.00011971428571428571,
      "loss": 2.8159,
      "step": 8430
    },
    {
      "epoch": 1.2057142857142857,
      "grad_norm": 1.4204285144805908,
      "learning_rate": 0.00011961904761904763,
      "loss": 2.7873,
      "step": 8440
    },
    {
      "epoch": 1.207142857142857,
      "grad_norm": 1.1421693563461304,
      "learning_rate": 0.00011952380952380953,
      "loss": 2.543,
      "step": 8450
    },
    {
      "epoch": 1.2085714285714286,
      "grad_norm": 1.236117959022522,
      "learning_rate": 0.00011942857142857145,
      "loss": 2.7831,
      "step": 8460
    },
    {
      "epoch": 1.21,
      "grad_norm": 1.0413284301757812,
      "learning_rate": 0.00011933333333333334,
      "loss": 2.7078,
      "step": 8470
    },
    {
      "epoch": 1.2114285714285715,
      "grad_norm": 1.125970721244812,
      "learning_rate": 0.00011923809523809524,
      "loss": 2.6203,
      "step": 8480
    },
    {
      "epoch": 1.2128571428571429,
      "grad_norm": 1.3953434228897095,
      "learning_rate": 0.00011914285714285716,
      "loss": 2.8615,
      "step": 8490
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 1.3558858633041382,
      "learning_rate": 0.00011904761904761905,
      "loss": 2.8366,
      "step": 8500
    },
    {
      "epoch": 1.2157142857142857,
      "grad_norm": 1.3181267976760864,
      "learning_rate": 0.00011895238095238095,
      "loss": 2.7949,
      "step": 8510
    },
    {
      "epoch": 1.217142857142857,
      "grad_norm": 1.006723165512085,
      "learning_rate": 0.00011885714285714287,
      "loss": 2.7496,
      "step": 8520
    },
    {
      "epoch": 1.2185714285714286,
      "grad_norm": 1.2960822582244873,
      "learning_rate": 0.00011876190476190476,
      "loss": 2.6122,
      "step": 8530
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.5859911441802979,
      "learning_rate": 0.00011866666666666669,
      "loss": 2.7849,
      "step": 8540
    },
    {
      "epoch": 1.2214285714285715,
      "grad_norm": 0.9559836983680725,
      "learning_rate": 0.00011857142857142858,
      "loss": 2.8957,
      "step": 8550
    },
    {
      "epoch": 1.2228571428571429,
      "grad_norm": 1.1541672945022583,
      "learning_rate": 0.00011847619047619048,
      "loss": 2.842,
      "step": 8560
    },
    {
      "epoch": 1.2242857142857142,
      "grad_norm": 0.9887745380401611,
      "learning_rate": 0.0001183809523809524,
      "loss": 2.9051,
      "step": 8570
    },
    {
      "epoch": 1.2257142857142858,
      "grad_norm": 1.4164178371429443,
      "learning_rate": 0.00011828571428571429,
      "loss": 2.7076,
      "step": 8580
    },
    {
      "epoch": 1.227142857142857,
      "grad_norm": 1.3631489276885986,
      "learning_rate": 0.0001181904761904762,
      "loss": 2.7937,
      "step": 8590
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 1.2136893272399902,
      "learning_rate": 0.0001180952380952381,
      "loss": 2.8132,
      "step": 8600
    },
    {
      "epoch": 1.23,
      "grad_norm": 0.9907533526420593,
      "learning_rate": 0.000118,
      "loss": 2.6882,
      "step": 8610
    },
    {
      "epoch": 1.2314285714285713,
      "grad_norm": 1.729910135269165,
      "learning_rate": 0.00011790476190476191,
      "loss": 2.8063,
      "step": 8620
    },
    {
      "epoch": 1.2328571428571429,
      "grad_norm": 0.8650830984115601,
      "learning_rate": 0.00011780952380952381,
      "loss": 2.7142,
      "step": 8630
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 0.9575071334838867,
      "learning_rate": 0.0001177142857142857,
      "loss": 2.8102,
      "step": 8640
    },
    {
      "epoch": 1.2357142857142858,
      "grad_norm": 1.2326574325561523,
      "learning_rate": 0.00011761904761904763,
      "loss": 2.6618,
      "step": 8650
    },
    {
      "epoch": 1.237142857142857,
      "grad_norm": 0.9180296659469604,
      "learning_rate": 0.00011752380952380952,
      "loss": 2.734,
      "step": 8660
    },
    {
      "epoch": 1.2385714285714287,
      "grad_norm": 0.9854812622070312,
      "learning_rate": 0.00011742857142857144,
      "loss": 2.7511,
      "step": 8670
    },
    {
      "epoch": 1.24,
      "grad_norm": 1.3142739534378052,
      "learning_rate": 0.00011733333333333334,
      "loss": 2.7261,
      "step": 8680
    },
    {
      "epoch": 1.2414285714285715,
      "grad_norm": 1.5105284452438354,
      "learning_rate": 0.00011723809523809523,
      "loss": 2.6264,
      "step": 8690
    },
    {
      "epoch": 1.2428571428571429,
      "grad_norm": 1.0455830097198486,
      "learning_rate": 0.00011714285714285715,
      "loss": 2.7662,
      "step": 8700
    },
    {
      "epoch": 1.2442857142857142,
      "grad_norm": 0.9855660200119019,
      "learning_rate": 0.00011704761904761905,
      "loss": 2.6473,
      "step": 8710
    },
    {
      "epoch": 1.2457142857142858,
      "grad_norm": 1.0681248903274536,
      "learning_rate": 0.00011695238095238097,
      "loss": 2.8419,
      "step": 8720
    },
    {
      "epoch": 1.247142857142857,
      "grad_norm": 1.2600367069244385,
      "learning_rate": 0.00011685714285714286,
      "loss": 2.7649,
      "step": 8730
    },
    {
      "epoch": 1.2485714285714287,
      "grad_norm": 1.1157574653625488,
      "learning_rate": 0.00011676190476190476,
      "loss": 2.7457,
      "step": 8740
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.141540288925171,
      "learning_rate": 0.00011666666666666668,
      "loss": 2.6804,
      "step": 8750
    },
    {
      "epoch": 1.2514285714285713,
      "grad_norm": 1.4910049438476562,
      "learning_rate": 0.00011657142857142858,
      "loss": 2.8628,
      "step": 8760
    },
    {
      "epoch": 1.252857142857143,
      "grad_norm": 0.9186047911643982,
      "learning_rate": 0.00011647619047619047,
      "loss": 2.7372,
      "step": 8770
    },
    {
      "epoch": 1.2542857142857142,
      "grad_norm": 1.1123127937316895,
      "learning_rate": 0.00011638095238095239,
      "loss": 2.6991,
      "step": 8780
    },
    {
      "epoch": 1.2557142857142858,
      "grad_norm": 1.1281263828277588,
      "learning_rate": 0.00011628571428571429,
      "loss": 2.7991,
      "step": 8790
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 1.6657588481903076,
      "learning_rate": 0.00011619047619047621,
      "loss": 2.6943,
      "step": 8800
    },
    {
      "epoch": 1.2585714285714285,
      "grad_norm": 1.2940089702606201,
      "learning_rate": 0.0001160952380952381,
      "loss": 2.6258,
      "step": 8810
    },
    {
      "epoch": 1.26,
      "grad_norm": 1.557105302810669,
      "learning_rate": 0.000116,
      "loss": 2.7247,
      "step": 8820
    },
    {
      "epoch": 1.2614285714285716,
      "grad_norm": 1.4265458583831787,
      "learning_rate": 0.00011590476190476192,
      "loss": 2.7577,
      "step": 8830
    },
    {
      "epoch": 1.262857142857143,
      "grad_norm": 1.1593995094299316,
      "learning_rate": 0.00011580952380952381,
      "loss": 2.7769,
      "step": 8840
    },
    {
      "epoch": 1.2642857142857142,
      "grad_norm": 1.1037542819976807,
      "learning_rate": 0.00011571428571428574,
      "loss": 2.6872,
      "step": 8850
    },
    {
      "epoch": 1.2657142857142858,
      "grad_norm": 1.1733393669128418,
      "learning_rate": 0.00011561904761904763,
      "loss": 2.6282,
      "step": 8860
    },
    {
      "epoch": 1.2671428571428571,
      "grad_norm": 1.3079136610031128,
      "learning_rate": 0.00011552380952380953,
      "loss": 2.6114,
      "step": 8870
    },
    {
      "epoch": 1.2685714285714287,
      "grad_norm": 1.1142727136611938,
      "learning_rate": 0.00011542857142857145,
      "loss": 2.741,
      "step": 8880
    },
    {
      "epoch": 1.27,
      "grad_norm": 1.190771222114563,
      "learning_rate": 0.00011533333333333334,
      "loss": 2.6591,
      "step": 8890
    },
    {
      "epoch": 1.2714285714285714,
      "grad_norm": 1.1608402729034424,
      "learning_rate": 0.00011523809523809524,
      "loss": 2.7283,
      "step": 8900
    },
    {
      "epoch": 1.272857142857143,
      "grad_norm": 1.3515902757644653,
      "learning_rate": 0.00011514285714285716,
      "loss": 2.7505,
      "step": 8910
    },
    {
      "epoch": 1.2742857142857142,
      "grad_norm": 1.0301318168640137,
      "learning_rate": 0.00011504761904761905,
      "loss": 2.8023,
      "step": 8920
    },
    {
      "epoch": 1.2757142857142858,
      "grad_norm": 1.138654351234436,
      "learning_rate": 0.00011495238095238096,
      "loss": 2.7935,
      "step": 8930
    },
    {
      "epoch": 1.2771428571428571,
      "grad_norm": 1.2785217761993408,
      "learning_rate": 0.00011485714285714286,
      "loss": 2.6552,
      "step": 8940
    },
    {
      "epoch": 1.2785714285714285,
      "grad_norm": 1.2927557229995728,
      "learning_rate": 0.00011476190476190475,
      "loss": 2.8874,
      "step": 8950
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.9647601246833801,
      "learning_rate": 0.00011466666666666667,
      "loss": 2.8811,
      "step": 8960
    },
    {
      "epoch": 1.2814285714285714,
      "grad_norm": 1.012052059173584,
      "learning_rate": 0.00011457142857142857,
      "loss": 2.8479,
      "step": 8970
    },
    {
      "epoch": 1.282857142857143,
      "grad_norm": 1.1839399337768555,
      "learning_rate": 0.00011447619047619049,
      "loss": 2.7954,
      "step": 8980
    },
    {
      "epoch": 1.2842857142857143,
      "grad_norm": 0.902838945388794,
      "learning_rate": 0.0001143809523809524,
      "loss": 2.7099,
      "step": 8990
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.216458797454834,
      "learning_rate": 0.00011428571428571428,
      "loss": 2.7025,
      "step": 9000
    },
    {
      "epoch": 1.2871428571428571,
      "grad_norm": 1.204874038696289,
      "learning_rate": 0.0001141904761904762,
      "loss": 2.7291,
      "step": 9010
    },
    {
      "epoch": 1.2885714285714287,
      "grad_norm": 1.1730390787124634,
      "learning_rate": 0.0001140952380952381,
      "loss": 2.6024,
      "step": 9020
    },
    {
      "epoch": 1.29,
      "grad_norm": 1.0837774276733398,
      "learning_rate": 0.00011399999999999999,
      "loss": 2.9139,
      "step": 9030
    },
    {
      "epoch": 1.2914285714285714,
      "grad_norm": 1.4439256191253662,
      "learning_rate": 0.00011390476190476191,
      "loss": 2.7394,
      "step": 9040
    },
    {
      "epoch": 1.292857142857143,
      "grad_norm": 1.2056366205215454,
      "learning_rate": 0.00011380952380952381,
      "loss": 2.7259,
      "step": 9050
    },
    {
      "epoch": 1.2942857142857143,
      "grad_norm": 1.2100437879562378,
      "learning_rate": 0.00011371428571428573,
      "loss": 2.7988,
      "step": 9060
    },
    {
      "epoch": 1.2957142857142858,
      "grad_norm": 1.1206496953964233,
      "learning_rate": 0.00011361904761904762,
      "loss": 2.8215,
      "step": 9070
    },
    {
      "epoch": 1.2971428571428572,
      "grad_norm": 1.1576144695281982,
      "learning_rate": 0.00011352380952380952,
      "loss": 2.8087,
      "step": 9080
    },
    {
      "epoch": 1.2985714285714285,
      "grad_norm": 1.079962134361267,
      "learning_rate": 0.00011342857142857144,
      "loss": 2.6133,
      "step": 9090
    },
    {
      "epoch": 1.3,
      "grad_norm": 1.2942239046096802,
      "learning_rate": 0.00011333333333333334,
      "loss": 2.7224,
      "step": 9100
    },
    {
      "epoch": 1.3014285714285714,
      "grad_norm": 0.9726483821868896,
      "learning_rate": 0.00011323809523809526,
      "loss": 2.7252,
      "step": 9110
    },
    {
      "epoch": 1.302857142857143,
      "grad_norm": 1.1934821605682373,
      "learning_rate": 0.00011314285714285715,
      "loss": 2.672,
      "step": 9120
    },
    {
      "epoch": 1.3042857142857143,
      "grad_norm": 1.6744674444198608,
      "learning_rate": 0.00011304761904761905,
      "loss": 2.6686,
      "step": 9130
    },
    {
      "epoch": 1.3057142857142856,
      "grad_norm": 0.9742322564125061,
      "learning_rate": 0.00011295238095238097,
      "loss": 2.7173,
      "step": 9140
    },
    {
      "epoch": 1.3071428571428572,
      "grad_norm": 1.06282639503479,
      "learning_rate": 0.00011285714285714286,
      "loss": 2.7822,
      "step": 9150
    },
    {
      "epoch": 1.3085714285714285,
      "grad_norm": 1.0472239255905151,
      "learning_rate": 0.00011276190476190476,
      "loss": 2.6796,
      "step": 9160
    },
    {
      "epoch": 1.31,
      "grad_norm": 1.5114736557006836,
      "learning_rate": 0.00011266666666666668,
      "loss": 2.6711,
      "step": 9170
    },
    {
      "epoch": 1.3114285714285714,
      "grad_norm": 1.1766283512115479,
      "learning_rate": 0.00011257142857142857,
      "loss": 2.6543,
      "step": 9180
    },
    {
      "epoch": 1.3128571428571427,
      "grad_norm": 0.9259236454963684,
      "learning_rate": 0.0001124761904761905,
      "loss": 2.7639,
      "step": 9190
    },
    {
      "epoch": 1.3142857142857143,
      "grad_norm": 1.211153268814087,
      "learning_rate": 0.00011238095238095239,
      "loss": 2.6177,
      "step": 9200
    },
    {
      "epoch": 1.3157142857142858,
      "grad_norm": 1.3292043209075928,
      "learning_rate": 0.00011228571428571429,
      "loss": 2.8325,
      "step": 9210
    },
    {
      "epoch": 1.3171428571428572,
      "grad_norm": 0.9089626669883728,
      "learning_rate": 0.0001121904761904762,
      "loss": 2.6817,
      "step": 9220
    },
    {
      "epoch": 1.3185714285714285,
      "grad_norm": 1.1026604175567627,
      "learning_rate": 0.0001120952380952381,
      "loss": 2.7165,
      "step": 9230
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.3219717741012573,
      "learning_rate": 0.00011200000000000001,
      "loss": 2.7162,
      "step": 9240
    },
    {
      "epoch": 1.3214285714285714,
      "grad_norm": 1.3857005834579468,
      "learning_rate": 0.00011190476190476191,
      "loss": 2.7313,
      "step": 9250
    },
    {
      "epoch": 1.322857142857143,
      "grad_norm": 0.9688118696212769,
      "learning_rate": 0.0001118095238095238,
      "loss": 2.8515,
      "step": 9260
    },
    {
      "epoch": 1.3242857142857143,
      "grad_norm": 1.179636836051941,
      "learning_rate": 0.00011171428571428572,
      "loss": 2.7832,
      "step": 9270
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 1.0584315061569214,
      "learning_rate": 0.00011161904761904762,
      "loss": 2.8338,
      "step": 9280
    },
    {
      "epoch": 1.3271428571428572,
      "grad_norm": 1.2880606651306152,
      "learning_rate": 0.00011152380952380951,
      "loss": 2.8519,
      "step": 9290
    },
    {
      "epoch": 1.3285714285714285,
      "grad_norm": 1.2553704977035522,
      "learning_rate": 0.00011142857142857144,
      "loss": 2.6879,
      "step": 9300
    },
    {
      "epoch": 1.33,
      "grad_norm": 1.4390602111816406,
      "learning_rate": 0.00011133333333333333,
      "loss": 2.5808,
      "step": 9310
    },
    {
      "epoch": 1.3314285714285714,
      "grad_norm": 1.2285505533218384,
      "learning_rate": 0.00011123809523809525,
      "loss": 2.6895,
      "step": 9320
    },
    {
      "epoch": 1.3328571428571427,
      "grad_norm": 1.1689478158950806,
      "learning_rate": 0.00011114285714285715,
      "loss": 2.7319,
      "step": 9330
    },
    {
      "epoch": 1.3342857142857143,
      "grad_norm": 1.847794532775879,
      "learning_rate": 0.00011104761904761904,
      "loss": 2.6134,
      "step": 9340
    },
    {
      "epoch": 1.3357142857142856,
      "grad_norm": 1.1994744539260864,
      "learning_rate": 0.00011095238095238096,
      "loss": 2.7703,
      "step": 9350
    },
    {
      "epoch": 1.3371428571428572,
      "grad_norm": 1.0078530311584473,
      "learning_rate": 0.00011085714285714286,
      "loss": 2.5533,
      "step": 9360
    },
    {
      "epoch": 1.3385714285714285,
      "grad_norm": 1.2914808988571167,
      "learning_rate": 0.00011076190476190478,
      "loss": 2.8135,
      "step": 9370
    },
    {
      "epoch": 1.34,
      "grad_norm": 1.3511135578155518,
      "learning_rate": 0.00011066666666666667,
      "loss": 2.8258,
      "step": 9380
    },
    {
      "epoch": 1.3414285714285714,
      "grad_norm": 1.2486207485198975,
      "learning_rate": 0.00011057142857142857,
      "loss": 2.7494,
      "step": 9390
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 1.4080642461776733,
      "learning_rate": 0.00011047619047619049,
      "loss": 2.6341,
      "step": 9400
    },
    {
      "epoch": 1.3442857142857143,
      "grad_norm": 1.266418218612671,
      "learning_rate": 0.00011038095238095239,
      "loss": 2.7044,
      "step": 9410
    },
    {
      "epoch": 1.3457142857142856,
      "grad_norm": 1.0812448263168335,
      "learning_rate": 0.00011028571428571428,
      "loss": 2.6501,
      "step": 9420
    },
    {
      "epoch": 1.3471428571428572,
      "grad_norm": 1.2828925848007202,
      "learning_rate": 0.0001101904761904762,
      "loss": 2.7057,
      "step": 9430
    },
    {
      "epoch": 1.3485714285714285,
      "grad_norm": 1.0716707706451416,
      "learning_rate": 0.0001100952380952381,
      "loss": 2.6641,
      "step": 9440
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.0880459547042847,
      "learning_rate": 0.00011000000000000002,
      "loss": 2.7794,
      "step": 9450
    },
    {
      "epoch": 1.3514285714285714,
      "grad_norm": 1.4156478643417358,
      "learning_rate": 0.00010990476190476191,
      "loss": 2.7324,
      "step": 9460
    },
    {
      "epoch": 1.3528571428571428,
      "grad_norm": 1.2908157110214233,
      "learning_rate": 0.00010980952380952381,
      "loss": 2.6323,
      "step": 9470
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 1.2810784578323364,
      "learning_rate": 0.00010971428571428573,
      "loss": 2.7439,
      "step": 9480
    },
    {
      "epoch": 1.3557142857142856,
      "grad_norm": 1.0968588590621948,
      "learning_rate": 0.00010961904761904762,
      "loss": 2.5695,
      "step": 9490
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 1.320774793624878,
      "learning_rate": 0.00010952380952380953,
      "loss": 2.7148,
      "step": 9500
    },
    {
      "epoch": 1.3585714285714285,
      "grad_norm": 1.2036551237106323,
      "learning_rate": 0.00010942857142857144,
      "loss": 2.7132,
      "step": 9510
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 1.3288716077804565,
      "learning_rate": 0.00010933333333333333,
      "loss": 2.6883,
      "step": 9520
    },
    {
      "epoch": 1.3614285714285714,
      "grad_norm": 0.877667248249054,
      "learning_rate": 0.00010923809523809526,
      "loss": 2.7797,
      "step": 9530
    },
    {
      "epoch": 1.362857142857143,
      "grad_norm": 1.3485357761383057,
      "learning_rate": 0.00010914285714285715,
      "loss": 2.8254,
      "step": 9540
    },
    {
      "epoch": 1.3642857142857143,
      "grad_norm": 1.2521151304244995,
      "learning_rate": 0.00010904761904761905,
      "loss": 2.7146,
      "step": 9550
    },
    {
      "epoch": 1.3657142857142857,
      "grad_norm": 1.5015450716018677,
      "learning_rate": 0.00010895238095238097,
      "loss": 2.667,
      "step": 9560
    },
    {
      "epoch": 1.3671428571428572,
      "grad_norm": 1.401849627494812,
      "learning_rate": 0.00010885714285714285,
      "loss": 2.7771,
      "step": 9570
    },
    {
      "epoch": 1.3685714285714285,
      "grad_norm": 1.5641647577285767,
      "learning_rate": 0.00010876190476190477,
      "loss": 2.7117,
      "step": 9580
    },
    {
      "epoch": 1.37,
      "grad_norm": 0.9437844753265381,
      "learning_rate": 0.00010866666666666667,
      "loss": 2.796,
      "step": 9590
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 1.7182053327560425,
      "learning_rate": 0.00010857142857142856,
      "loss": 2.6893,
      "step": 9600
    },
    {
      "epoch": 1.3728571428571428,
      "grad_norm": 1.590796947479248,
      "learning_rate": 0.00010847619047619048,
      "loss": 2.6066,
      "step": 9610
    },
    {
      "epoch": 1.3742857142857143,
      "grad_norm": 1.2761669158935547,
      "learning_rate": 0.00010838095238095238,
      "loss": 2.7504,
      "step": 9620
    },
    {
      "epoch": 1.3757142857142857,
      "grad_norm": 1.381044864654541,
      "learning_rate": 0.0001082857142857143,
      "loss": 2.7903,
      "step": 9630
    },
    {
      "epoch": 1.3771428571428572,
      "grad_norm": 0.9222652912139893,
      "learning_rate": 0.0001081904761904762,
      "loss": 2.6633,
      "step": 9640
    },
    {
      "epoch": 1.3785714285714286,
      "grad_norm": 1.3718771934509277,
      "learning_rate": 0.00010809523809523809,
      "loss": 2.8134,
      "step": 9650
    },
    {
      "epoch": 1.38,
      "grad_norm": 1.2591360807418823,
      "learning_rate": 0.00010800000000000001,
      "loss": 2.778,
      "step": 9660
    },
    {
      "epoch": 1.3814285714285715,
      "grad_norm": 1.091275691986084,
      "learning_rate": 0.00010790476190476191,
      "loss": 2.774,
      "step": 9670
    },
    {
      "epoch": 1.3828571428571428,
      "grad_norm": 1.15514075756073,
      "learning_rate": 0.0001078095238095238,
      "loss": 2.6337,
      "step": 9680
    },
    {
      "epoch": 1.3842857142857143,
      "grad_norm": 1.184434175491333,
      "learning_rate": 0.00010771428571428572,
      "loss": 2.7406,
      "step": 9690
    },
    {
      "epoch": 1.3857142857142857,
      "grad_norm": 1.3338536024093628,
      "learning_rate": 0.00010761904761904762,
      "loss": 2.5819,
      "step": 9700
    },
    {
      "epoch": 1.387142857142857,
      "grad_norm": 1.4241504669189453,
      "learning_rate": 0.00010752380952380954,
      "loss": 2.7635,
      "step": 9710
    },
    {
      "epoch": 1.3885714285714286,
      "grad_norm": 1.4053919315338135,
      "learning_rate": 0.00010742857142857143,
      "loss": 2.7099,
      "step": 9720
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 1.0271652936935425,
      "learning_rate": 0.00010733333333333333,
      "loss": 2.6712,
      "step": 9730
    },
    {
      "epoch": 1.3914285714285715,
      "grad_norm": 1.1042076349258423,
      "learning_rate": 0.00010723809523809525,
      "loss": 2.7087,
      "step": 9740
    },
    {
      "epoch": 1.3928571428571428,
      "grad_norm": 1.3711175918579102,
      "learning_rate": 0.00010714285714285715,
      "loss": 2.7247,
      "step": 9750
    },
    {
      "epoch": 1.3942857142857144,
      "grad_norm": 1.0638197660446167,
      "learning_rate": 0.00010704761904761907,
      "loss": 2.6895,
      "step": 9760
    },
    {
      "epoch": 1.3957142857142857,
      "grad_norm": 1.096595048904419,
      "learning_rate": 0.00010695238095238096,
      "loss": 2.6934,
      "step": 9770
    },
    {
      "epoch": 1.3971428571428572,
      "grad_norm": 1.1854583024978638,
      "learning_rate": 0.00010685714285714286,
      "loss": 2.7005,
      "step": 9780
    },
    {
      "epoch": 1.3985714285714286,
      "grad_norm": 1.0421584844589233,
      "learning_rate": 0.00010676190476190478,
      "loss": 2.8098,
      "step": 9790
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.0994199514389038,
      "learning_rate": 0.00010666666666666667,
      "loss": 2.7483,
      "step": 9800
    },
    {
      "epoch": 1.4014285714285715,
      "grad_norm": 1.2914644479751587,
      "learning_rate": 0.00010657142857142857,
      "loss": 2.734,
      "step": 9810
    },
    {
      "epoch": 1.4028571428571428,
      "grad_norm": 1.2471874952316284,
      "learning_rate": 0.00010647619047619049,
      "loss": 2.7684,
      "step": 9820
    },
    {
      "epoch": 1.4042857142857144,
      "grad_norm": 1.348976731300354,
      "learning_rate": 0.00010638095238095238,
      "loss": 2.6264,
      "step": 9830
    },
    {
      "epoch": 1.4057142857142857,
      "grad_norm": 1.239294171333313,
      "learning_rate": 0.0001062857142857143,
      "loss": 2.6536,
      "step": 9840
    },
    {
      "epoch": 1.407142857142857,
      "grad_norm": 1.3014013767242432,
      "learning_rate": 0.0001061904761904762,
      "loss": 2.7148,
      "step": 9850
    },
    {
      "epoch": 1.4085714285714286,
      "grad_norm": 1.0676637887954712,
      "learning_rate": 0.0001060952380952381,
      "loss": 2.7323,
      "step": 9860
    },
    {
      "epoch": 1.41,
      "grad_norm": 1.0452518463134766,
      "learning_rate": 0.00010600000000000002,
      "loss": 2.6286,
      "step": 9870
    },
    {
      "epoch": 1.4114285714285715,
      "grad_norm": 1.3617502450942993,
      "learning_rate": 0.0001059047619047619,
      "loss": 2.6267,
      "step": 9880
    },
    {
      "epoch": 1.4128571428571428,
      "grad_norm": 1.2476091384887695,
      "learning_rate": 0.00010580952380952382,
      "loss": 2.7303,
      "step": 9890
    },
    {
      "epoch": 1.4142857142857144,
      "grad_norm": 0.9206644892692566,
      "learning_rate": 0.00010571428571428572,
      "loss": 2.8194,
      "step": 9900
    },
    {
      "epoch": 1.4157142857142857,
      "grad_norm": 1.192277431488037,
      "learning_rate": 0.00010561904761904761,
      "loss": 2.719,
      "step": 9910
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 1.0766315460205078,
      "learning_rate": 0.00010552380952380953,
      "loss": 2.6235,
      "step": 9920
    },
    {
      "epoch": 1.4185714285714286,
      "grad_norm": 0.9376991987228394,
      "learning_rate": 0.00010542857142857143,
      "loss": 2.8129,
      "step": 9930
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.3325257301330566,
      "learning_rate": 0.00010533333333333332,
      "loss": 2.5381,
      "step": 9940
    },
    {
      "epoch": 1.4214285714285715,
      "grad_norm": 1.0260018110275269,
      "learning_rate": 0.00010523809523809525,
      "loss": 2.7374,
      "step": 9950
    },
    {
      "epoch": 1.4228571428571428,
      "grad_norm": 1.2688040733337402,
      "learning_rate": 0.00010514285714285714,
      "loss": 2.5934,
      "step": 9960
    },
    {
      "epoch": 1.4242857142857144,
      "grad_norm": 1.3239009380340576,
      "learning_rate": 0.00010504761904761906,
      "loss": 2.6842,
      "step": 9970
    },
    {
      "epoch": 1.4257142857142857,
      "grad_norm": 0.8186426758766174,
      "learning_rate": 0.00010495238095238096,
      "loss": 2.7684,
      "step": 9980
    },
    {
      "epoch": 1.427142857142857,
      "grad_norm": 1.2348008155822754,
      "learning_rate": 0.00010485714285714285,
      "loss": 2.7405,
      "step": 9990
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.1747251749038696,
      "learning_rate": 0.00010476190476190477,
      "loss": 2.6163,
      "step": 10000
    },
    {
      "epoch": 1.43,
      "grad_norm": 1.093880295753479,
      "learning_rate": 0.00010466666666666667,
      "loss": 2.7822,
      "step": 10010
    },
    {
      "epoch": 1.4314285714285715,
      "grad_norm": 1.4705561399459839,
      "learning_rate": 0.00010457142857142859,
      "loss": 2.8706,
      "step": 10020
    },
    {
      "epoch": 1.4328571428571428,
      "grad_norm": 1.1950875520706177,
      "learning_rate": 0.00010447619047619048,
      "loss": 2.7873,
      "step": 10030
    },
    {
      "epoch": 1.4342857142857142,
      "grad_norm": 1.314520239830017,
      "learning_rate": 0.00010438095238095238,
      "loss": 2.8342,
      "step": 10040
    },
    {
      "epoch": 1.4357142857142857,
      "grad_norm": 1.017077088356018,
      "learning_rate": 0.0001042857142857143,
      "loss": 2.6676,
      "step": 10050
    },
    {
      "epoch": 1.4371428571428573,
      "grad_norm": 1.6740894317626953,
      "learning_rate": 0.00010419047619047619,
      "loss": 2.7972,
      "step": 10060
    },
    {
      "epoch": 1.4385714285714286,
      "grad_norm": 1.4393947124481201,
      "learning_rate": 0.00010409523809523809,
      "loss": 2.6934,
      "step": 10070
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.1054980754852295,
      "learning_rate": 0.00010400000000000001,
      "loss": 2.728,
      "step": 10080
    },
    {
      "epoch": 1.4414285714285715,
      "grad_norm": 1.1801835298538208,
      "learning_rate": 0.00010390476190476191,
      "loss": 2.8331,
      "step": 10090
    },
    {
      "epoch": 1.4428571428571428,
      "grad_norm": 0.827828049659729,
      "learning_rate": 0.00010380952380952383,
      "loss": 2.6188,
      "step": 10100
    },
    {
      "epoch": 1.4442857142857144,
      "grad_norm": 1.1015572547912598,
      "learning_rate": 0.00010371428571428572,
      "loss": 2.7382,
      "step": 10110
    },
    {
      "epoch": 1.4457142857142857,
      "grad_norm": 1.321768879890442,
      "learning_rate": 0.00010361904761904762,
      "loss": 2.842,
      "step": 10120
    },
    {
      "epoch": 1.447142857142857,
      "grad_norm": 1.1724246740341187,
      "learning_rate": 0.00010352380952380954,
      "loss": 2.6767,
      "step": 10130
    },
    {
      "epoch": 1.4485714285714286,
      "grad_norm": 1.4837349653244019,
      "learning_rate": 0.00010342857142857143,
      "loss": 2.7034,
      "step": 10140
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.4532973766326904,
      "learning_rate": 0.00010333333333333334,
      "loss": 2.6138,
      "step": 10150
    },
    {
      "epoch": 1.4514285714285715,
      "grad_norm": 1.135928988456726,
      "learning_rate": 0.00010323809523809525,
      "loss": 2.6959,
      "step": 10160
    },
    {
      "epoch": 1.4528571428571428,
      "grad_norm": 1.0107684135437012,
      "learning_rate": 0.00010314285714285713,
      "loss": 2.8794,
      "step": 10170
    },
    {
      "epoch": 1.4542857142857142,
      "grad_norm": 1.0565152168273926,
      "learning_rate": 0.00010304761904761907,
      "loss": 2.7476,
      "step": 10180
    },
    {
      "epoch": 1.4557142857142857,
      "grad_norm": 1.4058486223220825,
      "learning_rate": 0.00010295238095238095,
      "loss": 2.853,
      "step": 10190
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 1.2382348775863647,
      "learning_rate": 0.00010285714285714286,
      "loss": 2.8573,
      "step": 10200
    },
    {
      "epoch": 1.4585714285714286,
      "grad_norm": 1.0533729791641235,
      "learning_rate": 0.00010276190476190477,
      "loss": 2.9093,
      "step": 10210
    },
    {
      "epoch": 1.46,
      "grad_norm": 1.0704683065414429,
      "learning_rate": 0.00010266666666666666,
      "loss": 2.6826,
      "step": 10220
    },
    {
      "epoch": 1.4614285714285713,
      "grad_norm": 1.2839593887329102,
      "learning_rate": 0.00010257142857142858,
      "loss": 2.7872,
      "step": 10230
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 10.706841468811035,
      "learning_rate": 0.00010247619047619048,
      "loss": 2.7608,
      "step": 10240
    },
    {
      "epoch": 1.4642857142857144,
      "grad_norm": 0.9212947487831116,
      "learning_rate": 0.00010238095238095237,
      "loss": 2.757,
      "step": 10250
    },
    {
      "epoch": 1.4657142857142857,
      "grad_norm": 1.4961496591567993,
      "learning_rate": 0.00010228571428571429,
      "loss": 2.743,
      "step": 10260
    },
    {
      "epoch": 1.467142857142857,
      "grad_norm": 1.308641791343689,
      "learning_rate": 0.00010219047619047619,
      "loss": 2.6558,
      "step": 10270
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 1.037529468536377,
      "learning_rate": 0.00010209523809523811,
      "loss": 2.6001,
      "step": 10280
    },
    {
      "epoch": 1.47,
      "grad_norm": 1.535847783088684,
      "learning_rate": 0.00010200000000000001,
      "loss": 2.7923,
      "step": 10290
    },
    {
      "epoch": 1.4714285714285715,
      "grad_norm": 1.1209940910339355,
      "learning_rate": 0.0001019047619047619,
      "loss": 2.6868,
      "step": 10300
    },
    {
      "epoch": 1.4728571428571429,
      "grad_norm": 0.9902236461639404,
      "learning_rate": 0.00010180952380952382,
      "loss": 2.6645,
      "step": 10310
    },
    {
      "epoch": 1.4742857142857142,
      "grad_norm": 0.9132638573646545,
      "learning_rate": 0.00010171428571428572,
      "loss": 2.7443,
      "step": 10320
    },
    {
      "epoch": 1.4757142857142858,
      "grad_norm": 1.4570488929748535,
      "learning_rate": 0.00010161904761904761,
      "loss": 2.8278,
      "step": 10330
    },
    {
      "epoch": 1.477142857142857,
      "grad_norm": 0.9857407212257385,
      "learning_rate": 0.00010152380952380953,
      "loss": 2.7448,
      "step": 10340
    },
    {
      "epoch": 1.4785714285714286,
      "grad_norm": 1.4243499040603638,
      "learning_rate": 0.00010142857142857143,
      "loss": 2.7474,
      "step": 10350
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.1070321798324585,
      "learning_rate": 0.00010133333333333335,
      "loss": 2.6861,
      "step": 10360
    },
    {
      "epoch": 1.4814285714285713,
      "grad_norm": 1.1152772903442383,
      "learning_rate": 0.00010123809523809524,
      "loss": 2.7542,
      "step": 10370
    },
    {
      "epoch": 1.4828571428571429,
      "grad_norm": 1.1713263988494873,
      "learning_rate": 0.00010114285714285714,
      "loss": 2.6787,
      "step": 10380
    },
    {
      "epoch": 1.4842857142857142,
      "grad_norm": 1.1581073999404907,
      "learning_rate": 0.00010104761904761906,
      "loss": 2.8339,
      "step": 10390
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 1.2684941291809082,
      "learning_rate": 0.00010095238095238096,
      "loss": 2.7643,
      "step": 10400
    },
    {
      "epoch": 1.487142857142857,
      "grad_norm": 1.4435734748840332,
      "learning_rate": 0.00010085714285714288,
      "loss": 2.621,
      "step": 10410
    },
    {
      "epoch": 1.4885714285714284,
      "grad_norm": 1.4446183443069458,
      "learning_rate": 0.00010076190476190477,
      "loss": 2.7093,
      "step": 10420
    },
    {
      "epoch": 1.49,
      "grad_norm": 1.0107603073120117,
      "learning_rate": 0.00010066666666666667,
      "loss": 2.7873,
      "step": 10430
    },
    {
      "epoch": 1.4914285714285715,
      "grad_norm": 0.9233353137969971,
      "learning_rate": 0.00010057142857142859,
      "loss": 2.7036,
      "step": 10440
    },
    {
      "epoch": 1.4928571428571429,
      "grad_norm": 1.9167906045913696,
      "learning_rate": 0.00010047619047619048,
      "loss": 2.7517,
      "step": 10450
    },
    {
      "epoch": 1.4942857142857142,
      "grad_norm": 1.2758572101593018,
      "learning_rate": 0.00010038095238095238,
      "loss": 2.5852,
      "step": 10460
    },
    {
      "epoch": 1.4957142857142858,
      "grad_norm": 1.3298498392105103,
      "learning_rate": 0.0001002857142857143,
      "loss": 2.6871,
      "step": 10470
    },
    {
      "epoch": 1.497142857142857,
      "grad_norm": 1.2956629991531372,
      "learning_rate": 0.00010019047619047619,
      "loss": 2.5739,
      "step": 10480
    },
    {
      "epoch": 1.4985714285714287,
      "grad_norm": 0.9707157015800476,
      "learning_rate": 0.00010009523809523812,
      "loss": 2.7164,
      "step": 10490
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.5343830585479736,
      "learning_rate": 0.0001,
      "loss": 2.6968,
      "step": 10500
    },
    {
      "epoch": 1.5014285714285713,
      "grad_norm": 0.9173058867454529,
      "learning_rate": 9.990476190476191e-05,
      "loss": 2.8211,
      "step": 10510
    },
    {
      "epoch": 1.502857142857143,
      "grad_norm": 1.1874672174453735,
      "learning_rate": 9.980952380952382e-05,
      "loss": 2.7138,
      "step": 10520
    },
    {
      "epoch": 1.5042857142857144,
      "grad_norm": 1.1455957889556885,
      "learning_rate": 9.971428571428571e-05,
      "loss": 2.6854,
      "step": 10530
    },
    {
      "epoch": 1.5057142857142858,
      "grad_norm": 1.1851534843444824,
      "learning_rate": 9.961904761904762e-05,
      "loss": 2.8066,
      "step": 10540
    },
    {
      "epoch": 1.5071428571428571,
      "grad_norm": 1.3588720560073853,
      "learning_rate": 9.952380952380953e-05,
      "loss": 2.6123,
      "step": 10550
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 1.2448350191116333,
      "learning_rate": 9.942857142857144e-05,
      "loss": 2.6777,
      "step": 10560
    },
    {
      "epoch": 1.51,
      "grad_norm": 1.2168055772781372,
      "learning_rate": 9.933333333333334e-05,
      "loss": 2.457,
      "step": 10570
    },
    {
      "epoch": 1.5114285714285716,
      "grad_norm": 1.1985489130020142,
      "learning_rate": 9.923809523809524e-05,
      "loss": 2.7775,
      "step": 10580
    },
    {
      "epoch": 1.512857142857143,
      "grad_norm": 1.070730209350586,
      "learning_rate": 9.914285714285715e-05,
      "loss": 2.7103,
      "step": 10590
    },
    {
      "epoch": 1.5142857142857142,
      "grad_norm": 1.2936508655548096,
      "learning_rate": 9.904761904761905e-05,
      "loss": 2.7072,
      "step": 10600
    },
    {
      "epoch": 1.5157142857142856,
      "grad_norm": 0.9441297054290771,
      "learning_rate": 9.895238095238095e-05,
      "loss": 2.7587,
      "step": 10610
    },
    {
      "epoch": 1.5171428571428571,
      "grad_norm": 1.556125283241272,
      "learning_rate": 9.885714285714286e-05,
      "loss": 2.66,
      "step": 10620
    },
    {
      "epoch": 1.5185714285714287,
      "grad_norm": 1.0776252746582031,
      "learning_rate": 9.876190476190477e-05,
      "loss": 2.7051,
      "step": 10630
    },
    {
      "epoch": 1.52,
      "grad_norm": 1.044847846031189,
      "learning_rate": 9.866666666666668e-05,
      "loss": 2.7844,
      "step": 10640
    },
    {
      "epoch": 1.5214285714285714,
      "grad_norm": 1.3500579595565796,
      "learning_rate": 9.857142857142858e-05,
      "loss": 2.7812,
      "step": 10650
    },
    {
      "epoch": 1.522857142857143,
      "grad_norm": 1.8499646186828613,
      "learning_rate": 9.847619047619048e-05,
      "loss": 2.624,
      "step": 10660
    },
    {
      "epoch": 1.5242857142857142,
      "grad_norm": 1.5177465677261353,
      "learning_rate": 9.838095238095238e-05,
      "loss": 2.8363,
      "step": 10670
    },
    {
      "epoch": 1.5257142857142858,
      "grad_norm": 1.2667884826660156,
      "learning_rate": 9.828571428571429e-05,
      "loss": 2.5152,
      "step": 10680
    },
    {
      "epoch": 1.5271428571428571,
      "grad_norm": 0.9820287823677063,
      "learning_rate": 9.81904761904762e-05,
      "loss": 2.5807,
      "step": 10690
    },
    {
      "epoch": 1.5285714285714285,
      "grad_norm": 1.1049858331680298,
      "learning_rate": 9.80952380952381e-05,
      "loss": 2.6162,
      "step": 10700
    },
    {
      "epoch": 1.53,
      "grad_norm": 1.1280620098114014,
      "learning_rate": 9.8e-05,
      "loss": 2.7646,
      "step": 10710
    },
    {
      "epoch": 1.5314285714285716,
      "grad_norm": 1.078534722328186,
      "learning_rate": 9.790476190476191e-05,
      "loss": 2.6881,
      "step": 10720
    },
    {
      "epoch": 1.532857142857143,
      "grad_norm": 1.1032800674438477,
      "learning_rate": 9.780952380952382e-05,
      "loss": 2.7135,
      "step": 10730
    },
    {
      "epoch": 1.5342857142857143,
      "grad_norm": 1.183440089225769,
      "learning_rate": 9.771428571428572e-05,
      "loss": 2.8779,
      "step": 10740
    },
    {
      "epoch": 1.5357142857142856,
      "grad_norm": 1.021474003791809,
      "learning_rate": 9.761904761904762e-05,
      "loss": 2.7564,
      "step": 10750
    },
    {
      "epoch": 1.5371428571428571,
      "grad_norm": 1.3508563041687012,
      "learning_rate": 9.752380952380953e-05,
      "loss": 2.6625,
      "step": 10760
    },
    {
      "epoch": 1.5385714285714287,
      "grad_norm": 1.5147515535354614,
      "learning_rate": 9.742857142857143e-05,
      "loss": 2.9147,
      "step": 10770
    },
    {
      "epoch": 1.54,
      "grad_norm": 1.368409276008606,
      "learning_rate": 9.733333333333335e-05,
      "loss": 2.6901,
      "step": 10780
    },
    {
      "epoch": 1.5414285714285714,
      "grad_norm": 1.1317578554153442,
      "learning_rate": 9.723809523809524e-05,
      "loss": 2.5601,
      "step": 10790
    },
    {
      "epoch": 1.5428571428571427,
      "grad_norm": 1.1715317964553833,
      "learning_rate": 9.714285714285715e-05,
      "loss": 2.7209,
      "step": 10800
    },
    {
      "epoch": 1.5442857142857143,
      "grad_norm": 1.1758100986480713,
      "learning_rate": 9.704761904761905e-05,
      "loss": 2.789,
      "step": 10810
    },
    {
      "epoch": 1.5457142857142858,
      "grad_norm": 1.095139741897583,
      "learning_rate": 9.695238095238096e-05,
      "loss": 2.5675,
      "step": 10820
    },
    {
      "epoch": 1.5471428571428572,
      "grad_norm": 1.4918192625045776,
      "learning_rate": 9.685714285714286e-05,
      "loss": 2.7748,
      "step": 10830
    },
    {
      "epoch": 1.5485714285714285,
      "grad_norm": 1.1538479328155518,
      "learning_rate": 9.676190476190476e-05,
      "loss": 2.6888,
      "step": 10840
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.295916199684143,
      "learning_rate": 9.666666666666667e-05,
      "loss": 2.7147,
      "step": 10850
    },
    {
      "epoch": 1.5514285714285714,
      "grad_norm": 1.4987235069274902,
      "learning_rate": 9.657142857142858e-05,
      "loss": 2.6338,
      "step": 10860
    },
    {
      "epoch": 1.552857142857143,
      "grad_norm": 1.516513705253601,
      "learning_rate": 9.647619047619047e-05,
      "loss": 2.5366,
      "step": 10870
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 0.7073951959609985,
      "learning_rate": 9.638095238095238e-05,
      "loss": 2.7716,
      "step": 10880
    },
    {
      "epoch": 1.5557142857142856,
      "grad_norm": 1.302134394645691,
      "learning_rate": 9.628571428571429e-05,
      "loss": 2.761,
      "step": 10890
    },
    {
      "epoch": 1.5571428571428572,
      "grad_norm": 1.147050380706787,
      "learning_rate": 9.61904761904762e-05,
      "loss": 2.6291,
      "step": 10900
    },
    {
      "epoch": 1.5585714285714287,
      "grad_norm": 1.0648952722549438,
      "learning_rate": 9.60952380952381e-05,
      "loss": 2.7425,
      "step": 10910
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.196423053741455,
      "learning_rate": 9.6e-05,
      "loss": 2.657,
      "step": 10920
    },
    {
      "epoch": 1.5614285714285714,
      "grad_norm": 1.2189141511917114,
      "learning_rate": 9.59047619047619e-05,
      "loss": 2.7167,
      "step": 10930
    },
    {
      "epoch": 1.5628571428571427,
      "grad_norm": 1.1510213613510132,
      "learning_rate": 9.580952380952382e-05,
      "loss": 2.6066,
      "step": 10940
    },
    {
      "epoch": 1.5642857142857143,
      "grad_norm": 1.021507978439331,
      "learning_rate": 9.571428571428573e-05,
      "loss": 2.7863,
      "step": 10950
    },
    {
      "epoch": 1.5657142857142858,
      "grad_norm": 1.1403956413269043,
      "learning_rate": 9.561904761904761e-05,
      "loss": 2.6069,
      "step": 10960
    },
    {
      "epoch": 1.5671428571428572,
      "grad_norm": 1.3479677438735962,
      "learning_rate": 9.552380952380953e-05,
      "loss": 2.7473,
      "step": 10970
    },
    {
      "epoch": 1.5685714285714285,
      "grad_norm": 1.3119524717330933,
      "learning_rate": 9.542857142857143e-05,
      "loss": 2.5672,
      "step": 10980
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 1.8072274923324585,
      "learning_rate": 9.533333333333334e-05,
      "loss": 2.7044,
      "step": 10990
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 1.0592573881149292,
      "learning_rate": 9.523809523809524e-05,
      "loss": 2.5746,
      "step": 11000
    },
    {
      "epoch": 1.572857142857143,
      "grad_norm": 1.1938737630844116,
      "learning_rate": 9.514285714285714e-05,
      "loss": 2.7242,
      "step": 11010
    },
    {
      "epoch": 1.5742857142857143,
      "grad_norm": 1.5202233791351318,
      "learning_rate": 9.504761904761905e-05,
      "loss": 2.7176,
      "step": 11020
    },
    {
      "epoch": 1.5757142857142856,
      "grad_norm": 1.708870530128479,
      "learning_rate": 9.495238095238096e-05,
      "loss": 2.5934,
      "step": 11030
    },
    {
      "epoch": 1.5771428571428572,
      "grad_norm": 1.7333444356918335,
      "learning_rate": 9.485714285714287e-05,
      "loss": 2.6107,
      "step": 11040
    },
    {
      "epoch": 1.5785714285714287,
      "grad_norm": 1.4184184074401855,
      "learning_rate": 9.476190476190476e-05,
      "loss": 2.6811,
      "step": 11050
    },
    {
      "epoch": 1.58,
      "grad_norm": 1.2122776508331299,
      "learning_rate": 9.466666666666667e-05,
      "loss": 2.7482,
      "step": 11060
    },
    {
      "epoch": 1.5814285714285714,
      "grad_norm": 0.9880839586257935,
      "learning_rate": 9.457142857142858e-05,
      "loss": 2.6599,
      "step": 11070
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 1.0182245969772339,
      "learning_rate": 9.447619047619048e-05,
      "loss": 2.7786,
      "step": 11080
    },
    {
      "epoch": 1.5842857142857143,
      "grad_norm": 1.161201000213623,
      "learning_rate": 9.438095238095238e-05,
      "loss": 2.6984,
      "step": 11090
    },
    {
      "epoch": 1.5857142857142859,
      "grad_norm": 1.3000396490097046,
      "learning_rate": 9.428571428571429e-05,
      "loss": 2.5908,
      "step": 11100
    },
    {
      "epoch": 1.5871428571428572,
      "grad_norm": 1.093526840209961,
      "learning_rate": 9.41904761904762e-05,
      "loss": 2.7912,
      "step": 11110
    },
    {
      "epoch": 1.5885714285714285,
      "grad_norm": 1.321781039237976,
      "learning_rate": 9.40952380952381e-05,
      "loss": 2.5656,
      "step": 11120
    },
    {
      "epoch": 1.5899999999999999,
      "grad_norm": 1.1584713459014893,
      "learning_rate": 9.4e-05,
      "loss": 2.7742,
      "step": 11130
    },
    {
      "epoch": 1.5914285714285714,
      "grad_norm": 1.3619440793991089,
      "learning_rate": 9.390476190476191e-05,
      "loss": 2.6278,
      "step": 11140
    },
    {
      "epoch": 1.592857142857143,
      "grad_norm": 1.076982021331787,
      "learning_rate": 9.380952380952381e-05,
      "loss": 2.7317,
      "step": 11150
    },
    {
      "epoch": 1.5942857142857143,
      "grad_norm": 1.3964803218841553,
      "learning_rate": 9.371428571428572e-05,
      "loss": 2.561,
      "step": 11160
    },
    {
      "epoch": 1.5957142857142856,
      "grad_norm": 1.2387505769729614,
      "learning_rate": 9.361904761904763e-05,
      "loss": 2.743,
      "step": 11170
    },
    {
      "epoch": 1.5971428571428572,
      "grad_norm": 1.0466033220291138,
      "learning_rate": 9.352380952380952e-05,
      "loss": 2.7492,
      "step": 11180
    },
    {
      "epoch": 1.5985714285714285,
      "grad_norm": 1.3950673341751099,
      "learning_rate": 9.342857142857143e-05,
      "loss": 2.6922,
      "step": 11190
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.45548415184021,
      "learning_rate": 9.333333333333334e-05,
      "loss": 2.6522,
      "step": 11200
    },
    {
      "epoch": 1.6014285714285714,
      "grad_norm": 1.1073760986328125,
      "learning_rate": 9.323809523809525e-05,
      "loss": 2.5383,
      "step": 11210
    },
    {
      "epoch": 1.6028571428571428,
      "grad_norm": 1.483508586883545,
      "learning_rate": 9.314285714285715e-05,
      "loss": 2.6439,
      "step": 11220
    },
    {
      "epoch": 1.6042857142857143,
      "grad_norm": 1.3492708206176758,
      "learning_rate": 9.304761904761905e-05,
      "loss": 2.6989,
      "step": 11230
    },
    {
      "epoch": 1.6057142857142859,
      "grad_norm": 1.7278246879577637,
      "learning_rate": 9.295238095238096e-05,
      "loss": 2.6214,
      "step": 11240
    },
    {
      "epoch": 1.6071428571428572,
      "grad_norm": 0.9531589150428772,
      "learning_rate": 9.285714285714286e-05,
      "loss": 2.8501,
      "step": 11250
    },
    {
      "epoch": 1.6085714285714285,
      "grad_norm": 1.372806191444397,
      "learning_rate": 9.276190476190476e-05,
      "loss": 2.8688,
      "step": 11260
    },
    {
      "epoch": 1.6099999999999999,
      "grad_norm": 1.1524150371551514,
      "learning_rate": 9.266666666666666e-05,
      "loss": 2.6966,
      "step": 11270
    },
    {
      "epoch": 1.6114285714285714,
      "grad_norm": 1.0649408102035522,
      "learning_rate": 9.257142857142858e-05,
      "loss": 2.7058,
      "step": 11280
    },
    {
      "epoch": 1.612857142857143,
      "grad_norm": 1.0634015798568726,
      "learning_rate": 9.247619047619048e-05,
      "loss": 2.7962,
      "step": 11290
    },
    {
      "epoch": 1.6142857142857143,
      "grad_norm": 1.563987374305725,
      "learning_rate": 9.238095238095239e-05,
      "loss": 2.6567,
      "step": 11300
    },
    {
      "epoch": 1.6157142857142857,
      "grad_norm": 1.145975947380066,
      "learning_rate": 9.228571428571429e-05,
      "loss": 2.6172,
      "step": 11310
    },
    {
      "epoch": 1.617142857142857,
      "grad_norm": 1.2667330503463745,
      "learning_rate": 9.21904761904762e-05,
      "loss": 2.7604,
      "step": 11320
    },
    {
      "epoch": 1.6185714285714285,
      "grad_norm": 1.1237999200820923,
      "learning_rate": 9.20952380952381e-05,
      "loss": 2.7785,
      "step": 11330
    },
    {
      "epoch": 1.62,
      "grad_norm": 1.173518180847168,
      "learning_rate": 9.200000000000001e-05,
      "loss": 2.6649,
      "step": 11340
    },
    {
      "epoch": 1.6214285714285714,
      "grad_norm": 1.0285836458206177,
      "learning_rate": 9.19047619047619e-05,
      "loss": 2.6248,
      "step": 11350
    },
    {
      "epoch": 1.6228571428571428,
      "grad_norm": 1.3620797395706177,
      "learning_rate": 9.18095238095238e-05,
      "loss": 2.6271,
      "step": 11360
    },
    {
      "epoch": 1.6242857142857143,
      "grad_norm": 1.03590726852417,
      "learning_rate": 9.171428571428572e-05,
      "loss": 2.4833,
      "step": 11370
    },
    {
      "epoch": 1.6257142857142857,
      "grad_norm": 1.77328360080719,
      "learning_rate": 9.161904761904763e-05,
      "loss": 2.8022,
      "step": 11380
    },
    {
      "epoch": 1.6271428571428572,
      "grad_norm": 1.3934295177459717,
      "learning_rate": 9.152380952380953e-05,
      "loss": 2.6325,
      "step": 11390
    },
    {
      "epoch": 1.6285714285714286,
      "grad_norm": 1.1055980920791626,
      "learning_rate": 9.142857142857143e-05,
      "loss": 2.5719,
      "step": 11400
    },
    {
      "epoch": 1.63,
      "grad_norm": 1.5230202674865723,
      "learning_rate": 9.133333333333334e-05,
      "loss": 2.6556,
      "step": 11410
    },
    {
      "epoch": 1.6314285714285715,
      "grad_norm": 1.2543814182281494,
      "learning_rate": 9.123809523809524e-05,
      "loss": 2.6461,
      "step": 11420
    },
    {
      "epoch": 1.632857142857143,
      "grad_norm": 1.2418062686920166,
      "learning_rate": 9.114285714285716e-05,
      "loss": 2.597,
      "step": 11430
    },
    {
      "epoch": 1.6342857142857143,
      "grad_norm": 1.7355514764785767,
      "learning_rate": 9.104761904761904e-05,
      "loss": 2.648,
      "step": 11440
    },
    {
      "epoch": 1.6357142857142857,
      "grad_norm": 1.1035858392715454,
      "learning_rate": 9.095238095238096e-05,
      "loss": 2.7247,
      "step": 11450
    },
    {
      "epoch": 1.637142857142857,
      "grad_norm": 1.0910212993621826,
      "learning_rate": 9.085714285714286e-05,
      "loss": 2.6198,
      "step": 11460
    },
    {
      "epoch": 1.6385714285714286,
      "grad_norm": 1.0188212394714355,
      "learning_rate": 9.076190476190477e-05,
      "loss": 2.8131,
      "step": 11470
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 1.242250919342041,
      "learning_rate": 9.066666666666667e-05,
      "loss": 2.6264,
      "step": 11480
    },
    {
      "epoch": 1.6414285714285715,
      "grad_norm": 0.9955025315284729,
      "learning_rate": 9.057142857142857e-05,
      "loss": 2.6616,
      "step": 11490
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 1.4160529375076294,
      "learning_rate": 9.047619047619048e-05,
      "loss": 2.6511,
      "step": 11500
    },
    {
      "epoch": 1.6442857142857141,
      "grad_norm": 1.4848649501800537,
      "learning_rate": 9.03809523809524e-05,
      "loss": 2.647,
      "step": 11510
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 1.0681763887405396,
      "learning_rate": 9.028571428571428e-05,
      "loss": 2.5857,
      "step": 11520
    },
    {
      "epoch": 1.6471428571428572,
      "grad_norm": 1.1031787395477295,
      "learning_rate": 9.019047619047619e-05,
      "loss": 2.7709,
      "step": 11530
    },
    {
      "epoch": 1.6485714285714286,
      "grad_norm": 1.0269092321395874,
      "learning_rate": 9.00952380952381e-05,
      "loss": 2.5549,
      "step": 11540
    },
    {
      "epoch": 1.65,
      "grad_norm": 1.10054349899292,
      "learning_rate": 9e-05,
      "loss": 2.642,
      "step": 11550
    },
    {
      "epoch": 1.6514285714285715,
      "grad_norm": 1.815874695777893,
      "learning_rate": 8.990476190476191e-05,
      "loss": 2.5093,
      "step": 11560
    },
    {
      "epoch": 1.6528571428571428,
      "grad_norm": 1.5723267793655396,
      "learning_rate": 8.980952380952381e-05,
      "loss": 2.7074,
      "step": 11570
    },
    {
      "epoch": 1.6542857142857144,
      "grad_norm": 1.4043455123901367,
      "learning_rate": 8.971428571428571e-05,
      "loss": 2.7658,
      "step": 11580
    },
    {
      "epoch": 1.6557142857142857,
      "grad_norm": 1.4518749713897705,
      "learning_rate": 8.961904761904762e-05,
      "loss": 2.6566,
      "step": 11590
    },
    {
      "epoch": 1.657142857142857,
      "grad_norm": 1.182125210762024,
      "learning_rate": 8.952380952380953e-05,
      "loss": 2.6585,
      "step": 11600
    },
    {
      "epoch": 1.6585714285714286,
      "grad_norm": 1.2286232709884644,
      "learning_rate": 8.942857142857142e-05,
      "loss": 2.771,
      "step": 11610
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 1.175281047821045,
      "learning_rate": 8.933333333333334e-05,
      "loss": 2.8637,
      "step": 11620
    },
    {
      "epoch": 1.6614285714285715,
      "grad_norm": 1.176466941833496,
      "learning_rate": 8.923809523809524e-05,
      "loss": 2.5648,
      "step": 11630
    },
    {
      "epoch": 1.6628571428571428,
      "grad_norm": 1.3068650960922241,
      "learning_rate": 8.914285714285715e-05,
      "loss": 2.7657,
      "step": 11640
    },
    {
      "epoch": 1.6642857142857141,
      "grad_norm": 1.3393865823745728,
      "learning_rate": 8.904761904761905e-05,
      "loss": 2.6612,
      "step": 11650
    },
    {
      "epoch": 1.6657142857142857,
      "grad_norm": 1.64877188205719,
      "learning_rate": 8.895238095238095e-05,
      "loss": 2.7827,
      "step": 11660
    },
    {
      "epoch": 1.6671428571428573,
      "grad_norm": 1.7360769510269165,
      "learning_rate": 8.885714285714286e-05,
      "loss": 2.8269,
      "step": 11670
    },
    {
      "epoch": 1.6685714285714286,
      "grad_norm": 1.4494539499282837,
      "learning_rate": 8.876190476190477e-05,
      "loss": 2.6907,
      "step": 11680
    },
    {
      "epoch": 1.67,
      "grad_norm": 1.0517973899841309,
      "learning_rate": 8.866666666666668e-05,
      "loss": 2.5284,
      "step": 11690
    },
    {
      "epoch": 1.6714285714285713,
      "grad_norm": 1.3200623989105225,
      "learning_rate": 8.857142857142857e-05,
      "loss": 2.7375,
      "step": 11700
    },
    {
      "epoch": 1.6728571428571428,
      "grad_norm": 1.1103544235229492,
      "learning_rate": 8.847619047619048e-05,
      "loss": 2.7812,
      "step": 11710
    },
    {
      "epoch": 1.6742857142857144,
      "grad_norm": 1.2409659624099731,
      "learning_rate": 8.838095238095239e-05,
      "loss": 2.7994,
      "step": 11720
    },
    {
      "epoch": 1.6757142857142857,
      "grad_norm": 1.3489207029342651,
      "learning_rate": 8.828571428571429e-05,
      "loss": 2.705,
      "step": 11730
    },
    {
      "epoch": 1.677142857142857,
      "grad_norm": 1.928948998451233,
      "learning_rate": 8.819047619047619e-05,
      "loss": 2.6184,
      "step": 11740
    },
    {
      "epoch": 1.6785714285714286,
      "grad_norm": 0.9927406311035156,
      "learning_rate": 8.80952380952381e-05,
      "loss": 2.682,
      "step": 11750
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 1.5332709550857544,
      "learning_rate": 8.800000000000001e-05,
      "loss": 2.817,
      "step": 11760
    },
    {
      "epoch": 1.6814285714285715,
      "grad_norm": 1.5759018659591675,
      "learning_rate": 8.790476190476191e-05,
      "loss": 2.7479,
      "step": 11770
    },
    {
      "epoch": 1.6828571428571428,
      "grad_norm": 1.2032612562179565,
      "learning_rate": 8.78095238095238e-05,
      "loss": 2.7046,
      "step": 11780
    },
    {
      "epoch": 1.6842857142857142,
      "grad_norm": 1.3327651023864746,
      "learning_rate": 8.771428571428572e-05,
      "loss": 2.6322,
      "step": 11790
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 1.8401230573654175,
      "learning_rate": 8.761904761904762e-05,
      "loss": 2.6269,
      "step": 11800
    },
    {
      "epoch": 1.6871428571428573,
      "grad_norm": 1.2238610982894897,
      "learning_rate": 8.752380952380953e-05,
      "loss": 2.6475,
      "step": 11810
    },
    {
      "epoch": 1.6885714285714286,
      "grad_norm": 1.0817830562591553,
      "learning_rate": 8.742857142857144e-05,
      "loss": 2.7543,
      "step": 11820
    },
    {
      "epoch": 1.69,
      "grad_norm": 1.0211890935897827,
      "learning_rate": 8.733333333333333e-05,
      "loss": 2.6595,
      "step": 11830
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 1.6938751935958862,
      "learning_rate": 8.723809523809524e-05,
      "loss": 2.7795,
      "step": 11840
    },
    {
      "epoch": 1.6928571428571428,
      "grad_norm": 1.6386139392852783,
      "learning_rate": 8.714285714285715e-05,
      "loss": 2.8151,
      "step": 11850
    },
    {
      "epoch": 1.6942857142857144,
      "grad_norm": 1.206896185874939,
      "learning_rate": 8.704761904761906e-05,
      "loss": 2.6623,
      "step": 11860
    },
    {
      "epoch": 1.6957142857142857,
      "grad_norm": 1.3168864250183105,
      "learning_rate": 8.695238095238095e-05,
      "loss": 2.7064,
      "step": 11870
    },
    {
      "epoch": 1.697142857142857,
      "grad_norm": 0.8177430033683777,
      "learning_rate": 8.685714285714286e-05,
      "loss": 2.6575,
      "step": 11880
    },
    {
      "epoch": 1.6985714285714286,
      "grad_norm": 1.1393200159072876,
      "learning_rate": 8.676190476190477e-05,
      "loss": 2.6101,
      "step": 11890
    },
    {
      "epoch": 1.7,
      "grad_norm": 3.0536036491394043,
      "learning_rate": 8.666666666666667e-05,
      "loss": 2.7307,
      "step": 11900
    },
    {
      "epoch": 1.7014285714285715,
      "grad_norm": 1.2368508577346802,
      "learning_rate": 8.657142857142858e-05,
      "loss": 2.7521,
      "step": 11910
    },
    {
      "epoch": 1.7028571428571428,
      "grad_norm": 1.1475368738174438,
      "learning_rate": 8.647619047619047e-05,
      "loss": 2.715,
      "step": 11920
    },
    {
      "epoch": 1.7042857142857142,
      "grad_norm": 1.4244115352630615,
      "learning_rate": 8.638095238095239e-05,
      "loss": 2.7797,
      "step": 11930
    },
    {
      "epoch": 1.7057142857142857,
      "grad_norm": 1.2775449752807617,
      "learning_rate": 8.62857142857143e-05,
      "loss": 2.7216,
      "step": 11940
    },
    {
      "epoch": 1.7071428571428573,
      "grad_norm": 0.9462884664535522,
      "learning_rate": 8.61904761904762e-05,
      "loss": 2.6604,
      "step": 11950
    },
    {
      "epoch": 1.7085714285714286,
      "grad_norm": 1.0226622819900513,
      "learning_rate": 8.60952380952381e-05,
      "loss": 2.4532,
      "step": 11960
    },
    {
      "epoch": 1.71,
      "grad_norm": 1.1139791011810303,
      "learning_rate": 8.6e-05,
      "loss": 2.7134,
      "step": 11970
    },
    {
      "epoch": 1.7114285714285713,
      "grad_norm": 1.3386132717132568,
      "learning_rate": 8.59047619047619e-05,
      "loss": 2.5478,
      "step": 11980
    },
    {
      "epoch": 1.7128571428571429,
      "grad_norm": 1.124139428138733,
      "learning_rate": 8.580952380952382e-05,
      "loss": 2.7605,
      "step": 11990
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.2507898807525635,
      "learning_rate": 8.571428571428571e-05,
      "loss": 2.6389,
      "step": 12000
    },
    {
      "epoch": 1.7157142857142857,
      "grad_norm": 1.8712536096572876,
      "learning_rate": 8.561904761904762e-05,
      "loss": 2.6916,
      "step": 12010
    },
    {
      "epoch": 1.717142857142857,
      "grad_norm": 1.9426259994506836,
      "learning_rate": 8.552380952380953e-05,
      "loss": 2.5198,
      "step": 12020
    },
    {
      "epoch": 1.7185714285714284,
      "grad_norm": 1.517775297164917,
      "learning_rate": 8.542857142857144e-05,
      "loss": 2.7326,
      "step": 12030
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.9664031863212585,
      "learning_rate": 8.533333333333334e-05,
      "loss": 2.6553,
      "step": 12040
    },
    {
      "epoch": 1.7214285714285715,
      "grad_norm": 1.2221344709396362,
      "learning_rate": 8.523809523809524e-05,
      "loss": 2.7328,
      "step": 12050
    },
    {
      "epoch": 1.7228571428571429,
      "grad_norm": 1.0564912557601929,
      "learning_rate": 8.514285714285714e-05,
      "loss": 2.7325,
      "step": 12060
    },
    {
      "epoch": 1.7242857142857142,
      "grad_norm": 1.1540838479995728,
      "learning_rate": 8.504761904761905e-05,
      "loss": 2.5905,
      "step": 12070
    },
    {
      "epoch": 1.7257142857142858,
      "grad_norm": 1.132859706878662,
      "learning_rate": 8.495238095238096e-05,
      "loss": 2.6849,
      "step": 12080
    },
    {
      "epoch": 1.727142857142857,
      "grad_norm": 1.0913327932357788,
      "learning_rate": 8.485714285714285e-05,
      "loss": 2.7814,
      "step": 12090
    },
    {
      "epoch": 1.7285714285714286,
      "grad_norm": 0.7775243520736694,
      "learning_rate": 8.476190476190477e-05,
      "loss": 2.7406,
      "step": 12100
    },
    {
      "epoch": 1.73,
      "grad_norm": 1.166414737701416,
      "learning_rate": 8.466666666666667e-05,
      "loss": 2.6341,
      "step": 12110
    },
    {
      "epoch": 1.7314285714285713,
      "grad_norm": 1.0518946647644043,
      "learning_rate": 8.457142857142858e-05,
      "loss": 2.7319,
      "step": 12120
    },
    {
      "epoch": 1.7328571428571429,
      "grad_norm": 1.0365509986877441,
      "learning_rate": 8.447619047619048e-05,
      "loss": 2.8201,
      "step": 12130
    },
    {
      "epoch": 1.7342857142857144,
      "grad_norm": 1.386252522468567,
      "learning_rate": 8.438095238095238e-05,
      "loss": 2.8321,
      "step": 12140
    },
    {
      "epoch": 1.7357142857142858,
      "grad_norm": 1.1029183864593506,
      "learning_rate": 8.428571428571429e-05,
      "loss": 2.7239,
      "step": 12150
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 0.9806149005889893,
      "learning_rate": 8.41904761904762e-05,
      "loss": 2.749,
      "step": 12160
    },
    {
      "epoch": 1.7385714285714284,
      "grad_norm": 1.3079546689987183,
      "learning_rate": 8.40952380952381e-05,
      "loss": 2.6945,
      "step": 12170
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.3843204975128174,
      "learning_rate": 8.4e-05,
      "loss": 2.6244,
      "step": 12180
    },
    {
      "epoch": 1.7414285714285715,
      "grad_norm": 1.3116439580917358,
      "learning_rate": 8.390476190476191e-05,
      "loss": 2.8294,
      "step": 12190
    },
    {
      "epoch": 1.7428571428571429,
      "grad_norm": 1.28694748878479,
      "learning_rate": 8.380952380952382e-05,
      "loss": 2.5656,
      "step": 12200
    },
    {
      "epoch": 1.7442857142857142,
      "grad_norm": 1.0290155410766602,
      "learning_rate": 8.371428571428572e-05,
      "loss": 2.7105,
      "step": 12210
    },
    {
      "epoch": 1.7457142857142856,
      "grad_norm": 1.2824997901916504,
      "learning_rate": 8.361904761904762e-05,
      "loss": 2.6252,
      "step": 12220
    },
    {
      "epoch": 1.747142857142857,
      "grad_norm": 1.3584309816360474,
      "learning_rate": 8.352380952380952e-05,
      "loss": 2.7741,
      "step": 12230
    },
    {
      "epoch": 1.7485714285714287,
      "grad_norm": 1.2120413780212402,
      "learning_rate": 8.342857142857143e-05,
      "loss": 2.6278,
      "step": 12240
    },
    {
      "epoch": 1.75,
      "grad_norm": 1.2786327600479126,
      "learning_rate": 8.333333333333334e-05,
      "loss": 2.7714,
      "step": 12250
    },
    {
      "epoch": 1.7514285714285713,
      "grad_norm": 1.4176526069641113,
      "learning_rate": 8.323809523809523e-05,
      "loss": 2.7015,
      "step": 12260
    },
    {
      "epoch": 1.752857142857143,
      "grad_norm": 1.3406773805618286,
      "learning_rate": 8.314285714285715e-05,
      "loss": 2.5349,
      "step": 12270
    },
    {
      "epoch": 1.7542857142857144,
      "grad_norm": 1.1134719848632812,
      "learning_rate": 8.304761904761905e-05,
      "loss": 2.7797,
      "step": 12280
    },
    {
      "epoch": 1.7557142857142858,
      "grad_norm": 1.2271071672439575,
      "learning_rate": 8.295238095238096e-05,
      "loss": 2.671,
      "step": 12290
    },
    {
      "epoch": 1.7571428571428571,
      "grad_norm": 1.0410441160202026,
      "learning_rate": 8.285714285714287e-05,
      "loss": 2.6759,
      "step": 12300
    },
    {
      "epoch": 1.7585714285714285,
      "grad_norm": 1.1151602268218994,
      "learning_rate": 8.276190476190476e-05,
      "loss": 2.6632,
      "step": 12310
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.0314319133758545,
      "learning_rate": 8.266666666666667e-05,
      "loss": 2.5373,
      "step": 12320
    },
    {
      "epoch": 1.7614285714285716,
      "grad_norm": 1.1066747903823853,
      "learning_rate": 8.257142857142858e-05,
      "loss": 2.7376,
      "step": 12330
    },
    {
      "epoch": 1.762857142857143,
      "grad_norm": 1.1840800046920776,
      "learning_rate": 8.247619047619049e-05,
      "loss": 2.8043,
      "step": 12340
    },
    {
      "epoch": 1.7642857142857142,
      "grad_norm": 1.2601423263549805,
      "learning_rate": 8.238095238095238e-05,
      "loss": 2.7032,
      "step": 12350
    },
    {
      "epoch": 1.7657142857142856,
      "grad_norm": 1.1460283994674683,
      "learning_rate": 8.228571428571429e-05,
      "loss": 2.7557,
      "step": 12360
    },
    {
      "epoch": 1.7671428571428571,
      "grad_norm": 1.1669621467590332,
      "learning_rate": 8.21904761904762e-05,
      "loss": 2.5634,
      "step": 12370
    },
    {
      "epoch": 1.7685714285714287,
      "grad_norm": 1.400872826576233,
      "learning_rate": 8.20952380952381e-05,
      "loss": 2.8823,
      "step": 12380
    },
    {
      "epoch": 1.77,
      "grad_norm": 1.054310917854309,
      "learning_rate": 8.2e-05,
      "loss": 2.6129,
      "step": 12390
    },
    {
      "epoch": 1.7714285714285714,
      "grad_norm": 1.039164662361145,
      "learning_rate": 8.19047619047619e-05,
      "loss": 2.6381,
      "step": 12400
    },
    {
      "epoch": 1.772857142857143,
      "grad_norm": 1.4491405487060547,
      "learning_rate": 8.180952380952381e-05,
      "loss": 2.8853,
      "step": 12410
    },
    {
      "epoch": 1.7742857142857142,
      "grad_norm": 0.8190157413482666,
      "learning_rate": 8.171428571428572e-05,
      "loss": 2.6297,
      "step": 12420
    },
    {
      "epoch": 1.7757142857142858,
      "grad_norm": 1.4729044437408447,
      "learning_rate": 8.161904761904763e-05,
      "loss": 2.7692,
      "step": 12430
    },
    {
      "epoch": 1.7771428571428571,
      "grad_norm": 1.4322606325149536,
      "learning_rate": 8.152380952380953e-05,
      "loss": 2.5903,
      "step": 12440
    },
    {
      "epoch": 1.7785714285714285,
      "grad_norm": 1.7501628398895264,
      "learning_rate": 8.142857142857143e-05,
      "loss": 2.6726,
      "step": 12450
    },
    {
      "epoch": 1.78,
      "grad_norm": 1.10633385181427,
      "learning_rate": 8.133333333333334e-05,
      "loss": 2.6108,
      "step": 12460
    },
    {
      "epoch": 1.7814285714285716,
      "grad_norm": 1.1027441024780273,
      "learning_rate": 8.123809523809525e-05,
      "loss": 2.6168,
      "step": 12470
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 1.308928370475769,
      "learning_rate": 8.114285714285714e-05,
      "loss": 2.7862,
      "step": 12480
    },
    {
      "epoch": 1.7842857142857143,
      "grad_norm": 1.367427110671997,
      "learning_rate": 8.104761904761905e-05,
      "loss": 2.5544,
      "step": 12490
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 1.2669737339019775,
      "learning_rate": 8.095238095238096e-05,
      "loss": 2.6051,
      "step": 12500
    },
    {
      "epoch": 1.7871428571428571,
      "grad_norm": 1.1992462873458862,
      "learning_rate": 8.085714285714287e-05,
      "loss": 2.5978,
      "step": 12510
    },
    {
      "epoch": 1.7885714285714287,
      "grad_norm": 1.2230452299118042,
      "learning_rate": 8.076190476190475e-05,
      "loss": 2.8305,
      "step": 12520
    },
    {
      "epoch": 1.79,
      "grad_norm": 0.8797463178634644,
      "learning_rate": 8.066666666666667e-05,
      "loss": 2.6863,
      "step": 12530
    },
    {
      "epoch": 1.7914285714285714,
      "grad_norm": 1.1127008199691772,
      "learning_rate": 8.057142857142857e-05,
      "loss": 2.8584,
      "step": 12540
    },
    {
      "epoch": 1.7928571428571427,
      "grad_norm": 0.9435140490531921,
      "learning_rate": 8.047619047619048e-05,
      "loss": 2.7524,
      "step": 12550
    },
    {
      "epoch": 1.7942857142857143,
      "grad_norm": 1.8654661178588867,
      "learning_rate": 8.03809523809524e-05,
      "loss": 2.9371,
      "step": 12560
    },
    {
      "epoch": 1.7957142857142858,
      "grad_norm": 1.143955945968628,
      "learning_rate": 8.028571428571428e-05,
      "loss": 2.6844,
      "step": 12570
    },
    {
      "epoch": 1.7971428571428572,
      "grad_norm": 0.9117718935012817,
      "learning_rate": 8.01904761904762e-05,
      "loss": 2.6119,
      "step": 12580
    },
    {
      "epoch": 1.7985714285714285,
      "grad_norm": 1.7340666055679321,
      "learning_rate": 8.00952380952381e-05,
      "loss": 2.8039,
      "step": 12590
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.157420039176941,
      "learning_rate": 8e-05,
      "loss": 2.7699,
      "step": 12600
    },
    {
      "epoch": 1.8014285714285714,
      "grad_norm": 1.2429194450378418,
      "learning_rate": 7.990476190476191e-05,
      "loss": 2.6797,
      "step": 12610
    },
    {
      "epoch": 1.802857142857143,
      "grad_norm": 1.51498544216156,
      "learning_rate": 7.980952380952381e-05,
      "loss": 2.6345,
      "step": 12620
    },
    {
      "epoch": 1.8042857142857143,
      "grad_norm": 1.279163122177124,
      "learning_rate": 7.971428571428572e-05,
      "loss": 2.7356,
      "step": 12630
    },
    {
      "epoch": 1.8057142857142856,
      "grad_norm": 1.5468217134475708,
      "learning_rate": 7.961904761904763e-05,
      "loss": 2.7812,
      "step": 12640
    },
    {
      "epoch": 1.8071428571428572,
      "grad_norm": 1.1789133548736572,
      "learning_rate": 7.952380952380952e-05,
      "loss": 2.6831,
      "step": 12650
    },
    {
      "epoch": 1.8085714285714287,
      "grad_norm": 1.0696395635604858,
      "learning_rate": 7.942857142857143e-05,
      "loss": 2.7532,
      "step": 12660
    },
    {
      "epoch": 1.81,
      "grad_norm": 1.0009547472000122,
      "learning_rate": 7.933333333333334e-05,
      "loss": 2.7866,
      "step": 12670
    },
    {
      "epoch": 1.8114285714285714,
      "grad_norm": 1.3239715099334717,
      "learning_rate": 7.923809523809524e-05,
      "loss": 2.8598,
      "step": 12680
    },
    {
      "epoch": 1.8128571428571427,
      "grad_norm": 1.2238836288452148,
      "learning_rate": 7.914285714285715e-05,
      "loss": 2.7164,
      "step": 12690
    },
    {
      "epoch": 1.8142857142857143,
      "grad_norm": 1.2856760025024414,
      "learning_rate": 7.904761904761905e-05,
      "loss": 2.7295,
      "step": 12700
    },
    {
      "epoch": 1.8157142857142858,
      "grad_norm": 1.386863112449646,
      "learning_rate": 7.895238095238095e-05,
      "loss": 2.5035,
      "step": 12710
    },
    {
      "epoch": 1.8171428571428572,
      "grad_norm": 1.8137898445129395,
      "learning_rate": 7.885714285714286e-05,
      "loss": 2.7801,
      "step": 12720
    },
    {
      "epoch": 1.8185714285714285,
      "grad_norm": 1.083863377571106,
      "learning_rate": 7.876190476190477e-05,
      "loss": 2.544,
      "step": 12730
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 1.0439866781234741,
      "learning_rate": 7.866666666666666e-05,
      "loss": 2.6361,
      "step": 12740
    },
    {
      "epoch": 1.8214285714285714,
      "grad_norm": 1.1163214445114136,
      "learning_rate": 7.857142857142858e-05,
      "loss": 2.6592,
      "step": 12750
    },
    {
      "epoch": 1.822857142857143,
      "grad_norm": 0.8680307269096375,
      "learning_rate": 7.847619047619048e-05,
      "loss": 2.6992,
      "step": 12760
    },
    {
      "epoch": 1.8242857142857143,
      "grad_norm": 1.1251983642578125,
      "learning_rate": 7.838095238095239e-05,
      "loss": 2.6428,
      "step": 12770
    },
    {
      "epoch": 1.8257142857142856,
      "grad_norm": 1.6352559328079224,
      "learning_rate": 7.828571428571429e-05,
      "loss": 2.6474,
      "step": 12780
    },
    {
      "epoch": 1.8271428571428572,
      "grad_norm": 1.7530242204666138,
      "learning_rate": 7.819047619047619e-05,
      "loss": 2.7938,
      "step": 12790
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 1.2631983757019043,
      "learning_rate": 7.80952380952381e-05,
      "loss": 2.7023,
      "step": 12800
    },
    {
      "epoch": 1.83,
      "grad_norm": 1.215116024017334,
      "learning_rate": 7.800000000000001e-05,
      "loss": 2.861,
      "step": 12810
    },
    {
      "epoch": 1.8314285714285714,
      "grad_norm": 1.053702712059021,
      "learning_rate": 7.790476190476192e-05,
      "loss": 2.6811,
      "step": 12820
    },
    {
      "epoch": 1.8328571428571427,
      "grad_norm": 1.244530439376831,
      "learning_rate": 7.78095238095238e-05,
      "loss": 2.6223,
      "step": 12830
    },
    {
      "epoch": 1.8342857142857143,
      "grad_norm": 1.1858850717544556,
      "learning_rate": 7.771428571428572e-05,
      "loss": 2.6894,
      "step": 12840
    },
    {
      "epoch": 1.8357142857142859,
      "grad_norm": 1.3120344877243042,
      "learning_rate": 7.761904761904762e-05,
      "loss": 2.7659,
      "step": 12850
    },
    {
      "epoch": 1.8371428571428572,
      "grad_norm": 0.9778181910514832,
      "learning_rate": 7.752380952380953e-05,
      "loss": 2.4866,
      "step": 12860
    },
    {
      "epoch": 1.8385714285714285,
      "grad_norm": 1.6972863674163818,
      "learning_rate": 7.742857142857143e-05,
      "loss": 2.7808,
      "step": 12870
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 1.18009352684021,
      "learning_rate": 7.733333333333333e-05,
      "loss": 2.6418,
      "step": 12880
    },
    {
      "epoch": 1.8414285714285714,
      "grad_norm": 1.0534172058105469,
      "learning_rate": 7.723809523809524e-05,
      "loss": 2.6265,
      "step": 12890
    },
    {
      "epoch": 1.842857142857143,
      "grad_norm": 1.458742618560791,
      "learning_rate": 7.714285714285715e-05,
      "loss": 2.5818,
      "step": 12900
    },
    {
      "epoch": 1.8442857142857143,
      "grad_norm": 1.3249526023864746,
      "learning_rate": 7.704761904761904e-05,
      "loss": 2.6176,
      "step": 12910
    },
    {
      "epoch": 1.8457142857142856,
      "grad_norm": 1.2894383668899536,
      "learning_rate": 7.695238095238096e-05,
      "loss": 2.7134,
      "step": 12920
    },
    {
      "epoch": 1.8471428571428572,
      "grad_norm": 1.020628571510315,
      "learning_rate": 7.685714285714286e-05,
      "loss": 2.7829,
      "step": 12930
    },
    {
      "epoch": 1.8485714285714285,
      "grad_norm": 1.2486006021499634,
      "learning_rate": 7.676190476190477e-05,
      "loss": 2.6757,
      "step": 12940
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.9405432939529419,
      "learning_rate": 7.666666666666667e-05,
      "loss": 2.6633,
      "step": 12950
    },
    {
      "epoch": 1.8514285714285714,
      "grad_norm": 1.2653344869613647,
      "learning_rate": 7.657142857142857e-05,
      "loss": 2.561,
      "step": 12960
    },
    {
      "epoch": 1.8528571428571428,
      "grad_norm": 1.2751704454421997,
      "learning_rate": 7.647619047619048e-05,
      "loss": 2.6251,
      "step": 12970
    },
    {
      "epoch": 1.8542857142857143,
      "grad_norm": 1.194533348083496,
      "learning_rate": 7.638095238095239e-05,
      "loss": 2.7546,
      "step": 12980
    },
    {
      "epoch": 1.8557142857142859,
      "grad_norm": 1.3165431022644043,
      "learning_rate": 7.62857142857143e-05,
      "loss": 2.7064,
      "step": 12990
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.2569525241851807,
      "learning_rate": 7.619047619047618e-05,
      "loss": 2.6824,
      "step": 13000
    },
    {
      "epoch": 1.8585714285714285,
      "grad_norm": 1.1531767845153809,
      "learning_rate": 7.60952380952381e-05,
      "loss": 2.6379,
      "step": 13010
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 1.0666911602020264,
      "learning_rate": 7.6e-05,
      "loss": 2.7969,
      "step": 13020
    },
    {
      "epoch": 1.8614285714285714,
      "grad_norm": 1.3512853384017944,
      "learning_rate": 7.590476190476191e-05,
      "loss": 2.7521,
      "step": 13030
    },
    {
      "epoch": 1.862857142857143,
      "grad_norm": 1.241346836090088,
      "learning_rate": 7.580952380952381e-05,
      "loss": 2.6359,
      "step": 13040
    },
    {
      "epoch": 1.8642857142857143,
      "grad_norm": 1.4203968048095703,
      "learning_rate": 7.571428571428571e-05,
      "loss": 2.6922,
      "step": 13050
    },
    {
      "epoch": 1.8657142857142857,
      "grad_norm": 1.0986623764038086,
      "learning_rate": 7.561904761904762e-05,
      "loss": 2.6052,
      "step": 13060
    },
    {
      "epoch": 1.867142857142857,
      "grad_norm": 1.5528768301010132,
      "learning_rate": 7.552380952380953e-05,
      "loss": 2.6922,
      "step": 13070
    },
    {
      "epoch": 1.8685714285714285,
      "grad_norm": 1.0851287841796875,
      "learning_rate": 7.542857142857144e-05,
      "loss": 2.8386,
      "step": 13080
    },
    {
      "epoch": 1.87,
      "grad_norm": 1.0461714267730713,
      "learning_rate": 7.533333333333334e-05,
      "loss": 2.6833,
      "step": 13090
    },
    {
      "epoch": 1.8714285714285714,
      "grad_norm": 1.3125081062316895,
      "learning_rate": 7.523809523809524e-05,
      "loss": 2.6356,
      "step": 13100
    },
    {
      "epoch": 1.8728571428571428,
      "grad_norm": 1.3048149347305298,
      "learning_rate": 7.514285714285715e-05,
      "loss": 2.6211,
      "step": 13110
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 1.2673026323318481,
      "learning_rate": 7.504761904761906e-05,
      "loss": 2.7096,
      "step": 13120
    },
    {
      "epoch": 1.8757142857142857,
      "grad_norm": 1.0898137092590332,
      "learning_rate": 7.495238095238095e-05,
      "loss": 2.5965,
      "step": 13130
    },
    {
      "epoch": 1.8771428571428572,
      "grad_norm": 0.9801717400550842,
      "learning_rate": 7.485714285714285e-05,
      "loss": 2.5296,
      "step": 13140
    },
    {
      "epoch": 1.8785714285714286,
      "grad_norm": 1.2939672470092773,
      "learning_rate": 7.476190476190477e-05,
      "loss": 2.731,
      "step": 13150
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.1026018857955933,
      "learning_rate": 7.466666666666667e-05,
      "loss": 2.5841,
      "step": 13160
    },
    {
      "epoch": 1.8814285714285715,
      "grad_norm": 1.036346673965454,
      "learning_rate": 7.457142857142856e-05,
      "loss": 2.5796,
      "step": 13170
    },
    {
      "epoch": 1.882857142857143,
      "grad_norm": 1.2791534662246704,
      "learning_rate": 7.447619047619048e-05,
      "loss": 2.676,
      "step": 13180
    },
    {
      "epoch": 1.8842857142857143,
      "grad_norm": 1.0133366584777832,
      "learning_rate": 7.438095238095238e-05,
      "loss": 2.446,
      "step": 13190
    },
    {
      "epoch": 1.8857142857142857,
      "grad_norm": 1.486161470413208,
      "learning_rate": 7.428571428571429e-05,
      "loss": 2.7442,
      "step": 13200
    },
    {
      "epoch": 1.887142857142857,
      "grad_norm": 1.2084951400756836,
      "learning_rate": 7.41904761904762e-05,
      "loss": 2.6707,
      "step": 13210
    },
    {
      "epoch": 1.8885714285714286,
      "grad_norm": 1.0233666896820068,
      "learning_rate": 7.409523809523809e-05,
      "loss": 2.6566,
      "step": 13220
    },
    {
      "epoch": 1.8900000000000001,
      "grad_norm": 1.5392793416976929,
      "learning_rate": 7.4e-05,
      "loss": 2.7599,
      "step": 13230
    },
    {
      "epoch": 1.8914285714285715,
      "grad_norm": 1.2792836427688599,
      "learning_rate": 7.390476190476191e-05,
      "loss": 2.7119,
      "step": 13240
    },
    {
      "epoch": 1.8928571428571428,
      "grad_norm": 1.262078046798706,
      "learning_rate": 7.380952380952382e-05,
      "loss": 2.7417,
      "step": 13250
    },
    {
      "epoch": 1.8942857142857141,
      "grad_norm": 1.0482429265975952,
      "learning_rate": 7.371428571428572e-05,
      "loss": 2.8479,
      "step": 13260
    },
    {
      "epoch": 1.8957142857142857,
      "grad_norm": 1.3156367540359497,
      "learning_rate": 7.361904761904762e-05,
      "loss": 2.635,
      "step": 13270
    },
    {
      "epoch": 1.8971428571428572,
      "grad_norm": 0.9167172312736511,
      "learning_rate": 7.352380952380953e-05,
      "loss": 2.6269,
      "step": 13280
    },
    {
      "epoch": 1.8985714285714286,
      "grad_norm": 1.4037878513336182,
      "learning_rate": 7.342857142857144e-05,
      "loss": 2.5664,
      "step": 13290
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.326197862625122,
      "learning_rate": 7.333333333333333e-05,
      "loss": 2.7675,
      "step": 13300
    },
    {
      "epoch": 1.9014285714285715,
      "grad_norm": 1.3322272300720215,
      "learning_rate": 7.323809523809523e-05,
      "loss": 2.6016,
      "step": 13310
    },
    {
      "epoch": 1.9028571428571428,
      "grad_norm": 1.355507493019104,
      "learning_rate": 7.314285714285715e-05,
      "loss": 2.712,
      "step": 13320
    },
    {
      "epoch": 1.9042857142857144,
      "grad_norm": 0.8816518187522888,
      "learning_rate": 7.304761904761905e-05,
      "loss": 2.6334,
      "step": 13330
    },
    {
      "epoch": 1.9057142857142857,
      "grad_norm": 0.9832010865211487,
      "learning_rate": 7.295238095238096e-05,
      "loss": 2.553,
      "step": 13340
    },
    {
      "epoch": 1.907142857142857,
      "grad_norm": 1.1188738346099854,
      "learning_rate": 7.285714285714286e-05,
      "loss": 2.7661,
      "step": 13350
    },
    {
      "epoch": 1.9085714285714286,
      "grad_norm": 1.0581331253051758,
      "learning_rate": 7.276190476190476e-05,
      "loss": 2.6688,
      "step": 13360
    },
    {
      "epoch": 1.9100000000000001,
      "grad_norm": 1.2898569107055664,
      "learning_rate": 7.266666666666667e-05,
      "loss": 2.6452,
      "step": 13370
    },
    {
      "epoch": 1.9114285714285715,
      "grad_norm": 1.1109896898269653,
      "learning_rate": 7.257142857142858e-05,
      "loss": 2.6678,
      "step": 13380
    },
    {
      "epoch": 1.9128571428571428,
      "grad_norm": 1.1331475973129272,
      "learning_rate": 7.247619047619047e-05,
      "loss": 2.663,
      "step": 13390
    },
    {
      "epoch": 1.9142857142857141,
      "grad_norm": 1.1140267848968506,
      "learning_rate": 7.238095238095238e-05,
      "loss": 2.781,
      "step": 13400
    },
    {
      "epoch": 1.9157142857142857,
      "grad_norm": 1.3545345067977905,
      "learning_rate": 7.228571428571429e-05,
      "loss": 2.6905,
      "step": 13410
    },
    {
      "epoch": 1.9171428571428573,
      "grad_norm": 1.0103861093521118,
      "learning_rate": 7.21904761904762e-05,
      "loss": 2.6577,
      "step": 13420
    },
    {
      "epoch": 1.9185714285714286,
      "grad_norm": 1.5785270929336548,
      "learning_rate": 7.20952380952381e-05,
      "loss": 2.7518,
      "step": 13430
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.0501781702041626,
      "learning_rate": 7.2e-05,
      "loss": 2.5741,
      "step": 13440
    },
    {
      "epoch": 1.9214285714285713,
      "grad_norm": 1.1166914701461792,
      "learning_rate": 7.19047619047619e-05,
      "loss": 2.6522,
      "step": 13450
    },
    {
      "epoch": 1.9228571428571428,
      "grad_norm": 1.0456409454345703,
      "learning_rate": 7.180952380952382e-05,
      "loss": 2.7854,
      "step": 13460
    },
    {
      "epoch": 1.9242857142857144,
      "grad_norm": 0.9921072125434875,
      "learning_rate": 7.171428571428572e-05,
      "loss": 2.5877,
      "step": 13470
    },
    {
      "epoch": 1.9257142857142857,
      "grad_norm": 1.4050968885421753,
      "learning_rate": 7.161904761904761e-05,
      "loss": 2.5942,
      "step": 13480
    },
    {
      "epoch": 1.927142857142857,
      "grad_norm": 1.1455518007278442,
      "learning_rate": 7.152380952380953e-05,
      "loss": 2.5118,
      "step": 13490
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 0.8891262412071228,
      "learning_rate": 7.142857142857143e-05,
      "loss": 2.5757,
      "step": 13500
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 1.6965513229370117,
      "learning_rate": 7.133333333333334e-05,
      "loss": 2.52,
      "step": 13510
    },
    {
      "epoch": 1.9314285714285715,
      "grad_norm": 1.4555314779281616,
      "learning_rate": 7.123809523809524e-05,
      "loss": 2.724,
      "step": 13520
    },
    {
      "epoch": 1.9328571428571428,
      "grad_norm": 1.1275187730789185,
      "learning_rate": 7.114285714285714e-05,
      "loss": 2.6147,
      "step": 13530
    },
    {
      "epoch": 1.9342857142857142,
      "grad_norm": 1.5042177438735962,
      "learning_rate": 7.104761904761905e-05,
      "loss": 2.6911,
      "step": 13540
    },
    {
      "epoch": 1.9357142857142857,
      "grad_norm": 1.1746127605438232,
      "learning_rate": 7.095238095238096e-05,
      "loss": 2.6659,
      "step": 13550
    },
    {
      "epoch": 1.9371428571428573,
      "grad_norm": 1.575198769569397,
      "learning_rate": 7.085714285714285e-05,
      "loss": 2.6954,
      "step": 13560
    },
    {
      "epoch": 1.9385714285714286,
      "grad_norm": 1.4683490991592407,
      "learning_rate": 7.076190476190477e-05,
      "loss": 2.7452,
      "step": 13570
    },
    {
      "epoch": 1.94,
      "grad_norm": 1.2936475276947021,
      "learning_rate": 7.066666666666667e-05,
      "loss": 2.7272,
      "step": 13580
    },
    {
      "epoch": 1.9414285714285713,
      "grad_norm": 1.2727490663528442,
      "learning_rate": 7.057142857142858e-05,
      "loss": 2.5025,
      "step": 13590
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 1.397791862487793,
      "learning_rate": 7.047619047619048e-05,
      "loss": 2.6721,
      "step": 13600
    },
    {
      "epoch": 1.9442857142857144,
      "grad_norm": 1.4986786842346191,
      "learning_rate": 7.038095238095238e-05,
      "loss": 2.8829,
      "step": 13610
    },
    {
      "epoch": 1.9457142857142857,
      "grad_norm": 1.2478536367416382,
      "learning_rate": 7.028571428571428e-05,
      "loss": 2.5775,
      "step": 13620
    },
    {
      "epoch": 1.947142857142857,
      "grad_norm": 0.9995430111885071,
      "learning_rate": 7.01904761904762e-05,
      "loss": 2.7208,
      "step": 13630
    },
    {
      "epoch": 1.9485714285714286,
      "grad_norm": 1.1685686111450195,
      "learning_rate": 7.00952380952381e-05,
      "loss": 2.5711,
      "step": 13640
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.2772200107574463,
      "learning_rate": 7e-05,
      "loss": 2.6983,
      "step": 13650
    },
    {
      "epoch": 1.9514285714285715,
      "grad_norm": 1.1760330200195312,
      "learning_rate": 6.990476190476191e-05,
      "loss": 2.6229,
      "step": 13660
    },
    {
      "epoch": 1.9528571428571428,
      "grad_norm": 1.287125587463379,
      "learning_rate": 6.980952380952381e-05,
      "loss": 2.656,
      "step": 13670
    },
    {
      "epoch": 1.9542857142857142,
      "grad_norm": 0.9123584628105164,
      "learning_rate": 6.971428571428572e-05,
      "loss": 2.6538,
      "step": 13680
    },
    {
      "epoch": 1.9557142857142857,
      "grad_norm": 1.2710964679718018,
      "learning_rate": 6.961904761904762e-05,
      "loss": 2.5694,
      "step": 13690
    },
    {
      "epoch": 1.9571428571428573,
      "grad_norm": 0.8299453854560852,
      "learning_rate": 6.952380952380952e-05,
      "loss": 2.5483,
      "step": 13700
    },
    {
      "epoch": 1.9585714285714286,
      "grad_norm": 1.158004641532898,
      "learning_rate": 6.942857142857143e-05,
      "loss": 2.6424,
      "step": 13710
    },
    {
      "epoch": 1.96,
      "grad_norm": 1.1112654209136963,
      "learning_rate": 6.933333333333334e-05,
      "loss": 2.631,
      "step": 13720
    },
    {
      "epoch": 1.9614285714285713,
      "grad_norm": 1.1263855695724487,
      "learning_rate": 6.923809523809525e-05,
      "loss": 2.8228,
      "step": 13730
    },
    {
      "epoch": 1.9628571428571429,
      "grad_norm": 1.2070850133895874,
      "learning_rate": 6.914285714285715e-05,
      "loss": 2.5392,
      "step": 13740
    },
    {
      "epoch": 1.9642857142857144,
      "grad_norm": 1.0734046697616577,
      "learning_rate": 6.904761904761905e-05,
      "loss": 2.5747,
      "step": 13750
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 0.9848644733428955,
      "learning_rate": 6.895238095238095e-05,
      "loss": 2.9328,
      "step": 13760
    },
    {
      "epoch": 1.967142857142857,
      "grad_norm": 1.1071374416351318,
      "learning_rate": 6.885714285714286e-05,
      "loss": 2.7104,
      "step": 13770
    },
    {
      "epoch": 1.9685714285714284,
      "grad_norm": 1.227157473564148,
      "learning_rate": 6.876190476190476e-05,
      "loss": 2.7524,
      "step": 13780
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.2289855480194092,
      "learning_rate": 6.866666666666666e-05,
      "loss": 2.7094,
      "step": 13790
    },
    {
      "epoch": 1.9714285714285715,
      "grad_norm": 0.9932730793952942,
      "learning_rate": 6.857142857142858e-05,
      "loss": 2.7016,
      "step": 13800
    },
    {
      "epoch": 1.9728571428571429,
      "grad_norm": 1.215874433517456,
      "learning_rate": 6.847619047619048e-05,
      "loss": 2.6511,
      "step": 13810
    },
    {
      "epoch": 1.9742857142857142,
      "grad_norm": 0.9864531755447388,
      "learning_rate": 6.838095238095237e-05,
      "loss": 2.6804,
      "step": 13820
    },
    {
      "epoch": 1.9757142857142858,
      "grad_norm": 1.5378053188323975,
      "learning_rate": 6.828571428571429e-05,
      "loss": 2.7629,
      "step": 13830
    },
    {
      "epoch": 1.977142857142857,
      "grad_norm": 0.8209168314933777,
      "learning_rate": 6.81904761904762e-05,
      "loss": 2.7723,
      "step": 13840
    },
    {
      "epoch": 1.9785714285714286,
      "grad_norm": 1.1491317749023438,
      "learning_rate": 6.80952380952381e-05,
      "loss": 2.6699,
      "step": 13850
    },
    {
      "epoch": 1.98,
      "grad_norm": 1.2880247831344604,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.7229,
      "step": 13860
    },
    {
      "epoch": 1.9814285714285713,
      "grad_norm": 1.4689531326293945,
      "learning_rate": 6.79047619047619e-05,
      "loss": 2.7508,
      "step": 13870
    },
    {
      "epoch": 1.9828571428571429,
      "grad_norm": 1.1119682788848877,
      "learning_rate": 6.78095238095238e-05,
      "loss": 2.6278,
      "step": 13880
    },
    {
      "epoch": 1.9842857142857144,
      "grad_norm": 1.259034276008606,
      "learning_rate": 6.771428571428572e-05,
      "loss": 2.7339,
      "step": 13890
    },
    {
      "epoch": 1.9857142857142858,
      "grad_norm": 0.9642290472984314,
      "learning_rate": 6.761904761904763e-05,
      "loss": 2.5789,
      "step": 13900
    },
    {
      "epoch": 1.987142857142857,
      "grad_norm": 1.2183537483215332,
      "learning_rate": 6.752380952380953e-05,
      "loss": 2.5429,
      "step": 13910
    },
    {
      "epoch": 1.9885714285714284,
      "grad_norm": 1.0317896604537964,
      "learning_rate": 6.742857142857143e-05,
      "loss": 2.6203,
      "step": 13920
    },
    {
      "epoch": 1.99,
      "grad_norm": 0.9893324971199036,
      "learning_rate": 6.733333333333333e-05,
      "loss": 2.6247,
      "step": 13930
    },
    {
      "epoch": 1.9914285714285715,
      "grad_norm": 1.569694995880127,
      "learning_rate": 6.723809523809524e-05,
      "loss": 2.4881,
      "step": 13940
    },
    {
      "epoch": 1.9928571428571429,
      "grad_norm": 1.3834800720214844,
      "learning_rate": 6.714285714285714e-05,
      "loss": 2.8144,
      "step": 13950
    },
    {
      "epoch": 1.9942857142857142,
      "grad_norm": 1.303546667098999,
      "learning_rate": 6.704761904761904e-05,
      "loss": 2.7741,
      "step": 13960
    },
    {
      "epoch": 1.9957142857142856,
      "grad_norm": 1.3061810731887817,
      "learning_rate": 6.695238095238096e-05,
      "loss": 2.5631,
      "step": 13970
    },
    {
      "epoch": 1.997142857142857,
      "grad_norm": 1.2784793376922607,
      "learning_rate": 6.685714285714286e-05,
      "loss": 2.6918,
      "step": 13980
    },
    {
      "epoch": 1.9985714285714287,
      "grad_norm": 0.9917024970054626,
      "learning_rate": 6.676190476190477e-05,
      "loss": 2.7305,
      "step": 13990
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.0483075380325317,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.7203,
      "step": 14000
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.5016205310821533,
      "eval_rouge1": 0.13429322866685106,
      "eval_rouge2": 0.034452119559054493,
      "eval_rougeL": 0.10423069612065314,
      "eval_rougeLsum": 0.12789145807367502,
      "eval_runtime": 1727.869,
      "eval_samples_per_second": 13.89,
      "eval_steps_per_second": 3.472,
      "step": 14000
    },
    {
      "epoch": 2.0014285714285713,
      "grad_norm": 1.029320240020752,
      "learning_rate": 6.657142857142857e-05,
      "loss": 2.567,
      "step": 14010
    },
    {
      "epoch": 2.0028571428571427,
      "grad_norm": 1.2116987705230713,
      "learning_rate": 6.647619047619048e-05,
      "loss": 2.7126,
      "step": 14020
    },
    {
      "epoch": 2.0042857142857144,
      "grad_norm": 1.105697512626648,
      "learning_rate": 6.638095238095239e-05,
      "loss": 2.494,
      "step": 14030
    },
    {
      "epoch": 2.005714285714286,
      "grad_norm": 1.093241572380066,
      "learning_rate": 6.628571428571428e-05,
      "loss": 2.7105,
      "step": 14040
    },
    {
      "epoch": 2.007142857142857,
      "grad_norm": 0.9895708560943604,
      "learning_rate": 6.619047619047619e-05,
      "loss": 2.5502,
      "step": 14050
    },
    {
      "epoch": 2.0085714285714285,
      "grad_norm": 1.3950138092041016,
      "learning_rate": 6.60952380952381e-05,
      "loss": 2.5327,
      "step": 14060
    },
    {
      "epoch": 2.01,
      "grad_norm": 1.2173103094100952,
      "learning_rate": 6.6e-05,
      "loss": 2.3571,
      "step": 14070
    },
    {
      "epoch": 2.0114285714285716,
      "grad_norm": 1.1644476652145386,
      "learning_rate": 6.590476190476191e-05,
      "loss": 2.5315,
      "step": 14080
    },
    {
      "epoch": 2.012857142857143,
      "grad_norm": 1.2344902753829956,
      "learning_rate": 6.580952380952381e-05,
      "loss": 2.7126,
      "step": 14090
    },
    {
      "epoch": 2.0142857142857142,
      "grad_norm": 1.3979315757751465,
      "learning_rate": 6.571428571428571e-05,
      "loss": 2.6334,
      "step": 14100
    },
    {
      "epoch": 2.0157142857142856,
      "grad_norm": 1.2696093320846558,
      "learning_rate": 6.561904761904763e-05,
      "loss": 2.6868,
      "step": 14110
    },
    {
      "epoch": 2.0171428571428573,
      "grad_norm": 0.9018954038619995,
      "learning_rate": 6.552380952380953e-05,
      "loss": 2.6393,
      "step": 14120
    },
    {
      "epoch": 2.0185714285714287,
      "grad_norm": 1.1641613245010376,
      "learning_rate": 6.542857142857142e-05,
      "loss": 2.6044,
      "step": 14130
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.184781551361084,
      "learning_rate": 6.533333333333334e-05,
      "loss": 2.6067,
      "step": 14140
    },
    {
      "epoch": 2.0214285714285714,
      "grad_norm": 1.2606335878372192,
      "learning_rate": 6.523809523809524e-05,
      "loss": 2.4434,
      "step": 14150
    },
    {
      "epoch": 2.0228571428571427,
      "grad_norm": 1.0429444313049316,
      "learning_rate": 6.514285714285715e-05,
      "loss": 2.6307,
      "step": 14160
    },
    {
      "epoch": 2.0242857142857145,
      "grad_norm": 1.1510920524597168,
      "learning_rate": 6.504761904761905e-05,
      "loss": 2.5881,
      "step": 14170
    },
    {
      "epoch": 2.025714285714286,
      "grad_norm": 1.512468934059143,
      "learning_rate": 6.495238095238095e-05,
      "loss": 2.7504,
      "step": 14180
    },
    {
      "epoch": 2.027142857142857,
      "grad_norm": 2.555879831314087,
      "learning_rate": 6.485714285714286e-05,
      "loss": 2.7037,
      "step": 14190
    },
    {
      "epoch": 2.0285714285714285,
      "grad_norm": 1.0661044120788574,
      "learning_rate": 6.476190476190477e-05,
      "loss": 2.7057,
      "step": 14200
    },
    {
      "epoch": 2.03,
      "grad_norm": 1.2191764116287231,
      "learning_rate": 6.466666666666666e-05,
      "loss": 2.8216,
      "step": 14210
    },
    {
      "epoch": 2.0314285714285716,
      "grad_norm": 1.2369695901870728,
      "learning_rate": 6.457142857142856e-05,
      "loss": 2.6292,
      "step": 14220
    },
    {
      "epoch": 2.032857142857143,
      "grad_norm": 1.381019949913025,
      "learning_rate": 6.447619047619048e-05,
      "loss": 2.6892,
      "step": 14230
    },
    {
      "epoch": 2.0342857142857143,
      "grad_norm": 1.0029715299606323,
      "learning_rate": 6.438095238095238e-05,
      "loss": 2.7663,
      "step": 14240
    },
    {
      "epoch": 2.0357142857142856,
      "grad_norm": 1.302841305732727,
      "learning_rate": 6.428571428571429e-05,
      "loss": 2.7244,
      "step": 14250
    },
    {
      "epoch": 2.0371428571428574,
      "grad_norm": 1.1768662929534912,
      "learning_rate": 6.419047619047619e-05,
      "loss": 2.5891,
      "step": 14260
    },
    {
      "epoch": 2.0385714285714287,
      "grad_norm": 1.0799458026885986,
      "learning_rate": 6.40952380952381e-05,
      "loss": 2.6977,
      "step": 14270
    },
    {
      "epoch": 2.04,
      "grad_norm": 1.3583873510360718,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.6511,
      "step": 14280
    },
    {
      "epoch": 2.0414285714285714,
      "grad_norm": 1.5525002479553223,
      "learning_rate": 6.390476190476191e-05,
      "loss": 2.7399,
      "step": 14290
    },
    {
      "epoch": 2.0428571428571427,
      "grad_norm": 0.9612851142883301,
      "learning_rate": 6.38095238095238e-05,
      "loss": 2.6027,
      "step": 14300
    },
    {
      "epoch": 2.0442857142857145,
      "grad_norm": 1.2753044366836548,
      "learning_rate": 6.371428571428572e-05,
      "loss": 2.6626,
      "step": 14310
    },
    {
      "epoch": 2.045714285714286,
      "grad_norm": 1.2012336254119873,
      "learning_rate": 6.361904761904762e-05,
      "loss": 2.6388,
      "step": 14320
    },
    {
      "epoch": 2.047142857142857,
      "grad_norm": 1.0565904378890991,
      "learning_rate": 6.352380952380953e-05,
      "loss": 2.5406,
      "step": 14330
    },
    {
      "epoch": 2.0485714285714285,
      "grad_norm": 0.9733519554138184,
      "learning_rate": 6.342857142857143e-05,
      "loss": 2.7115,
      "step": 14340
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.136066198348999,
      "learning_rate": 6.333333333333333e-05,
      "loss": 2.6673,
      "step": 14350
    },
    {
      "epoch": 2.0514285714285716,
      "grad_norm": 0.8306003212928772,
      "learning_rate": 6.323809523809524e-05,
      "loss": 2.7687,
      "step": 14360
    },
    {
      "epoch": 2.052857142857143,
      "grad_norm": 1.059922218322754,
      "learning_rate": 6.314285714285715e-05,
      "loss": 2.7053,
      "step": 14370
    },
    {
      "epoch": 2.0542857142857143,
      "grad_norm": 1.0178589820861816,
      "learning_rate": 6.304761904761906e-05,
      "loss": 2.7077,
      "step": 14380
    },
    {
      "epoch": 2.0557142857142856,
      "grad_norm": 1.0422234535217285,
      "learning_rate": 6.295238095238096e-05,
      "loss": 2.6286,
      "step": 14390
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 1.5194450616836548,
      "learning_rate": 6.285714285714286e-05,
      "loss": 2.372,
      "step": 14400
    },
    {
      "epoch": 2.0585714285714287,
      "grad_norm": 1.2000658512115479,
      "learning_rate": 6.276190476190476e-05,
      "loss": 2.6051,
      "step": 14410
    },
    {
      "epoch": 2.06,
      "grad_norm": 1.5900079011917114,
      "learning_rate": 6.266666666666667e-05,
      "loss": 2.6747,
      "step": 14420
    },
    {
      "epoch": 2.0614285714285714,
      "grad_norm": 1.0100231170654297,
      "learning_rate": 6.257142857142857e-05,
      "loss": 2.542,
      "step": 14430
    },
    {
      "epoch": 2.0628571428571427,
      "grad_norm": 0.9842889308929443,
      "learning_rate": 6.247619047619047e-05,
      "loss": 2.6271,
      "step": 14440
    },
    {
      "epoch": 2.064285714285714,
      "grad_norm": 1.5864378213882446,
      "learning_rate": 6.238095238095239e-05,
      "loss": 2.6491,
      "step": 14450
    },
    {
      "epoch": 2.065714285714286,
      "grad_norm": 1.3046616315841675,
      "learning_rate": 6.22857142857143e-05,
      "loss": 2.5838,
      "step": 14460
    },
    {
      "epoch": 2.067142857142857,
      "grad_norm": 1.0514159202575684,
      "learning_rate": 6.21904761904762e-05,
      "loss": 2.4285,
      "step": 14470
    },
    {
      "epoch": 2.0685714285714285,
      "grad_norm": 1.1692715883255005,
      "learning_rate": 6.20952380952381e-05,
      "loss": 2.7426,
      "step": 14480
    },
    {
      "epoch": 2.07,
      "grad_norm": 0.8865987062454224,
      "learning_rate": 6.2e-05,
      "loss": 2.5504,
      "step": 14490
    },
    {
      "epoch": 2.0714285714285716,
      "grad_norm": 1.1415692567825317,
      "learning_rate": 6.19047619047619e-05,
      "loss": 2.4785,
      "step": 14500
    },
    {
      "epoch": 2.072857142857143,
      "grad_norm": 1.1363286972045898,
      "learning_rate": 6.180952380952382e-05,
      "loss": 2.6772,
      "step": 14510
    },
    {
      "epoch": 2.0742857142857143,
      "grad_norm": 1.0861274003982544,
      "learning_rate": 6.171428571428571e-05,
      "loss": 2.5247,
      "step": 14520
    },
    {
      "epoch": 2.0757142857142856,
      "grad_norm": 1.0012259483337402,
      "learning_rate": 6.161904761904762e-05,
      "loss": 2.5897,
      "step": 14530
    },
    {
      "epoch": 2.077142857142857,
      "grad_norm": 0.9701220393180847,
      "learning_rate": 6.152380952380953e-05,
      "loss": 2.3693,
      "step": 14540
    },
    {
      "epoch": 2.0785714285714287,
      "grad_norm": 1.4101053476333618,
      "learning_rate": 6.142857142857143e-05,
      "loss": 2.6707,
      "step": 14550
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.2540249824523926,
      "learning_rate": 6.133333333333334e-05,
      "loss": 2.5778,
      "step": 14560
    },
    {
      "epoch": 2.0814285714285714,
      "grad_norm": 1.1060341596603394,
      "learning_rate": 6.123809523809524e-05,
      "loss": 2.6976,
      "step": 14570
    },
    {
      "epoch": 2.0828571428571427,
      "grad_norm": 1.1860984563827515,
      "learning_rate": 6.114285714285714e-05,
      "loss": 2.5729,
      "step": 14580
    },
    {
      "epoch": 2.084285714285714,
      "grad_norm": 2.0019240379333496,
      "learning_rate": 6.104761904761905e-05,
      "loss": 2.7266,
      "step": 14590
    },
    {
      "epoch": 2.085714285714286,
      "grad_norm": 0.9765678644180298,
      "learning_rate": 6.0952380952380964e-05,
      "loss": 2.4761,
      "step": 14600
    },
    {
      "epoch": 2.087142857142857,
      "grad_norm": 1.4427162408828735,
      "learning_rate": 6.085714285714286e-05,
      "loss": 2.6445,
      "step": 14610
    },
    {
      "epoch": 2.0885714285714285,
      "grad_norm": 1.4841700792312622,
      "learning_rate": 6.076190476190476e-05,
      "loss": 2.5217,
      "step": 14620
    },
    {
      "epoch": 2.09,
      "grad_norm": 1.1971042156219482,
      "learning_rate": 6.066666666666667e-05,
      "loss": 2.4728,
      "step": 14630
    },
    {
      "epoch": 2.0914285714285716,
      "grad_norm": 1.1651314496994019,
      "learning_rate": 6.0571428571428576e-05,
      "loss": 2.8076,
      "step": 14640
    },
    {
      "epoch": 2.092857142857143,
      "grad_norm": 1.23646879196167,
      "learning_rate": 6.047619047619047e-05,
      "loss": 2.6826,
      "step": 14650
    },
    {
      "epoch": 2.0942857142857143,
      "grad_norm": 1.5628981590270996,
      "learning_rate": 6.038095238095238e-05,
      "loss": 2.597,
      "step": 14660
    },
    {
      "epoch": 2.0957142857142856,
      "grad_norm": 1.7936034202575684,
      "learning_rate": 6.028571428571429e-05,
      "loss": 2.6734,
      "step": 14670
    },
    {
      "epoch": 2.097142857142857,
      "grad_norm": 1.0006009340286255,
      "learning_rate": 6.0190476190476195e-05,
      "loss": 2.7002,
      "step": 14680
    },
    {
      "epoch": 2.0985714285714288,
      "grad_norm": 1.2682082653045654,
      "learning_rate": 6.009523809523809e-05,
      "loss": 2.6673,
      "step": 14690
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.029135823249817,
      "learning_rate": 6e-05,
      "loss": 2.6583,
      "step": 14700
    },
    {
      "epoch": 2.1014285714285714,
      "grad_norm": 1.3145748376846313,
      "learning_rate": 5.9904761904761905e-05,
      "loss": 2.6313,
      "step": 14710
    },
    {
      "epoch": 2.1028571428571428,
      "grad_norm": 1.369565486907959,
      "learning_rate": 5.9809523809523814e-05,
      "loss": 2.6474,
      "step": 14720
    },
    {
      "epoch": 2.104285714285714,
      "grad_norm": 1.2199561595916748,
      "learning_rate": 5.9714285714285724e-05,
      "loss": 2.7075,
      "step": 14730
    },
    {
      "epoch": 2.105714285714286,
      "grad_norm": 1.1092360019683838,
      "learning_rate": 5.961904761904762e-05,
      "loss": 2.7875,
      "step": 14740
    },
    {
      "epoch": 2.107142857142857,
      "grad_norm": 1.3008086681365967,
      "learning_rate": 5.9523809523809524e-05,
      "loss": 2.5979,
      "step": 14750
    },
    {
      "epoch": 2.1085714285714285,
      "grad_norm": 1.1127533912658691,
      "learning_rate": 5.9428571428571434e-05,
      "loss": 2.6651,
      "step": 14760
    },
    {
      "epoch": 2.11,
      "grad_norm": 0.9820801615715027,
      "learning_rate": 5.9333333333333343e-05,
      "loss": 2.552,
      "step": 14770
    },
    {
      "epoch": 2.111428571428571,
      "grad_norm": 1.240993618965149,
      "learning_rate": 5.923809523809524e-05,
      "loss": 2.7709,
      "step": 14780
    },
    {
      "epoch": 2.112857142857143,
      "grad_norm": 1.5217103958129883,
      "learning_rate": 5.914285714285714e-05,
      "loss": 2.6613,
      "step": 14790
    },
    {
      "epoch": 2.1142857142857143,
      "grad_norm": 1.058443546295166,
      "learning_rate": 5.904761904761905e-05,
      "loss": 2.8275,
      "step": 14800
    },
    {
      "epoch": 2.1157142857142857,
      "grad_norm": 1.2670493125915527,
      "learning_rate": 5.8952380952380956e-05,
      "loss": 2.7482,
      "step": 14810
    },
    {
      "epoch": 2.117142857142857,
      "grad_norm": 1.1918941736221313,
      "learning_rate": 5.885714285714285e-05,
      "loss": 2.6614,
      "step": 14820
    },
    {
      "epoch": 2.1185714285714288,
      "grad_norm": 1.1232484579086304,
      "learning_rate": 5.876190476190476e-05,
      "loss": 2.6254,
      "step": 14830
    },
    {
      "epoch": 2.12,
      "grad_norm": 1.328568935394287,
      "learning_rate": 5.866666666666667e-05,
      "loss": 2.6992,
      "step": 14840
    },
    {
      "epoch": 2.1214285714285714,
      "grad_norm": 1.2376312017440796,
      "learning_rate": 5.8571428571428575e-05,
      "loss": 2.6716,
      "step": 14850
    },
    {
      "epoch": 2.1228571428571428,
      "grad_norm": 1.2438424825668335,
      "learning_rate": 5.8476190476190485e-05,
      "loss": 2.5046,
      "step": 14860
    },
    {
      "epoch": 2.124285714285714,
      "grad_norm": 1.4979350566864014,
      "learning_rate": 5.838095238095238e-05,
      "loss": 2.619,
      "step": 14870
    },
    {
      "epoch": 2.125714285714286,
      "grad_norm": 1.0213146209716797,
      "learning_rate": 5.828571428571429e-05,
      "loss": 2.578,
      "step": 14880
    },
    {
      "epoch": 2.1271428571428572,
      "grad_norm": 1.6285489797592163,
      "learning_rate": 5.8190476190476194e-05,
      "loss": 2.5527,
      "step": 14890
    },
    {
      "epoch": 2.1285714285714286,
      "grad_norm": 1.0757535696029663,
      "learning_rate": 5.8095238095238104e-05,
      "loss": 2.6406,
      "step": 14900
    },
    {
      "epoch": 2.13,
      "grad_norm": 1.2681159973144531,
      "learning_rate": 5.8e-05,
      "loss": 2.6973,
      "step": 14910
    },
    {
      "epoch": 2.1314285714285712,
      "grad_norm": 1.0993927717208862,
      "learning_rate": 5.7904761904761903e-05,
      "loss": 2.7787,
      "step": 14920
    },
    {
      "epoch": 2.132857142857143,
      "grad_norm": 1.4702892303466797,
      "learning_rate": 5.780952380952381e-05,
      "loss": 2.6852,
      "step": 14930
    },
    {
      "epoch": 2.1342857142857143,
      "grad_norm": 1.051080584526062,
      "learning_rate": 5.771428571428572e-05,
      "loss": 2.588,
      "step": 14940
    },
    {
      "epoch": 2.1357142857142857,
      "grad_norm": 1.1701462268829346,
      "learning_rate": 5.761904761904762e-05,
      "loss": 2.5007,
      "step": 14950
    },
    {
      "epoch": 2.137142857142857,
      "grad_norm": 1.4882627725601196,
      "learning_rate": 5.752380952380952e-05,
      "loss": 2.7457,
      "step": 14960
    },
    {
      "epoch": 2.138571428571429,
      "grad_norm": 1.2477773427963257,
      "learning_rate": 5.742857142857143e-05,
      "loss": 2.5254,
      "step": 14970
    },
    {
      "epoch": 2.14,
      "grad_norm": 1.2791873216629028,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 2.5577,
      "step": 14980
    },
    {
      "epoch": 2.1414285714285715,
      "grad_norm": 1.0638558864593506,
      "learning_rate": 5.7238095238095245e-05,
      "loss": 2.6045,
      "step": 14990
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.4650574922561646,
      "learning_rate": 5.714285714285714e-05,
      "loss": 2.6385,
      "step": 15000
    },
    {
      "epoch": 2.144285714285714,
      "grad_norm": 0.9795349836349487,
      "learning_rate": 5.704761904761905e-05,
      "loss": 2.6242,
      "step": 15010
    },
    {
      "epoch": 2.145714285714286,
      "grad_norm": 1.1137970685958862,
      "learning_rate": 5.6952380952380955e-05,
      "loss": 2.584,
      "step": 15020
    },
    {
      "epoch": 2.1471428571428572,
      "grad_norm": 1.2051726579666138,
      "learning_rate": 5.6857142857142865e-05,
      "loss": 2.6643,
      "step": 15030
    },
    {
      "epoch": 2.1485714285714286,
      "grad_norm": 1.7778390645980835,
      "learning_rate": 5.676190476190476e-05,
      "loss": 2.8293,
      "step": 15040
    },
    {
      "epoch": 2.15,
      "grad_norm": 1.2278021574020386,
      "learning_rate": 5.666666666666667e-05,
      "loss": 2.672,
      "step": 15050
    },
    {
      "epoch": 2.1514285714285712,
      "grad_norm": 1.1025837659835815,
      "learning_rate": 5.6571428571428574e-05,
      "loss": 2.614,
      "step": 15060
    },
    {
      "epoch": 2.152857142857143,
      "grad_norm": 1.6029877662658691,
      "learning_rate": 5.6476190476190484e-05,
      "loss": 2.712,
      "step": 15070
    },
    {
      "epoch": 2.1542857142857144,
      "grad_norm": 1.007973313331604,
      "learning_rate": 5.638095238095238e-05,
      "loss": 2.7458,
      "step": 15080
    },
    {
      "epoch": 2.1557142857142857,
      "grad_norm": 1.1615623235702515,
      "learning_rate": 5.628571428571428e-05,
      "loss": 2.6115,
      "step": 15090
    },
    {
      "epoch": 2.157142857142857,
      "grad_norm": 0.9628655314445496,
      "learning_rate": 5.619047619047619e-05,
      "loss": 2.7434,
      "step": 15100
    },
    {
      "epoch": 2.1585714285714284,
      "grad_norm": 1.5405373573303223,
      "learning_rate": 5.60952380952381e-05,
      "loss": 2.6218,
      "step": 15110
    },
    {
      "epoch": 2.16,
      "grad_norm": 1.2147173881530762,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.5959,
      "step": 15120
    },
    {
      "epoch": 2.1614285714285715,
      "grad_norm": 1.2335686683654785,
      "learning_rate": 5.59047619047619e-05,
      "loss": 2.7407,
      "step": 15130
    },
    {
      "epoch": 2.162857142857143,
      "grad_norm": 1.1324539184570312,
      "learning_rate": 5.580952380952381e-05,
      "loss": 2.6351,
      "step": 15140
    },
    {
      "epoch": 2.164285714285714,
      "grad_norm": 1.5146844387054443,
      "learning_rate": 5.571428571428572e-05,
      "loss": 2.645,
      "step": 15150
    },
    {
      "epoch": 2.1657142857142855,
      "grad_norm": 1.390731930732727,
      "learning_rate": 5.5619047619047625e-05,
      "loss": 2.6423,
      "step": 15160
    },
    {
      "epoch": 2.1671428571428573,
      "grad_norm": 1.5262410640716553,
      "learning_rate": 5.552380952380952e-05,
      "loss": 2.7067,
      "step": 15170
    },
    {
      "epoch": 2.1685714285714286,
      "grad_norm": 1.679369330406189,
      "learning_rate": 5.542857142857143e-05,
      "loss": 2.6203,
      "step": 15180
    },
    {
      "epoch": 2.17,
      "grad_norm": 0.9691640734672546,
      "learning_rate": 5.5333333333333334e-05,
      "loss": 2.7973,
      "step": 15190
    },
    {
      "epoch": 2.1714285714285713,
      "grad_norm": 1.3251757621765137,
      "learning_rate": 5.5238095238095244e-05,
      "loss": 2.7202,
      "step": 15200
    },
    {
      "epoch": 2.172857142857143,
      "grad_norm": 1.3238719701766968,
      "learning_rate": 5.514285714285714e-05,
      "loss": 2.5779,
      "step": 15210
    },
    {
      "epoch": 2.1742857142857144,
      "grad_norm": 0.9062818288803101,
      "learning_rate": 5.504761904761905e-05,
      "loss": 2.5455,
      "step": 15220
    },
    {
      "epoch": 2.1757142857142857,
      "grad_norm": 1.419919729232788,
      "learning_rate": 5.4952380952380954e-05,
      "loss": 2.6343,
      "step": 15230
    },
    {
      "epoch": 2.177142857142857,
      "grad_norm": 1.1419281959533691,
      "learning_rate": 5.485714285714286e-05,
      "loss": 2.6567,
      "step": 15240
    },
    {
      "epoch": 2.1785714285714284,
      "grad_norm": 1.544174313545227,
      "learning_rate": 5.4761904761904766e-05,
      "loss": 2.6713,
      "step": 15250
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.1944270133972168,
      "learning_rate": 5.466666666666666e-05,
      "loss": 2.688,
      "step": 15260
    },
    {
      "epoch": 2.1814285714285715,
      "grad_norm": 1.224220871925354,
      "learning_rate": 5.457142857142857e-05,
      "loss": 2.6755,
      "step": 15270
    },
    {
      "epoch": 2.182857142857143,
      "grad_norm": 1.1527165174484253,
      "learning_rate": 5.447619047619048e-05,
      "loss": 2.5661,
      "step": 15280
    },
    {
      "epoch": 2.184285714285714,
      "grad_norm": 1.2260282039642334,
      "learning_rate": 5.4380952380952386e-05,
      "loss": 2.6924,
      "step": 15290
    },
    {
      "epoch": 2.185714285714286,
      "grad_norm": 0.9364953637123108,
      "learning_rate": 5.428571428571428e-05,
      "loss": 2.6846,
      "step": 15300
    },
    {
      "epoch": 2.1871428571428573,
      "grad_norm": 1.4806586503982544,
      "learning_rate": 5.419047619047619e-05,
      "loss": 2.7391,
      "step": 15310
    },
    {
      "epoch": 2.1885714285714286,
      "grad_norm": 1.0466004610061646,
      "learning_rate": 5.40952380952381e-05,
      "loss": 2.5379,
      "step": 15320
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9564112424850464,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 2.6837,
      "step": 15330
    },
    {
      "epoch": 2.1914285714285713,
      "grad_norm": 1.042501449584961,
      "learning_rate": 5.39047619047619e-05,
      "loss": 2.6338,
      "step": 15340
    },
    {
      "epoch": 2.192857142857143,
      "grad_norm": 1.2663434743881226,
      "learning_rate": 5.380952380952381e-05,
      "loss": 2.7239,
      "step": 15350
    },
    {
      "epoch": 2.1942857142857144,
      "grad_norm": 1.0725758075714111,
      "learning_rate": 5.3714285714285714e-05,
      "loss": 2.5999,
      "step": 15360
    },
    {
      "epoch": 2.1957142857142857,
      "grad_norm": 0.9728055596351624,
      "learning_rate": 5.3619047619047624e-05,
      "loss": 2.5302,
      "step": 15370
    },
    {
      "epoch": 2.197142857142857,
      "grad_norm": 1.4165592193603516,
      "learning_rate": 5.3523809523809534e-05,
      "loss": 2.6534,
      "step": 15380
    },
    {
      "epoch": 2.1985714285714284,
      "grad_norm": 1.1465479135513306,
      "learning_rate": 5.342857142857143e-05,
      "loss": 2.6481,
      "step": 15390
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.3469313383102417,
      "learning_rate": 5.333333333333333e-05,
      "loss": 2.4827,
      "step": 15400
    },
    {
      "epoch": 2.2014285714285715,
      "grad_norm": 1.0823965072631836,
      "learning_rate": 5.323809523809524e-05,
      "loss": 2.5764,
      "step": 15410
    },
    {
      "epoch": 2.202857142857143,
      "grad_norm": 1.7077597379684448,
      "learning_rate": 5.314285714285715e-05,
      "loss": 2.7353,
      "step": 15420
    },
    {
      "epoch": 2.204285714285714,
      "grad_norm": 1.3513267040252686,
      "learning_rate": 5.304761904761905e-05,
      "loss": 2.5663,
      "step": 15430
    },
    {
      "epoch": 2.2057142857142855,
      "grad_norm": 1.2102015018463135,
      "learning_rate": 5.295238095238095e-05,
      "loss": 2.6815,
      "step": 15440
    },
    {
      "epoch": 2.2071428571428573,
      "grad_norm": 0.901668906211853,
      "learning_rate": 5.285714285714286e-05,
      "loss": 2.4822,
      "step": 15450
    },
    {
      "epoch": 2.2085714285714286,
      "grad_norm": 1.2928708791732788,
      "learning_rate": 5.2761904761904765e-05,
      "loss": 2.5823,
      "step": 15460
    },
    {
      "epoch": 2.21,
      "grad_norm": 1.5049744844436646,
      "learning_rate": 5.266666666666666e-05,
      "loss": 2.7057,
      "step": 15470
    },
    {
      "epoch": 2.2114285714285713,
      "grad_norm": 1.297997236251831,
      "learning_rate": 5.257142857142857e-05,
      "loss": 2.717,
      "step": 15480
    },
    {
      "epoch": 2.2128571428571426,
      "grad_norm": 1.835727334022522,
      "learning_rate": 5.247619047619048e-05,
      "loss": 2.5316,
      "step": 15490
    },
    {
      "epoch": 2.2142857142857144,
      "grad_norm": 1.1367032527923584,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 2.5469,
      "step": 15500
    },
    {
      "epoch": 2.2157142857142857,
      "grad_norm": 1.4215964078903198,
      "learning_rate": 5.2285714285714294e-05,
      "loss": 2.6928,
      "step": 15510
    },
    {
      "epoch": 2.217142857142857,
      "grad_norm": 1.6062177419662476,
      "learning_rate": 5.219047619047619e-05,
      "loss": 2.6801,
      "step": 15520
    },
    {
      "epoch": 2.2185714285714284,
      "grad_norm": 1.161611795425415,
      "learning_rate": 5.2095238095238094e-05,
      "loss": 2.6677,
      "step": 15530
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.9837559461593628,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 2.7343,
      "step": 15540
    },
    {
      "epoch": 2.2214285714285715,
      "grad_norm": 1.371884822845459,
      "learning_rate": 5.1904761904761913e-05,
      "loss": 2.5674,
      "step": 15550
    },
    {
      "epoch": 2.222857142857143,
      "grad_norm": 1.163683295249939,
      "learning_rate": 5.180952380952381e-05,
      "loss": 2.6827,
      "step": 15560
    },
    {
      "epoch": 2.224285714285714,
      "grad_norm": 1.2493150234222412,
      "learning_rate": 5.171428571428571e-05,
      "loss": 2.6052,
      "step": 15570
    },
    {
      "epoch": 2.2257142857142855,
      "grad_norm": 1.003504753112793,
      "learning_rate": 5.161904761904762e-05,
      "loss": 2.6425,
      "step": 15580
    },
    {
      "epoch": 2.2271428571428573,
      "grad_norm": 1.0978095531463623,
      "learning_rate": 5.152380952380953e-05,
      "loss": 2.7091,
      "step": 15590
    },
    {
      "epoch": 2.2285714285714286,
      "grad_norm": 1.2977935075759888,
      "learning_rate": 5.142857142857143e-05,
      "loss": 2.6152,
      "step": 15600
    },
    {
      "epoch": 2.23,
      "grad_norm": 1.1277520656585693,
      "learning_rate": 5.133333333333333e-05,
      "loss": 2.6292,
      "step": 15610
    },
    {
      "epoch": 2.2314285714285713,
      "grad_norm": 1.0586727857589722,
      "learning_rate": 5.123809523809524e-05,
      "loss": 2.4718,
      "step": 15620
    },
    {
      "epoch": 2.2328571428571427,
      "grad_norm": 1.2205636501312256,
      "learning_rate": 5.1142857142857145e-05,
      "loss": 2.6899,
      "step": 15630
    },
    {
      "epoch": 2.2342857142857144,
      "grad_norm": 1.6614238023757935,
      "learning_rate": 5.1047619047619055e-05,
      "loss": 2.6885,
      "step": 15640
    },
    {
      "epoch": 2.2357142857142858,
      "grad_norm": 1.0108013153076172,
      "learning_rate": 5.095238095238095e-05,
      "loss": 2.6,
      "step": 15650
    },
    {
      "epoch": 2.237142857142857,
      "grad_norm": 0.9830862283706665,
      "learning_rate": 5.085714285714286e-05,
      "loss": 2.7944,
      "step": 15660
    },
    {
      "epoch": 2.2385714285714284,
      "grad_norm": 1.1806597709655762,
      "learning_rate": 5.0761904761904764e-05,
      "loss": 2.6365,
      "step": 15670
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.0122452974319458,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 2.7354,
      "step": 15680
    },
    {
      "epoch": 2.2414285714285715,
      "grad_norm": 1.1744141578674316,
      "learning_rate": 5.057142857142857e-05,
      "loss": 2.5354,
      "step": 15690
    },
    {
      "epoch": 2.242857142857143,
      "grad_norm": 1.1536259651184082,
      "learning_rate": 5.047619047619048e-05,
      "loss": 2.5478,
      "step": 15700
    },
    {
      "epoch": 2.244285714285714,
      "grad_norm": 1.710339903831482,
      "learning_rate": 5.038095238095238e-05,
      "loss": 2.7129,
      "step": 15710
    },
    {
      "epoch": 2.2457142857142856,
      "grad_norm": 0.9987824559211731,
      "learning_rate": 5.028571428571429e-05,
      "loss": 2.7077,
      "step": 15720
    },
    {
      "epoch": 2.2471428571428573,
      "grad_norm": 1.3623855113983154,
      "learning_rate": 5.019047619047619e-05,
      "loss": 2.7191,
      "step": 15730
    },
    {
      "epoch": 2.2485714285714287,
      "grad_norm": 1.1538019180297852,
      "learning_rate": 5.009523809523809e-05,
      "loss": 2.7304,
      "step": 15740
    },
    {
      "epoch": 2.25,
      "grad_norm": 1.4427260160446167,
      "learning_rate": 5e-05,
      "loss": 2.6216,
      "step": 15750
    },
    {
      "epoch": 2.2514285714285713,
      "grad_norm": 1.1856439113616943,
      "learning_rate": 4.990476190476191e-05,
      "loss": 2.6157,
      "step": 15760
    },
    {
      "epoch": 2.2528571428571427,
      "grad_norm": 1.0354129076004028,
      "learning_rate": 4.980952380952381e-05,
      "loss": 2.5635,
      "step": 15770
    },
    {
      "epoch": 2.2542857142857144,
      "grad_norm": 1.2976051568984985,
      "learning_rate": 4.971428571428572e-05,
      "loss": 2.6629,
      "step": 15780
    },
    {
      "epoch": 2.255714285714286,
      "grad_norm": 1.3097422122955322,
      "learning_rate": 4.961904761904762e-05,
      "loss": 2.7355,
      "step": 15790
    },
    {
      "epoch": 2.257142857142857,
      "grad_norm": 1.1070008277893066,
      "learning_rate": 4.9523809523809525e-05,
      "loss": 2.5644,
      "step": 15800
    },
    {
      "epoch": 2.2585714285714285,
      "grad_norm": 1.2956851720809937,
      "learning_rate": 4.942857142857143e-05,
      "loss": 2.6626,
      "step": 15810
    },
    {
      "epoch": 2.26,
      "grad_norm": 1.3548526763916016,
      "learning_rate": 4.933333333333334e-05,
      "loss": 2.4778,
      "step": 15820
    },
    {
      "epoch": 2.2614285714285716,
      "grad_norm": 0.9287746548652649,
      "learning_rate": 4.923809523809524e-05,
      "loss": 2.5472,
      "step": 15830
    },
    {
      "epoch": 2.262857142857143,
      "grad_norm": 1.4371559619903564,
      "learning_rate": 4.9142857142857144e-05,
      "loss": 2.5992,
      "step": 15840
    },
    {
      "epoch": 2.2642857142857142,
      "grad_norm": 1.0738643407821655,
      "learning_rate": 4.904761904761905e-05,
      "loss": 2.5403,
      "step": 15850
    },
    {
      "epoch": 2.2657142857142856,
      "grad_norm": 1.2239651679992676,
      "learning_rate": 4.895238095238096e-05,
      "loss": 2.5703,
      "step": 15860
    },
    {
      "epoch": 2.267142857142857,
      "grad_norm": 1.0479497909545898,
      "learning_rate": 4.885714285714286e-05,
      "loss": 2.4066,
      "step": 15870
    },
    {
      "epoch": 2.2685714285714287,
      "grad_norm": 1.4187443256378174,
      "learning_rate": 4.876190476190476e-05,
      "loss": 2.5517,
      "step": 15880
    },
    {
      "epoch": 2.27,
      "grad_norm": 1.0570952892303467,
      "learning_rate": 4.866666666666667e-05,
      "loss": 2.5972,
      "step": 15890
    },
    {
      "epoch": 2.2714285714285714,
      "grad_norm": 1.2828477621078491,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 2.6114,
      "step": 15900
    },
    {
      "epoch": 2.2728571428571427,
      "grad_norm": 1.2193516492843628,
      "learning_rate": 4.847619047619048e-05,
      "loss": 2.5467,
      "step": 15910
    },
    {
      "epoch": 2.2742857142857145,
      "grad_norm": 1.0332977771759033,
      "learning_rate": 4.838095238095238e-05,
      "loss": 2.6505,
      "step": 15920
    },
    {
      "epoch": 2.275714285714286,
      "grad_norm": 0.8130179643630981,
      "learning_rate": 4.828571428571429e-05,
      "loss": 2.6656,
      "step": 15930
    },
    {
      "epoch": 2.277142857142857,
      "grad_norm": 1.1448345184326172,
      "learning_rate": 4.819047619047619e-05,
      "loss": 2.5218,
      "step": 15940
    },
    {
      "epoch": 2.2785714285714285,
      "grad_norm": 1.8405276536941528,
      "learning_rate": 4.80952380952381e-05,
      "loss": 2.4262,
      "step": 15950
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 1.3383029699325562,
      "learning_rate": 4.8e-05,
      "loss": 2.5439,
      "step": 15960
    },
    {
      "epoch": 2.2814285714285716,
      "grad_norm": 1.0443768501281738,
      "learning_rate": 4.790476190476191e-05,
      "loss": 2.67,
      "step": 15970
    },
    {
      "epoch": 2.282857142857143,
      "grad_norm": 1.1715222597122192,
      "learning_rate": 4.780952380952381e-05,
      "loss": 2.6083,
      "step": 15980
    },
    {
      "epoch": 2.2842857142857143,
      "grad_norm": 1.1907814741134644,
      "learning_rate": 4.771428571428572e-05,
      "loss": 2.6336,
      "step": 15990
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 1.2263455390930176,
      "learning_rate": 4.761904761904762e-05,
      "loss": 2.7038,
      "step": 16000
    },
    {
      "epoch": 2.2871428571428574,
      "grad_norm": 1.2740192413330078,
      "learning_rate": 4.7523809523809523e-05,
      "loss": 2.8822,
      "step": 16010
    },
    {
      "epoch": 2.2885714285714287,
      "grad_norm": 1.1661697626113892,
      "learning_rate": 4.742857142857143e-05,
      "loss": 2.6851,
      "step": 16020
    },
    {
      "epoch": 2.29,
      "grad_norm": 1.0404852628707886,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 2.6602,
      "step": 16030
    },
    {
      "epoch": 2.2914285714285714,
      "grad_norm": 1.021868109703064,
      "learning_rate": 4.723809523809524e-05,
      "loss": 2.6744,
      "step": 16040
    },
    {
      "epoch": 2.2928571428571427,
      "grad_norm": 1.186726689338684,
      "learning_rate": 4.714285714285714e-05,
      "loss": 2.6208,
      "step": 16050
    },
    {
      "epoch": 2.2942857142857145,
      "grad_norm": 1.2115849256515503,
      "learning_rate": 4.704761904761905e-05,
      "loss": 2.7042,
      "step": 16060
    },
    {
      "epoch": 2.295714285714286,
      "grad_norm": 1.0703281164169312,
      "learning_rate": 4.6952380952380956e-05,
      "loss": 2.6241,
      "step": 16070
    },
    {
      "epoch": 2.297142857142857,
      "grad_norm": 0.9998387694358826,
      "learning_rate": 4.685714285714286e-05,
      "loss": 2.7409,
      "step": 16080
    },
    {
      "epoch": 2.2985714285714285,
      "grad_norm": 0.9514314532279968,
      "learning_rate": 4.676190476190476e-05,
      "loss": 2.5163,
      "step": 16090
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.925217866897583,
      "learning_rate": 4.666666666666667e-05,
      "loss": 2.8706,
      "step": 16100
    },
    {
      "epoch": 2.3014285714285716,
      "grad_norm": 1.4126901626586914,
      "learning_rate": 4.6571428571428575e-05,
      "loss": 2.6836,
      "step": 16110
    },
    {
      "epoch": 2.302857142857143,
      "grad_norm": 1.2020714282989502,
      "learning_rate": 4.647619047619048e-05,
      "loss": 2.5528,
      "step": 16120
    },
    {
      "epoch": 2.3042857142857143,
      "grad_norm": 1.0615580081939697,
      "learning_rate": 4.638095238095238e-05,
      "loss": 2.7163,
      "step": 16130
    },
    {
      "epoch": 2.3057142857142856,
      "grad_norm": 0.9631093144416809,
      "learning_rate": 4.628571428571429e-05,
      "loss": 2.7631,
      "step": 16140
    },
    {
      "epoch": 2.307142857142857,
      "grad_norm": 1.2294073104858398,
      "learning_rate": 4.6190476190476194e-05,
      "loss": 2.6767,
      "step": 16150
    },
    {
      "epoch": 2.3085714285714287,
      "grad_norm": 1.2666451930999756,
      "learning_rate": 4.60952380952381e-05,
      "loss": 2.725,
      "step": 16160
    },
    {
      "epoch": 2.31,
      "grad_norm": 1.0170869827270508,
      "learning_rate": 4.600000000000001e-05,
      "loss": 2.5316,
      "step": 16170
    },
    {
      "epoch": 2.3114285714285714,
      "grad_norm": 1.353705644607544,
      "learning_rate": 4.59047619047619e-05,
      "loss": 2.5424,
      "step": 16180
    },
    {
      "epoch": 2.3128571428571427,
      "grad_norm": 1.144378423690796,
      "learning_rate": 4.580952380952381e-05,
      "loss": 2.6951,
      "step": 16190
    },
    {
      "epoch": 2.314285714285714,
      "grad_norm": 1.0927419662475586,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 2.5111,
      "step": 16200
    },
    {
      "epoch": 2.315714285714286,
      "grad_norm": 1.3101143836975098,
      "learning_rate": 4.561904761904762e-05,
      "loss": 2.5256,
      "step": 16210
    },
    {
      "epoch": 2.317142857142857,
      "grad_norm": 1.0719250440597534,
      "learning_rate": 4.552380952380952e-05,
      "loss": 2.5912,
      "step": 16220
    },
    {
      "epoch": 2.3185714285714285,
      "grad_norm": 0.8537195920944214,
      "learning_rate": 4.542857142857143e-05,
      "loss": 2.5952,
      "step": 16230
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.0549747943878174,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 2.6013,
      "step": 16240
    },
    {
      "epoch": 2.3214285714285716,
      "grad_norm": 1.3006542921066284,
      "learning_rate": 4.523809523809524e-05,
      "loss": 2.7086,
      "step": 16250
    },
    {
      "epoch": 2.322857142857143,
      "grad_norm": 1.006700873374939,
      "learning_rate": 4.514285714285714e-05,
      "loss": 2.6912,
      "step": 16260
    },
    {
      "epoch": 2.3242857142857143,
      "grad_norm": 1.0463590621948242,
      "learning_rate": 4.504761904761905e-05,
      "loss": 2.5969,
      "step": 16270
    },
    {
      "epoch": 2.3257142857142856,
      "grad_norm": 1.1391571760177612,
      "learning_rate": 4.4952380952380954e-05,
      "loss": 2.7123,
      "step": 16280
    },
    {
      "epoch": 2.327142857142857,
      "grad_norm": 1.1573104858398438,
      "learning_rate": 4.485714285714286e-05,
      "loss": 2.7318,
      "step": 16290
    },
    {
      "epoch": 2.3285714285714287,
      "grad_norm": 1.1800440549850464,
      "learning_rate": 4.476190476190477e-05,
      "loss": 2.7038,
      "step": 16300
    },
    {
      "epoch": 2.33,
      "grad_norm": 1.0262305736541748,
      "learning_rate": 4.466666666666667e-05,
      "loss": 2.6527,
      "step": 16310
    },
    {
      "epoch": 2.3314285714285714,
      "grad_norm": 1.2590090036392212,
      "learning_rate": 4.4571428571428574e-05,
      "loss": 2.6272,
      "step": 16320
    },
    {
      "epoch": 2.3328571428571427,
      "grad_norm": 1.0376851558685303,
      "learning_rate": 4.447619047619048e-05,
      "loss": 2.6746,
      "step": 16330
    },
    {
      "epoch": 2.3342857142857145,
      "grad_norm": 1.1603225469589233,
      "learning_rate": 4.4380952380952386e-05,
      "loss": 2.6196,
      "step": 16340
    },
    {
      "epoch": 2.335714285714286,
      "grad_norm": 1.373808741569519,
      "learning_rate": 4.428571428571428e-05,
      "loss": 2.7778,
      "step": 16350
    },
    {
      "epoch": 2.337142857142857,
      "grad_norm": 1.067482352256775,
      "learning_rate": 4.419047619047619e-05,
      "loss": 2.7997,
      "step": 16360
    },
    {
      "epoch": 2.3385714285714285,
      "grad_norm": 1.1245547533035278,
      "learning_rate": 4.4095238095238096e-05,
      "loss": 2.5283,
      "step": 16370
    },
    {
      "epoch": 2.34,
      "grad_norm": 1.3094269037246704,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.7035,
      "step": 16380
    },
    {
      "epoch": 2.3414285714285716,
      "grad_norm": 1.254378080368042,
      "learning_rate": 4.39047619047619e-05,
      "loss": 2.5143,
      "step": 16390
    },
    {
      "epoch": 2.342857142857143,
      "grad_norm": 1.1451773643493652,
      "learning_rate": 4.380952380952381e-05,
      "loss": 2.535,
      "step": 16400
    },
    {
      "epoch": 2.3442857142857143,
      "grad_norm": 1.1254782676696777,
      "learning_rate": 4.371428571428572e-05,
      "loss": 2.6688,
      "step": 16410
    },
    {
      "epoch": 2.3457142857142856,
      "grad_norm": 1.2895963191986084,
      "learning_rate": 4.361904761904762e-05,
      "loss": 2.7455,
      "step": 16420
    },
    {
      "epoch": 2.347142857142857,
      "grad_norm": 1.1684210300445557,
      "learning_rate": 4.352380952380953e-05,
      "loss": 2.7141,
      "step": 16430
    },
    {
      "epoch": 2.3485714285714288,
      "grad_norm": 1.522594928741455,
      "learning_rate": 4.342857142857143e-05,
      "loss": 2.4556,
      "step": 16440
    },
    {
      "epoch": 2.35,
      "grad_norm": 1.2810431718826294,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 2.5412,
      "step": 16450
    },
    {
      "epoch": 2.3514285714285714,
      "grad_norm": 1.1602668762207031,
      "learning_rate": 4.323809523809524e-05,
      "loss": 2.6796,
      "step": 16460
    },
    {
      "epoch": 2.3528571428571428,
      "grad_norm": 1.4945181608200073,
      "learning_rate": 4.314285714285715e-05,
      "loss": 2.6053,
      "step": 16470
    },
    {
      "epoch": 2.354285714285714,
      "grad_norm": 1.631603479385376,
      "learning_rate": 4.304761904761905e-05,
      "loss": 2.6513,
      "step": 16480
    },
    {
      "epoch": 2.355714285714286,
      "grad_norm": 1.3038036823272705,
      "learning_rate": 4.295238095238095e-05,
      "loss": 2.6883,
      "step": 16490
    },
    {
      "epoch": 2.357142857142857,
      "grad_norm": 0.8806813359260559,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 2.7695,
      "step": 16500
    },
    {
      "epoch": 2.3585714285714285,
      "grad_norm": 1.1904356479644775,
      "learning_rate": 4.2761904761904766e-05,
      "loss": 2.5795,
      "step": 16510
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.9839539527893066,
      "learning_rate": 4.266666666666667e-05,
      "loss": 2.6122,
      "step": 16520
    },
    {
      "epoch": 2.361428571428571,
      "grad_norm": 0.9866438508033752,
      "learning_rate": 4.257142857142857e-05,
      "loss": 2.634,
      "step": 16530
    },
    {
      "epoch": 2.362857142857143,
      "grad_norm": 1.0665440559387207,
      "learning_rate": 4.247619047619048e-05,
      "loss": 2.684,
      "step": 16540
    },
    {
      "epoch": 2.3642857142857143,
      "grad_norm": 1.123990774154663,
      "learning_rate": 4.2380952380952385e-05,
      "loss": 2.6511,
      "step": 16550
    },
    {
      "epoch": 2.3657142857142857,
      "grad_norm": 1.3974107503890991,
      "learning_rate": 4.228571428571429e-05,
      "loss": 2.6388,
      "step": 16560
    },
    {
      "epoch": 2.367142857142857,
      "grad_norm": 1.056531548500061,
      "learning_rate": 4.219047619047619e-05,
      "loss": 2.5937,
      "step": 16570
    },
    {
      "epoch": 2.3685714285714283,
      "grad_norm": 1.2864183187484741,
      "learning_rate": 4.20952380952381e-05,
      "loss": 2.7744,
      "step": 16580
    },
    {
      "epoch": 2.37,
      "grad_norm": 1.2295615673065186,
      "learning_rate": 4.2e-05,
      "loss": 2.6297,
      "step": 16590
    },
    {
      "epoch": 2.3714285714285714,
      "grad_norm": 1.0679984092712402,
      "learning_rate": 4.190476190476191e-05,
      "loss": 2.5866,
      "step": 16600
    },
    {
      "epoch": 2.3728571428571428,
      "grad_norm": 1.0105764865875244,
      "learning_rate": 4.180952380952381e-05,
      "loss": 2.4872,
      "step": 16610
    },
    {
      "epoch": 2.374285714285714,
      "grad_norm": 1.1568727493286133,
      "learning_rate": 4.1714285714285714e-05,
      "loss": 2.5687,
      "step": 16620
    },
    {
      "epoch": 2.375714285714286,
      "grad_norm": 1.4736721515655518,
      "learning_rate": 4.161904761904762e-05,
      "loss": 2.6478,
      "step": 16630
    },
    {
      "epoch": 2.3771428571428572,
      "grad_norm": 1.4164763689041138,
      "learning_rate": 4.152380952380953e-05,
      "loss": 2.5979,
      "step": 16640
    },
    {
      "epoch": 2.3785714285714286,
      "grad_norm": 1.4787960052490234,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 2.5023,
      "step": 16650
    },
    {
      "epoch": 2.38,
      "grad_norm": 1.4698861837387085,
      "learning_rate": 4.133333333333333e-05,
      "loss": 2.573,
      "step": 16660
    },
    {
      "epoch": 2.3814285714285717,
      "grad_norm": 1.173962116241455,
      "learning_rate": 4.123809523809524e-05,
      "loss": 2.6041,
      "step": 16670
    },
    {
      "epoch": 2.382857142857143,
      "grad_norm": 1.0971871614456177,
      "learning_rate": 4.1142857142857146e-05,
      "loss": 2.6414,
      "step": 16680
    },
    {
      "epoch": 2.3842857142857143,
      "grad_norm": 1.0149054527282715,
      "learning_rate": 4.104761904761905e-05,
      "loss": 2.6401,
      "step": 16690
    },
    {
      "epoch": 2.3857142857142857,
      "grad_norm": 1.2685327529907227,
      "learning_rate": 4.095238095238095e-05,
      "loss": 2.6209,
      "step": 16700
    },
    {
      "epoch": 2.387142857142857,
      "grad_norm": 0.9628116488456726,
      "learning_rate": 4.085714285714286e-05,
      "loss": 2.6548,
      "step": 16710
    },
    {
      "epoch": 2.388571428571429,
      "grad_norm": 1.0863208770751953,
      "learning_rate": 4.0761904761904765e-05,
      "loss": 2.7187,
      "step": 16720
    },
    {
      "epoch": 2.39,
      "grad_norm": 1.021639108657837,
      "learning_rate": 4.066666666666667e-05,
      "loss": 2.6222,
      "step": 16730
    },
    {
      "epoch": 2.3914285714285715,
      "grad_norm": 1.258023738861084,
      "learning_rate": 4.057142857142857e-05,
      "loss": 2.6432,
      "step": 16740
    },
    {
      "epoch": 2.392857142857143,
      "grad_norm": 1.3662052154541016,
      "learning_rate": 4.047619047619048e-05,
      "loss": 2.599,
      "step": 16750
    },
    {
      "epoch": 2.394285714285714,
      "grad_norm": 1.0693904161453247,
      "learning_rate": 4.038095238095238e-05,
      "loss": 2.5773,
      "step": 16760
    },
    {
      "epoch": 2.395714285714286,
      "grad_norm": 1.1277741193771362,
      "learning_rate": 4.028571428571429e-05,
      "loss": 2.6346,
      "step": 16770
    },
    {
      "epoch": 2.3971428571428572,
      "grad_norm": 1.209004282951355,
      "learning_rate": 4.01904761904762e-05,
      "loss": 2.6438,
      "step": 16780
    },
    {
      "epoch": 2.3985714285714286,
      "grad_norm": 1.2984546422958374,
      "learning_rate": 4.00952380952381e-05,
      "loss": 2.7725,
      "step": 16790
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.2616420984268188,
      "learning_rate": 4e-05,
      "loss": 2.5015,
      "step": 16800
    },
    {
      "epoch": 2.4014285714285712,
      "grad_norm": 0.9440129995346069,
      "learning_rate": 3.9904761904761906e-05,
      "loss": 2.6858,
      "step": 16810
    },
    {
      "epoch": 2.402857142857143,
      "grad_norm": 1.4453741312026978,
      "learning_rate": 3.9809523809523816e-05,
      "loss": 2.6742,
      "step": 16820
    },
    {
      "epoch": 2.4042857142857144,
      "grad_norm": 1.16273033618927,
      "learning_rate": 3.971428571428571e-05,
      "loss": 2.6789,
      "step": 16830
    },
    {
      "epoch": 2.4057142857142857,
      "grad_norm": 1.5785826444625854,
      "learning_rate": 3.961904761904762e-05,
      "loss": 2.5574,
      "step": 16840
    },
    {
      "epoch": 2.407142857142857,
      "grad_norm": 1.1180940866470337,
      "learning_rate": 3.9523809523809526e-05,
      "loss": 2.6138,
      "step": 16850
    },
    {
      "epoch": 2.4085714285714284,
      "grad_norm": 1.0822780132293701,
      "learning_rate": 3.942857142857143e-05,
      "loss": 2.6279,
      "step": 16860
    },
    {
      "epoch": 2.41,
      "grad_norm": 1.3151829242706299,
      "learning_rate": 3.933333333333333e-05,
      "loss": 2.6135,
      "step": 16870
    },
    {
      "epoch": 2.4114285714285715,
      "grad_norm": 1.2264786958694458,
      "learning_rate": 3.923809523809524e-05,
      "loss": 2.6328,
      "step": 16880
    },
    {
      "epoch": 2.412857142857143,
      "grad_norm": 1.0468777418136597,
      "learning_rate": 3.9142857142857145e-05,
      "loss": 2.6895,
      "step": 16890
    },
    {
      "epoch": 2.414285714285714,
      "grad_norm": 0.973233163356781,
      "learning_rate": 3.904761904761905e-05,
      "loss": 2.593,
      "step": 16900
    },
    {
      "epoch": 2.4157142857142855,
      "grad_norm": 0.9966414570808411,
      "learning_rate": 3.895238095238096e-05,
      "loss": 2.6785,
      "step": 16910
    },
    {
      "epoch": 2.4171428571428573,
      "grad_norm": 1.4504650831222534,
      "learning_rate": 3.885714285714286e-05,
      "loss": 2.5813,
      "step": 16920
    },
    {
      "epoch": 2.4185714285714286,
      "grad_norm": 1.265587329864502,
      "learning_rate": 3.8761904761904764e-05,
      "loss": 2.7165,
      "step": 16930
    },
    {
      "epoch": 2.42,
      "grad_norm": 1.258827567100525,
      "learning_rate": 3.866666666666667e-05,
      "loss": 2.7071,
      "step": 16940
    },
    {
      "epoch": 2.4214285714285713,
      "grad_norm": 1.2392079830169678,
      "learning_rate": 3.857142857142858e-05,
      "loss": 2.6864,
      "step": 16950
    },
    {
      "epoch": 2.422857142857143,
      "grad_norm": 1.2413605451583862,
      "learning_rate": 3.847619047619048e-05,
      "loss": 2.6481,
      "step": 16960
    },
    {
      "epoch": 2.4242857142857144,
      "grad_norm": 1.3585500717163086,
      "learning_rate": 3.838095238095238e-05,
      "loss": 2.5716,
      "step": 16970
    },
    {
      "epoch": 2.4257142857142857,
      "grad_norm": 1.2711615562438965,
      "learning_rate": 3.8285714285714286e-05,
      "loss": 2.8305,
      "step": 16980
    },
    {
      "epoch": 2.427142857142857,
      "grad_norm": 1.1073464155197144,
      "learning_rate": 3.8190476190476196e-05,
      "loss": 2.632,
      "step": 16990
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 1.0812010765075684,
      "learning_rate": 3.809523809523809e-05,
      "loss": 2.6576,
      "step": 17000
    },
    {
      "epoch": 2.43,
      "grad_norm": 1.2709013223648071,
      "learning_rate": 3.8e-05,
      "loss": 2.6543,
      "step": 17010
    },
    {
      "epoch": 2.4314285714285715,
      "grad_norm": 2.005969524383545,
      "learning_rate": 3.7904761904761905e-05,
      "loss": 2.7623,
      "step": 17020
    },
    {
      "epoch": 2.432857142857143,
      "grad_norm": 1.3222492933273315,
      "learning_rate": 3.780952380952381e-05,
      "loss": 2.5883,
      "step": 17030
    },
    {
      "epoch": 2.434285714285714,
      "grad_norm": 1.1291238069534302,
      "learning_rate": 3.771428571428572e-05,
      "loss": 2.6325,
      "step": 17040
    },
    {
      "epoch": 2.435714285714286,
      "grad_norm": 1.9280774593353271,
      "learning_rate": 3.761904761904762e-05,
      "loss": 2.6998,
      "step": 17050
    },
    {
      "epoch": 2.4371428571428573,
      "grad_norm": 1.163182020187378,
      "learning_rate": 3.752380952380953e-05,
      "loss": 2.7244,
      "step": 17060
    },
    {
      "epoch": 2.4385714285714286,
      "grad_norm": 1.1303683519363403,
      "learning_rate": 3.742857142857143e-05,
      "loss": 2.655,
      "step": 17070
    },
    {
      "epoch": 2.44,
      "grad_norm": 1.0336456298828125,
      "learning_rate": 3.733333333333334e-05,
      "loss": 2.4504,
      "step": 17080
    },
    {
      "epoch": 2.4414285714285713,
      "grad_norm": 1.1734675168991089,
      "learning_rate": 3.723809523809524e-05,
      "loss": 2.5064,
      "step": 17090
    },
    {
      "epoch": 2.442857142857143,
      "grad_norm": 1.1587368249893188,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 2.6467,
      "step": 17100
    },
    {
      "epoch": 2.4442857142857144,
      "grad_norm": 0.9909417033195496,
      "learning_rate": 3.7047619047619047e-05,
      "loss": 2.6269,
      "step": 17110
    },
    {
      "epoch": 2.4457142857142857,
      "grad_norm": 1.1166843175888062,
      "learning_rate": 3.6952380952380956e-05,
      "loss": 2.6773,
      "step": 17120
    },
    {
      "epoch": 2.447142857142857,
      "grad_norm": 1.3735201358795166,
      "learning_rate": 3.685714285714286e-05,
      "loss": 2.6823,
      "step": 17130
    },
    {
      "epoch": 2.4485714285714284,
      "grad_norm": 1.6075316667556763,
      "learning_rate": 3.676190476190476e-05,
      "loss": 2.6196,
      "step": 17140
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.27748703956604,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 2.3086,
      "step": 17150
    },
    {
      "epoch": 2.4514285714285715,
      "grad_norm": 1.0310896635055542,
      "learning_rate": 3.6571428571428576e-05,
      "loss": 2.4739,
      "step": 17160
    },
    {
      "epoch": 2.452857142857143,
      "grad_norm": 0.9830540418624878,
      "learning_rate": 3.647619047619048e-05,
      "loss": 2.8245,
      "step": 17170
    },
    {
      "epoch": 2.454285714285714,
      "grad_norm": 1.218729019165039,
      "learning_rate": 3.638095238095238e-05,
      "loss": 2.5099,
      "step": 17180
    },
    {
      "epoch": 2.4557142857142855,
      "grad_norm": 1.1533405780792236,
      "learning_rate": 3.628571428571429e-05,
      "loss": 2.7694,
      "step": 17190
    },
    {
      "epoch": 2.4571428571428573,
      "grad_norm": 1.0670628547668457,
      "learning_rate": 3.619047619047619e-05,
      "loss": 2.7661,
      "step": 17200
    },
    {
      "epoch": 2.4585714285714286,
      "grad_norm": 0.9758344292640686,
      "learning_rate": 3.60952380952381e-05,
      "loss": 2.4731,
      "step": 17210
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.5619882345199585,
      "learning_rate": 3.6e-05,
      "loss": 2.6323,
      "step": 17220
    },
    {
      "epoch": 2.4614285714285713,
      "grad_norm": 1.1628072261810303,
      "learning_rate": 3.590476190476191e-05,
      "loss": 2.6504,
      "step": 17230
    },
    {
      "epoch": 2.4628571428571426,
      "grad_norm": 1.1757689714431763,
      "learning_rate": 3.580952380952381e-05,
      "loss": 2.6184,
      "step": 17240
    },
    {
      "epoch": 2.4642857142857144,
      "grad_norm": 1.3612513542175293,
      "learning_rate": 3.571428571428572e-05,
      "loss": 2.4835,
      "step": 17250
    },
    {
      "epoch": 2.4657142857142857,
      "grad_norm": 1.2820771932601929,
      "learning_rate": 3.561904761904762e-05,
      "loss": 2.7484,
      "step": 17260
    },
    {
      "epoch": 2.467142857142857,
      "grad_norm": 1.0915169715881348,
      "learning_rate": 3.552380952380952e-05,
      "loss": 2.6711,
      "step": 17270
    },
    {
      "epoch": 2.4685714285714284,
      "grad_norm": 0.8446233868598938,
      "learning_rate": 3.5428571428571426e-05,
      "loss": 2.6124,
      "step": 17280
    },
    {
      "epoch": 2.4699999999999998,
      "grad_norm": 1.200839877128601,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 2.8002,
      "step": 17290
    },
    {
      "epoch": 2.4714285714285715,
      "grad_norm": 1.125463843345642,
      "learning_rate": 3.523809523809524e-05,
      "loss": 2.6845,
      "step": 17300
    },
    {
      "epoch": 2.472857142857143,
      "grad_norm": 1.2722439765930176,
      "learning_rate": 3.514285714285714e-05,
      "loss": 2.4019,
      "step": 17310
    },
    {
      "epoch": 2.474285714285714,
      "grad_norm": 1.205419659614563,
      "learning_rate": 3.504761904761905e-05,
      "loss": 2.6402,
      "step": 17320
    },
    {
      "epoch": 2.4757142857142855,
      "grad_norm": 1.1504231691360474,
      "learning_rate": 3.4952380952380955e-05,
      "loss": 2.7006,
      "step": 17330
    },
    {
      "epoch": 2.4771428571428573,
      "grad_norm": 0.9822229146957397,
      "learning_rate": 3.485714285714286e-05,
      "loss": 2.6252,
      "step": 17340
    },
    {
      "epoch": 2.4785714285714286,
      "grad_norm": 1.4528032541275024,
      "learning_rate": 3.476190476190476e-05,
      "loss": 2.5482,
      "step": 17350
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.1563128232955933,
      "learning_rate": 3.466666666666667e-05,
      "loss": 2.6049,
      "step": 17360
    },
    {
      "epoch": 2.4814285714285713,
      "grad_norm": 1.0444837808609009,
      "learning_rate": 3.4571428571428574e-05,
      "loss": 2.6451,
      "step": 17370
    },
    {
      "epoch": 2.482857142857143,
      "grad_norm": 1.130640983581543,
      "learning_rate": 3.447619047619048e-05,
      "loss": 2.6098,
      "step": 17380
    },
    {
      "epoch": 2.4842857142857144,
      "grad_norm": 1.1600319147109985,
      "learning_rate": 3.438095238095238e-05,
      "loss": 2.5941,
      "step": 17390
    },
    {
      "epoch": 2.4857142857142858,
      "grad_norm": 1.331708312034607,
      "learning_rate": 3.428571428571429e-05,
      "loss": 2.6423,
      "step": 17400
    },
    {
      "epoch": 2.487142857142857,
      "grad_norm": 1.448630452156067,
      "learning_rate": 3.419047619047619e-05,
      "loss": 2.6645,
      "step": 17410
    },
    {
      "epoch": 2.4885714285714284,
      "grad_norm": 1.3874380588531494,
      "learning_rate": 3.40952380952381e-05,
      "loss": 2.6641,
      "step": 17420
    },
    {
      "epoch": 2.49,
      "grad_norm": 1.1221016645431519,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 2.5117,
      "step": 17430
    },
    {
      "epoch": 2.4914285714285715,
      "grad_norm": 1.3043376207351685,
      "learning_rate": 3.39047619047619e-05,
      "loss": 2.7296,
      "step": 17440
    },
    {
      "epoch": 2.492857142857143,
      "grad_norm": 1.543683648109436,
      "learning_rate": 3.380952380952381e-05,
      "loss": 2.7905,
      "step": 17450
    },
    {
      "epoch": 2.494285714285714,
      "grad_norm": 1.2970556020736694,
      "learning_rate": 3.3714285714285716e-05,
      "loss": 2.6178,
      "step": 17460
    },
    {
      "epoch": 2.4957142857142856,
      "grad_norm": 1.2598470449447632,
      "learning_rate": 3.361904761904762e-05,
      "loss": 2.6162,
      "step": 17470
    },
    {
      "epoch": 2.4971428571428573,
      "grad_norm": 1.2091596126556396,
      "learning_rate": 3.352380952380952e-05,
      "loss": 2.8032,
      "step": 17480
    },
    {
      "epoch": 2.4985714285714287,
      "grad_norm": 1.2391606569290161,
      "learning_rate": 3.342857142857143e-05,
      "loss": 2.6728,
      "step": 17490
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.319462776184082,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.4555,
      "step": 17500
    },
    {
      "epoch": 2.5014285714285713,
      "grad_norm": 1.3953783512115479,
      "learning_rate": 3.323809523809524e-05,
      "loss": 2.6859,
      "step": 17510
    },
    {
      "epoch": 2.5028571428571427,
      "grad_norm": 1.3300353288650513,
      "learning_rate": 3.314285714285714e-05,
      "loss": 2.582,
      "step": 17520
    },
    {
      "epoch": 2.5042857142857144,
      "grad_norm": 1.4743660688400269,
      "learning_rate": 3.304761904761905e-05,
      "loss": 2.5416,
      "step": 17530
    },
    {
      "epoch": 2.505714285714286,
      "grad_norm": 1.1977249383926392,
      "learning_rate": 3.2952380952380954e-05,
      "loss": 2.5438,
      "step": 17540
    },
    {
      "epoch": 2.507142857142857,
      "grad_norm": 1.3526113033294678,
      "learning_rate": 3.285714285714286e-05,
      "loss": 2.5307,
      "step": 17550
    },
    {
      "epoch": 2.5085714285714285,
      "grad_norm": 0.9988065958023071,
      "learning_rate": 3.276190476190477e-05,
      "loss": 2.4473,
      "step": 17560
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.23239004611969,
      "learning_rate": 3.266666666666667e-05,
      "loss": 2.704,
      "step": 17570
    },
    {
      "epoch": 2.5114285714285716,
      "grad_norm": 1.3784193992614746,
      "learning_rate": 3.257142857142857e-05,
      "loss": 2.6886,
      "step": 17580
    },
    {
      "epoch": 2.512857142857143,
      "grad_norm": 1.2380698919296265,
      "learning_rate": 3.2476190476190476e-05,
      "loss": 2.6212,
      "step": 17590
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 1.3249813318252563,
      "learning_rate": 3.2380952380952386e-05,
      "loss": 2.6825,
      "step": 17600
    },
    {
      "epoch": 2.5157142857142856,
      "grad_norm": 1.0713955163955688,
      "learning_rate": 3.228571428571428e-05,
      "loss": 2.6668,
      "step": 17610
    },
    {
      "epoch": 2.517142857142857,
      "grad_norm": 1.0550148487091064,
      "learning_rate": 3.219047619047619e-05,
      "loss": 2.5104,
      "step": 17620
    },
    {
      "epoch": 2.5185714285714287,
      "grad_norm": 1.3926537036895752,
      "learning_rate": 3.2095238095238095e-05,
      "loss": 2.6112,
      "step": 17630
    },
    {
      "epoch": 2.52,
      "grad_norm": 1.0263760089874268,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.5459,
      "step": 17640
    },
    {
      "epoch": 2.5214285714285714,
      "grad_norm": 1.1948695182800293,
      "learning_rate": 3.19047619047619e-05,
      "loss": 2.6353,
      "step": 17650
    },
    {
      "epoch": 2.522857142857143,
      "grad_norm": 0.9866454601287842,
      "learning_rate": 3.180952380952381e-05,
      "loss": 2.6273,
      "step": 17660
    },
    {
      "epoch": 2.524285714285714,
      "grad_norm": 1.2499346733093262,
      "learning_rate": 3.1714285714285715e-05,
      "loss": 2.662,
      "step": 17670
    },
    {
      "epoch": 2.525714285714286,
      "grad_norm": 1.2475850582122803,
      "learning_rate": 3.161904761904762e-05,
      "loss": 2.5544,
      "step": 17680
    },
    {
      "epoch": 2.527142857142857,
      "grad_norm": 1.00313138961792,
      "learning_rate": 3.152380952380953e-05,
      "loss": 2.6135,
      "step": 17690
    },
    {
      "epoch": 2.5285714285714285,
      "grad_norm": 1.357230544090271,
      "learning_rate": 3.142857142857143e-05,
      "loss": 2.6308,
      "step": 17700
    },
    {
      "epoch": 2.5300000000000002,
      "grad_norm": 1.384820818901062,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 2.6131,
      "step": 17710
    },
    {
      "epoch": 2.5314285714285716,
      "grad_norm": 1.1374669075012207,
      "learning_rate": 3.123809523809524e-05,
      "loss": 2.6441,
      "step": 17720
    },
    {
      "epoch": 2.532857142857143,
      "grad_norm": 1.2177681922912598,
      "learning_rate": 3.114285714285715e-05,
      "loss": 2.5884,
      "step": 17730
    },
    {
      "epoch": 2.5342857142857143,
      "grad_norm": 1.0746458768844604,
      "learning_rate": 3.104761904761905e-05,
      "loss": 2.6838,
      "step": 17740
    },
    {
      "epoch": 2.5357142857142856,
      "grad_norm": 1.634286642074585,
      "learning_rate": 3.095238095238095e-05,
      "loss": 2.7084,
      "step": 17750
    },
    {
      "epoch": 2.5371428571428574,
      "grad_norm": 1.2305525541305542,
      "learning_rate": 3.0857142857142856e-05,
      "loss": 2.6283,
      "step": 17760
    },
    {
      "epoch": 2.5385714285714287,
      "grad_norm": 1.1410154104232788,
      "learning_rate": 3.0761904761904766e-05,
      "loss": 2.5397,
      "step": 17770
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.2486754655838013,
      "learning_rate": 3.066666666666667e-05,
      "loss": 2.6962,
      "step": 17780
    },
    {
      "epoch": 2.5414285714285714,
      "grad_norm": 1.2297362089157104,
      "learning_rate": 3.057142857142857e-05,
      "loss": 2.7028,
      "step": 17790
    },
    {
      "epoch": 2.5428571428571427,
      "grad_norm": 1.1156511306762695,
      "learning_rate": 3.0476190476190482e-05,
      "loss": 2.5439,
      "step": 17800
    },
    {
      "epoch": 2.5442857142857145,
      "grad_norm": 1.6524102687835693,
      "learning_rate": 3.038095238095238e-05,
      "loss": 2.6255,
      "step": 17810
    },
    {
      "epoch": 2.545714285714286,
      "grad_norm": 1.0669004917144775,
      "learning_rate": 3.0285714285714288e-05,
      "loss": 2.6599,
      "step": 17820
    },
    {
      "epoch": 2.547142857142857,
      "grad_norm": 1.324695110321045,
      "learning_rate": 3.019047619047619e-05,
      "loss": 2.5185,
      "step": 17830
    },
    {
      "epoch": 2.5485714285714285,
      "grad_norm": 1.3890622854232788,
      "learning_rate": 3.0095238095238098e-05,
      "loss": 2.6567,
      "step": 17840
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.167414903640747,
      "learning_rate": 3e-05,
      "loss": 2.7435,
      "step": 17850
    },
    {
      "epoch": 2.5514285714285716,
      "grad_norm": 1.114912509918213,
      "learning_rate": 2.9904761904761907e-05,
      "loss": 2.5423,
      "step": 17860
    },
    {
      "epoch": 2.552857142857143,
      "grad_norm": 1.2304182052612305,
      "learning_rate": 2.980952380952381e-05,
      "loss": 2.6317,
      "step": 17870
    },
    {
      "epoch": 2.5542857142857143,
      "grad_norm": 1.4260867834091187,
      "learning_rate": 2.9714285714285717e-05,
      "loss": 2.4778,
      "step": 17880
    },
    {
      "epoch": 2.5557142857142856,
      "grad_norm": 1.1575312614440918,
      "learning_rate": 2.961904761904762e-05,
      "loss": 2.6831,
      "step": 17890
    },
    {
      "epoch": 2.557142857142857,
      "grad_norm": 0.9860103726387024,
      "learning_rate": 2.9523809523809526e-05,
      "loss": 2.5213,
      "step": 17900
    },
    {
      "epoch": 2.5585714285714287,
      "grad_norm": 1.0624070167541504,
      "learning_rate": 2.9428571428571426e-05,
      "loss": 2.8394,
      "step": 17910
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.7575113773345947,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 2.6259,
      "step": 17920
    },
    {
      "epoch": 2.5614285714285714,
      "grad_norm": 1.3246488571166992,
      "learning_rate": 2.9238095238095242e-05,
      "loss": 2.6067,
      "step": 17930
    },
    {
      "epoch": 2.5628571428571427,
      "grad_norm": 1.4034126996994019,
      "learning_rate": 2.9142857142857146e-05,
      "loss": 2.6765,
      "step": 17940
    },
    {
      "epoch": 2.564285714285714,
      "grad_norm": 1.0228171348571777,
      "learning_rate": 2.9047619047619052e-05,
      "loss": 2.5702,
      "step": 17950
    },
    {
      "epoch": 2.565714285714286,
      "grad_norm": 1.2785335779190063,
      "learning_rate": 2.8952380952380952e-05,
      "loss": 2.5849,
      "step": 17960
    },
    {
      "epoch": 2.567142857142857,
      "grad_norm": 1.3106335401535034,
      "learning_rate": 2.885714285714286e-05,
      "loss": 2.6325,
      "step": 17970
    },
    {
      "epoch": 2.5685714285714285,
      "grad_norm": 1.6098004579544067,
      "learning_rate": 2.876190476190476e-05,
      "loss": 2.775,
      "step": 17980
    },
    {
      "epoch": 2.57,
      "grad_norm": 1.283992886543274,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 2.5721,
      "step": 17990
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.6916086673736572,
      "learning_rate": 2.857142857142857e-05,
      "loss": 2.6562,
      "step": 18000
    },
    {
      "epoch": 2.572857142857143,
      "grad_norm": 1.126911997795105,
      "learning_rate": 2.8476190476190477e-05,
      "loss": 2.7201,
      "step": 18010
    },
    {
      "epoch": 2.5742857142857143,
      "grad_norm": 0.9604932069778442,
      "learning_rate": 2.838095238095238e-05,
      "loss": 2.5873,
      "step": 18020
    },
    {
      "epoch": 2.5757142857142856,
      "grad_norm": 1.173996090888977,
      "learning_rate": 2.8285714285714287e-05,
      "loss": 2.6853,
      "step": 18030
    },
    {
      "epoch": 2.5771428571428574,
      "grad_norm": 1.1979279518127441,
      "learning_rate": 2.819047619047619e-05,
      "loss": 2.6459,
      "step": 18040
    },
    {
      "epoch": 2.5785714285714287,
      "grad_norm": 1.3179758787155151,
      "learning_rate": 2.8095238095238096e-05,
      "loss": 2.6165,
      "step": 18050
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.9428343772888184,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.6206,
      "step": 18060
    },
    {
      "epoch": 2.5814285714285714,
      "grad_norm": 1.22072172164917,
      "learning_rate": 2.7904761904761906e-05,
      "loss": 2.6445,
      "step": 18070
    },
    {
      "epoch": 2.5828571428571427,
      "grad_norm": 1.4170252084732056,
      "learning_rate": 2.7809523809523813e-05,
      "loss": 2.5075,
      "step": 18080
    },
    {
      "epoch": 2.5842857142857145,
      "grad_norm": 1.1914738416671753,
      "learning_rate": 2.7714285714285716e-05,
      "loss": 2.7566,
      "step": 18090
    },
    {
      "epoch": 2.585714285714286,
      "grad_norm": 1.244956135749817,
      "learning_rate": 2.7619047619047622e-05,
      "loss": 2.5048,
      "step": 18100
    },
    {
      "epoch": 2.587142857142857,
      "grad_norm": 1.1225086450576782,
      "learning_rate": 2.7523809523809525e-05,
      "loss": 2.5219,
      "step": 18110
    },
    {
      "epoch": 2.5885714285714285,
      "grad_norm": 1.0143392086029053,
      "learning_rate": 2.742857142857143e-05,
      "loss": 2.6422,
      "step": 18120
    },
    {
      "epoch": 2.59,
      "grad_norm": 6.639799118041992,
      "learning_rate": 2.733333333333333e-05,
      "loss": 2.6212,
      "step": 18130
    },
    {
      "epoch": 2.5914285714285716,
      "grad_norm": 1.1453344821929932,
      "learning_rate": 2.723809523809524e-05,
      "loss": 2.739,
      "step": 18140
    },
    {
      "epoch": 2.592857142857143,
      "grad_norm": 1.3689996004104614,
      "learning_rate": 2.714285714285714e-05,
      "loss": 2.4741,
      "step": 18150
    },
    {
      "epoch": 2.5942857142857143,
      "grad_norm": 1.3581546545028687,
      "learning_rate": 2.704761904761905e-05,
      "loss": 2.5826,
      "step": 18160
    },
    {
      "epoch": 2.5957142857142856,
      "grad_norm": 1.273679494857788,
      "learning_rate": 2.695238095238095e-05,
      "loss": 2.5828,
      "step": 18170
    },
    {
      "epoch": 2.597142857142857,
      "grad_norm": 1.1368050575256348,
      "learning_rate": 2.6857142857142857e-05,
      "loss": 2.6056,
      "step": 18180
    },
    {
      "epoch": 2.5985714285714288,
      "grad_norm": 1.238416075706482,
      "learning_rate": 2.6761904761904767e-05,
      "loss": 2.5781,
      "step": 18190
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.3823169469833374,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.641,
      "step": 18200
    },
    {
      "epoch": 2.6014285714285714,
      "grad_norm": 1.7044106721878052,
      "learning_rate": 2.6571428571428576e-05,
      "loss": 2.7024,
      "step": 18210
    },
    {
      "epoch": 2.6028571428571428,
      "grad_norm": 1.4462482929229736,
      "learning_rate": 2.6476190476190476e-05,
      "loss": 2.6273,
      "step": 18220
    },
    {
      "epoch": 2.604285714285714,
      "grad_norm": 0.9207512736320496,
      "learning_rate": 2.6380952380952383e-05,
      "loss": 2.591,
      "step": 18230
    },
    {
      "epoch": 2.605714285714286,
      "grad_norm": 1.1981703042984009,
      "learning_rate": 2.6285714285714286e-05,
      "loss": 2.6484,
      "step": 18240
    },
    {
      "epoch": 2.607142857142857,
      "grad_norm": 1.3032628297805786,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 2.7036,
      "step": 18250
    },
    {
      "epoch": 2.6085714285714285,
      "grad_norm": 0.8950863480567932,
      "learning_rate": 2.6095238095238095e-05,
      "loss": 2.5016,
      "step": 18260
    },
    {
      "epoch": 2.61,
      "grad_norm": 1.170090913772583,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 2.7774,
      "step": 18270
    },
    {
      "epoch": 2.611428571428571,
      "grad_norm": 1.173579454421997,
      "learning_rate": 2.5904761904761905e-05,
      "loss": 2.7022,
      "step": 18280
    },
    {
      "epoch": 2.612857142857143,
      "grad_norm": 1.2696022987365723,
      "learning_rate": 2.580952380952381e-05,
      "loss": 2.6648,
      "step": 18290
    },
    {
      "epoch": 2.6142857142857143,
      "grad_norm": 1.1129635572433472,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 2.7394,
      "step": 18300
    },
    {
      "epoch": 2.6157142857142857,
      "grad_norm": 0.982837975025177,
      "learning_rate": 2.561904761904762e-05,
      "loss": 2.6543,
      "step": 18310
    },
    {
      "epoch": 2.617142857142857,
      "grad_norm": 0.91789710521698,
      "learning_rate": 2.5523809523809527e-05,
      "loss": 2.644,
      "step": 18320
    },
    {
      "epoch": 2.6185714285714283,
      "grad_norm": 1.3369017839431763,
      "learning_rate": 2.542857142857143e-05,
      "loss": 2.6396,
      "step": 18330
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.8524092435836792,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 2.7063,
      "step": 18340
    },
    {
      "epoch": 2.6214285714285714,
      "grad_norm": 1.1144930124282837,
      "learning_rate": 2.523809523809524e-05,
      "loss": 2.6955,
      "step": 18350
    },
    {
      "epoch": 2.6228571428571428,
      "grad_norm": 1.3832002878189087,
      "learning_rate": 2.5142857142857147e-05,
      "loss": 2.4933,
      "step": 18360
    },
    {
      "epoch": 2.6242857142857146,
      "grad_norm": 1.4476649761199951,
      "learning_rate": 2.5047619047619046e-05,
      "loss": 2.7671,
      "step": 18370
    },
    {
      "epoch": 2.6257142857142854,
      "grad_norm": 1.2282607555389404,
      "learning_rate": 2.4952380952380956e-05,
      "loss": 2.9267,
      "step": 18380
    },
    {
      "epoch": 2.6271428571428572,
      "grad_norm": 1.3074930906295776,
      "learning_rate": 2.485714285714286e-05,
      "loss": 2.5382,
      "step": 18390
    },
    {
      "epoch": 2.6285714285714286,
      "grad_norm": 1.1964963674545288,
      "learning_rate": 2.4761904761904762e-05,
      "loss": 2.4221,
      "step": 18400
    },
    {
      "epoch": 2.63,
      "grad_norm": 1.178505539894104,
      "learning_rate": 2.466666666666667e-05,
      "loss": 2.3461,
      "step": 18410
    },
    {
      "epoch": 2.6314285714285717,
      "grad_norm": 0.885573148727417,
      "learning_rate": 2.4571428571428572e-05,
      "loss": 2.5803,
      "step": 18420
    },
    {
      "epoch": 2.632857142857143,
      "grad_norm": 1.0345505475997925,
      "learning_rate": 2.447619047619048e-05,
      "loss": 2.3506,
      "step": 18430
    },
    {
      "epoch": 2.6342857142857143,
      "grad_norm": 1.277147650718689,
      "learning_rate": 2.438095238095238e-05,
      "loss": 2.7437,
      "step": 18440
    },
    {
      "epoch": 2.6357142857142857,
      "grad_norm": 0.9875227808952332,
      "learning_rate": 2.4285714285714288e-05,
      "loss": 2.6062,
      "step": 18450
    },
    {
      "epoch": 2.637142857142857,
      "grad_norm": 1.1320980787277222,
      "learning_rate": 2.419047619047619e-05,
      "loss": 2.6344,
      "step": 18460
    },
    {
      "epoch": 2.638571428571429,
      "grad_norm": 1.2039810419082642,
      "learning_rate": 2.4095238095238094e-05,
      "loss": 2.5233,
      "step": 18470
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.2693179845809937,
      "learning_rate": 2.4e-05,
      "loss": 2.5585,
      "step": 18480
    },
    {
      "epoch": 2.6414285714285715,
      "grad_norm": 1.1883487701416016,
      "learning_rate": 2.3904761904761904e-05,
      "loss": 2.6636,
      "step": 18490
    },
    {
      "epoch": 2.642857142857143,
      "grad_norm": 1.1928290128707886,
      "learning_rate": 2.380952380952381e-05,
      "loss": 2.663,
      "step": 18500
    },
    {
      "epoch": 2.644285714285714,
      "grad_norm": 0.9736774563789368,
      "learning_rate": 2.3714285714285717e-05,
      "loss": 2.6534,
      "step": 18510
    },
    {
      "epoch": 2.645714285714286,
      "grad_norm": 1.1982684135437012,
      "learning_rate": 2.361904761904762e-05,
      "loss": 2.6033,
      "step": 18520
    },
    {
      "epoch": 2.6471428571428572,
      "grad_norm": 0.996849536895752,
      "learning_rate": 2.3523809523809526e-05,
      "loss": 2.6693,
      "step": 18530
    },
    {
      "epoch": 2.6485714285714286,
      "grad_norm": 1.3575993776321411,
      "learning_rate": 2.342857142857143e-05,
      "loss": 2.7873,
      "step": 18540
    },
    {
      "epoch": 2.65,
      "grad_norm": 1.382349967956543,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 2.702,
      "step": 18550
    },
    {
      "epoch": 2.6514285714285712,
      "grad_norm": 1.336158275604248,
      "learning_rate": 2.323809523809524e-05,
      "loss": 2.6175,
      "step": 18560
    },
    {
      "epoch": 2.652857142857143,
      "grad_norm": 1.2533516883850098,
      "learning_rate": 2.3142857142857145e-05,
      "loss": 2.6013,
      "step": 18570
    },
    {
      "epoch": 2.6542857142857144,
      "grad_norm": 1.3106669187545776,
      "learning_rate": 2.304761904761905e-05,
      "loss": 2.6483,
      "step": 18580
    },
    {
      "epoch": 2.6557142857142857,
      "grad_norm": 1.4146307706832886,
      "learning_rate": 2.295238095238095e-05,
      "loss": 2.6168,
      "step": 18590
    },
    {
      "epoch": 2.657142857142857,
      "grad_norm": 1.3229188919067383,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 2.5517,
      "step": 18600
    },
    {
      "epoch": 2.6585714285714284,
      "grad_norm": 1.27733314037323,
      "learning_rate": 2.276190476190476e-05,
      "loss": 2.5491,
      "step": 18610
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.9905620813369751,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 2.681,
      "step": 18620
    },
    {
      "epoch": 2.6614285714285715,
      "grad_norm": 1.2324227094650269,
      "learning_rate": 2.257142857142857e-05,
      "loss": 2.5739,
      "step": 18630
    },
    {
      "epoch": 2.662857142857143,
      "grad_norm": 1.1848483085632324,
      "learning_rate": 2.2476190476190477e-05,
      "loss": 2.6552,
      "step": 18640
    },
    {
      "epoch": 2.664285714285714,
      "grad_norm": 1.0081703662872314,
      "learning_rate": 2.2380952380952384e-05,
      "loss": 2.6064,
      "step": 18650
    },
    {
      "epoch": 2.6657142857142855,
      "grad_norm": 1.0824428796768188,
      "learning_rate": 2.2285714285714287e-05,
      "loss": 2.6851,
      "step": 18660
    },
    {
      "epoch": 2.6671428571428573,
      "grad_norm": 1.3916165828704834,
      "learning_rate": 2.2190476190476193e-05,
      "loss": 2.5414,
      "step": 18670
    },
    {
      "epoch": 2.6685714285714286,
      "grad_norm": 1.3467785120010376,
      "learning_rate": 2.2095238095238096e-05,
      "loss": 2.5665,
      "step": 18680
    },
    {
      "epoch": 2.67,
      "grad_norm": 1.6766772270202637,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.6483,
      "step": 18690
    },
    {
      "epoch": 2.6714285714285713,
      "grad_norm": 0.9888867735862732,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 2.6297,
      "step": 18700
    },
    {
      "epoch": 2.6728571428571426,
      "grad_norm": 1.2226660251617432,
      "learning_rate": 2.180952380952381e-05,
      "loss": 2.7081,
      "step": 18710
    },
    {
      "epoch": 2.6742857142857144,
      "grad_norm": 1.2246923446655273,
      "learning_rate": 2.1714285714285715e-05,
      "loss": 2.3433,
      "step": 18720
    },
    {
      "epoch": 2.6757142857142857,
      "grad_norm": 1.2212615013122559,
      "learning_rate": 2.161904761904762e-05,
      "loss": 2.6554,
      "step": 18730
    },
    {
      "epoch": 2.677142857142857,
      "grad_norm": 1.2452175617218018,
      "learning_rate": 2.1523809523809525e-05,
      "loss": 2.4236,
      "step": 18740
    },
    {
      "epoch": 2.678571428571429,
      "grad_norm": 1.5978150367736816,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 2.6784,
      "step": 18750
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.3281069993972778,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 2.6982,
      "step": 18760
    },
    {
      "epoch": 2.6814285714285715,
      "grad_norm": 1.0744956731796265,
      "learning_rate": 2.123809523809524e-05,
      "loss": 2.5744,
      "step": 18770
    },
    {
      "epoch": 2.682857142857143,
      "grad_norm": 1.3830267190933228,
      "learning_rate": 2.1142857142857144e-05,
      "loss": 2.6267,
      "step": 18780
    },
    {
      "epoch": 2.684285714285714,
      "grad_norm": 1.8677849769592285,
      "learning_rate": 2.104761904761905e-05,
      "loss": 2.5518,
      "step": 18790
    },
    {
      "epoch": 2.685714285714286,
      "grad_norm": 1.74461829662323,
      "learning_rate": 2.0952380952380954e-05,
      "loss": 2.5855,
      "step": 18800
    },
    {
      "epoch": 2.6871428571428573,
      "grad_norm": 1.7915403842926025,
      "learning_rate": 2.0857142857142857e-05,
      "loss": 2.4943,
      "step": 18810
    },
    {
      "epoch": 2.6885714285714286,
      "grad_norm": 1.2367950677871704,
      "learning_rate": 2.0761904761904763e-05,
      "loss": 2.5858,
      "step": 18820
    },
    {
      "epoch": 2.69,
      "grad_norm": 1.2034505605697632,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 2.6833,
      "step": 18830
    },
    {
      "epoch": 2.6914285714285713,
      "grad_norm": 1.2034703493118286,
      "learning_rate": 2.0571428571428573e-05,
      "loss": 2.6019,
      "step": 18840
    },
    {
      "epoch": 2.692857142857143,
      "grad_norm": 1.2973605394363403,
      "learning_rate": 2.0476190476190476e-05,
      "loss": 2.4754,
      "step": 18850
    },
    {
      "epoch": 2.6942857142857144,
      "grad_norm": 1.528882622718811,
      "learning_rate": 2.0380952380952382e-05,
      "loss": 2.5688,
      "step": 18860
    },
    {
      "epoch": 2.6957142857142857,
      "grad_norm": 1.2024235725402832,
      "learning_rate": 2.0285714285714286e-05,
      "loss": 2.6403,
      "step": 18870
    },
    {
      "epoch": 2.697142857142857,
      "grad_norm": 1.0010310411453247,
      "learning_rate": 2.019047619047619e-05,
      "loss": 2.6436,
      "step": 18880
    },
    {
      "epoch": 2.6985714285714284,
      "grad_norm": 0.9021174907684326,
      "learning_rate": 2.00952380952381e-05,
      "loss": 2.4663,
      "step": 18890
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.0544731616973877,
      "learning_rate": 2e-05,
      "loss": 2.7351,
      "step": 18900
    },
    {
      "epoch": 2.7014285714285715,
      "grad_norm": 1.294443130493164,
      "learning_rate": 1.9904761904761908e-05,
      "loss": 2.5649,
      "step": 18910
    },
    {
      "epoch": 2.702857142857143,
      "grad_norm": 1.1957358121871948,
      "learning_rate": 1.980952380952381e-05,
      "loss": 2.6773,
      "step": 18920
    },
    {
      "epoch": 2.704285714285714,
      "grad_norm": 1.4136607646942139,
      "learning_rate": 1.9714285714285714e-05,
      "loss": 2.7285,
      "step": 18930
    },
    {
      "epoch": 2.7057142857142855,
      "grad_norm": 1.301154613494873,
      "learning_rate": 1.961904761904762e-05,
      "loss": 2.5177,
      "step": 18940
    },
    {
      "epoch": 2.7071428571428573,
      "grad_norm": 1.0747421979904175,
      "learning_rate": 1.9523809523809524e-05,
      "loss": 2.8099,
      "step": 18950
    },
    {
      "epoch": 2.7085714285714286,
      "grad_norm": 1.3220980167388916,
      "learning_rate": 1.942857142857143e-05,
      "loss": 2.6195,
      "step": 18960
    },
    {
      "epoch": 2.71,
      "grad_norm": 1.2730807065963745,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 2.6482,
      "step": 18970
    },
    {
      "epoch": 2.7114285714285713,
      "grad_norm": 1.1171258687973022,
      "learning_rate": 1.923809523809524e-05,
      "loss": 2.5108,
      "step": 18980
    },
    {
      "epoch": 2.7128571428571426,
      "grad_norm": 1.1736476421356201,
      "learning_rate": 1.9142857142857143e-05,
      "loss": 2.4595,
      "step": 18990
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 1.1524380445480347,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 2.6853,
      "step": 19000
    },
    {
      "epoch": 2.7157142857142857,
      "grad_norm": 1.0303339958190918,
      "learning_rate": 1.8952380952380953e-05,
      "loss": 2.5977,
      "step": 19010
    },
    {
      "epoch": 2.717142857142857,
      "grad_norm": 1.2704477310180664,
      "learning_rate": 1.885714285714286e-05,
      "loss": 2.56,
      "step": 19020
    },
    {
      "epoch": 2.7185714285714284,
      "grad_norm": 1.0587499141693115,
      "learning_rate": 1.8761904761904766e-05,
      "loss": 2.7355,
      "step": 19030
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.0873247385025024,
      "learning_rate": 1.866666666666667e-05,
      "loss": 2.6944,
      "step": 19040
    },
    {
      "epoch": 2.7214285714285715,
      "grad_norm": 1.103520154953003,
      "learning_rate": 1.8571428571428572e-05,
      "loss": 2.7429,
      "step": 19050
    },
    {
      "epoch": 2.722857142857143,
      "grad_norm": 1.2055048942565918,
      "learning_rate": 1.8476190476190478e-05,
      "loss": 2.5713,
      "step": 19060
    },
    {
      "epoch": 2.724285714285714,
      "grad_norm": 1.2496039867401123,
      "learning_rate": 1.838095238095238e-05,
      "loss": 2.5598,
      "step": 19070
    },
    {
      "epoch": 2.725714285714286,
      "grad_norm": 1.1656954288482666,
      "learning_rate": 1.8285714285714288e-05,
      "loss": 2.5833,
      "step": 19080
    },
    {
      "epoch": 2.727142857142857,
      "grad_norm": 1.6844701766967773,
      "learning_rate": 1.819047619047619e-05,
      "loss": 2.7173,
      "step": 19090
    },
    {
      "epoch": 2.7285714285714286,
      "grad_norm": 1.0418505668640137,
      "learning_rate": 1.8095238095238094e-05,
      "loss": 2.5643,
      "step": 19100
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.1375499963760376,
      "learning_rate": 1.8e-05,
      "loss": 2.5565,
      "step": 19110
    },
    {
      "epoch": 2.7314285714285713,
      "grad_norm": 1.7277535200119019,
      "learning_rate": 1.7904761904761904e-05,
      "loss": 2.5974,
      "step": 19120
    },
    {
      "epoch": 2.732857142857143,
      "grad_norm": 1.6424609422683716,
      "learning_rate": 1.780952380952381e-05,
      "loss": 2.6454,
      "step": 19130
    },
    {
      "epoch": 2.7342857142857144,
      "grad_norm": 1.2666419744491577,
      "learning_rate": 1.7714285714285713e-05,
      "loss": 2.6833,
      "step": 19140
    },
    {
      "epoch": 2.7357142857142858,
      "grad_norm": 1.3460196256637573,
      "learning_rate": 1.761904761904762e-05,
      "loss": 2.5404,
      "step": 19150
    },
    {
      "epoch": 2.737142857142857,
      "grad_norm": 1.016433835029602,
      "learning_rate": 1.7523809523809526e-05,
      "loss": 2.5734,
      "step": 19160
    },
    {
      "epoch": 2.7385714285714284,
      "grad_norm": 1.2966960668563843,
      "learning_rate": 1.742857142857143e-05,
      "loss": 2.5688,
      "step": 19170
    },
    {
      "epoch": 2.74,
      "grad_norm": 1.1473811864852905,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 2.6093,
      "step": 19180
    },
    {
      "epoch": 2.7414285714285715,
      "grad_norm": 1.1957676410675049,
      "learning_rate": 1.723809523809524e-05,
      "loss": 2.6424,
      "step": 19190
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 1.2185050249099731,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 2.534,
      "step": 19200
    },
    {
      "epoch": 2.744285714285714,
      "grad_norm": 1.254615068435669,
      "learning_rate": 1.704761904761905e-05,
      "loss": 2.7269,
      "step": 19210
    },
    {
      "epoch": 2.7457142857142856,
      "grad_norm": 0.9627693295478821,
      "learning_rate": 1.695238095238095e-05,
      "loss": 2.6779,
      "step": 19220
    },
    {
      "epoch": 2.7471428571428573,
      "grad_norm": 1.0839958190917969,
      "learning_rate": 1.6857142857142858e-05,
      "loss": 2.3972,
      "step": 19230
    },
    {
      "epoch": 2.7485714285714287,
      "grad_norm": 1.3911809921264648,
      "learning_rate": 1.676190476190476e-05,
      "loss": 2.7338,
      "step": 19240
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.0100241899490356,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 2.6399,
      "step": 19250
    },
    {
      "epoch": 2.7514285714285713,
      "grad_norm": 1.168347716331482,
      "learning_rate": 1.657142857142857e-05,
      "loss": 2.6654,
      "step": 19260
    },
    {
      "epoch": 2.7528571428571427,
      "grad_norm": 1.7298247814178467,
      "learning_rate": 1.6476190476190477e-05,
      "loss": 2.7455,
      "step": 19270
    },
    {
      "epoch": 2.7542857142857144,
      "grad_norm": 1.1710848808288574,
      "learning_rate": 1.6380952380952384e-05,
      "loss": 2.6261,
      "step": 19280
    },
    {
      "epoch": 2.755714285714286,
      "grad_norm": 1.1865487098693848,
      "learning_rate": 1.6285714285714287e-05,
      "loss": 2.6132,
      "step": 19290
    },
    {
      "epoch": 2.757142857142857,
      "grad_norm": 1.5394102334976196,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 2.4791,
      "step": 19300
    },
    {
      "epoch": 2.7585714285714285,
      "grad_norm": 0.9978004097938538,
      "learning_rate": 1.6095238095238096e-05,
      "loss": 2.6984,
      "step": 19310
    },
    {
      "epoch": 2.76,
      "grad_norm": 1.1839479207992554,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.6153,
      "step": 19320
    },
    {
      "epoch": 2.7614285714285716,
      "grad_norm": 1.4613436460494995,
      "learning_rate": 1.5904761904761906e-05,
      "loss": 2.6488,
      "step": 19330
    },
    {
      "epoch": 2.762857142857143,
      "grad_norm": 1.2994641065597534,
      "learning_rate": 1.580952380952381e-05,
      "loss": 2.6645,
      "step": 19340
    },
    {
      "epoch": 2.7642857142857142,
      "grad_norm": 0.9384481310844421,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 2.4621,
      "step": 19350
    },
    {
      "epoch": 2.7657142857142856,
      "grad_norm": 1.234329104423523,
      "learning_rate": 1.561904761904762e-05,
      "loss": 2.7882,
      "step": 19360
    },
    {
      "epoch": 2.767142857142857,
      "grad_norm": 0.988035261631012,
      "learning_rate": 1.5523809523809525e-05,
      "loss": 2.77,
      "step": 19370
    },
    {
      "epoch": 2.7685714285714287,
      "grad_norm": 1.2923579216003418,
      "learning_rate": 1.5428571428571428e-05,
      "loss": 2.5808,
      "step": 19380
    },
    {
      "epoch": 2.77,
      "grad_norm": 1.1990610361099243,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 2.5886,
      "step": 19390
    },
    {
      "epoch": 2.7714285714285714,
      "grad_norm": 1.125968337059021,
      "learning_rate": 1.5238095238095241e-05,
      "loss": 2.5757,
      "step": 19400
    },
    {
      "epoch": 2.772857142857143,
      "grad_norm": 1.7446433305740356,
      "learning_rate": 1.5142857142857144e-05,
      "loss": 2.563,
      "step": 19410
    },
    {
      "epoch": 2.774285714285714,
      "grad_norm": 0.997142493724823,
      "learning_rate": 1.5047619047619049e-05,
      "loss": 2.6525,
      "step": 19420
    },
    {
      "epoch": 2.775714285714286,
      "grad_norm": 0.9913669228553772,
      "learning_rate": 1.4952380952380954e-05,
      "loss": 2.5075,
      "step": 19430
    },
    {
      "epoch": 2.777142857142857,
      "grad_norm": 1.277876853942871,
      "learning_rate": 1.4857142857142858e-05,
      "loss": 2.5847,
      "step": 19440
    },
    {
      "epoch": 2.7785714285714285,
      "grad_norm": 1.1371688842773438,
      "learning_rate": 1.4761904761904763e-05,
      "loss": 2.5487,
      "step": 19450
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 1.3459867238998413,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 2.4424,
      "step": 19460
    },
    {
      "epoch": 2.7814285714285716,
      "grad_norm": 1.1464170217514038,
      "learning_rate": 1.4571428571428573e-05,
      "loss": 2.5394,
      "step": 19470
    },
    {
      "epoch": 2.782857142857143,
      "grad_norm": 1.148195743560791,
      "learning_rate": 1.4476190476190476e-05,
      "loss": 2.6364,
      "step": 19480
    },
    {
      "epoch": 2.7842857142857143,
      "grad_norm": 1.0707229375839233,
      "learning_rate": 1.438095238095238e-05,
      "loss": 2.7448,
      "step": 19490
    },
    {
      "epoch": 2.7857142857142856,
      "grad_norm": 0.8404738306999207,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 2.5887,
      "step": 19500
    },
    {
      "epoch": 2.7871428571428574,
      "grad_norm": 0.9298263192176819,
      "learning_rate": 1.419047619047619e-05,
      "loss": 2.6366,
      "step": 19510
    },
    {
      "epoch": 2.7885714285714287,
      "grad_norm": 1.111579418182373,
      "learning_rate": 1.4095238095238095e-05,
      "loss": 2.6858,
      "step": 19520
    },
    {
      "epoch": 2.79,
      "grad_norm": 1.2239640951156616,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.6702,
      "step": 19530
    },
    {
      "epoch": 2.7914285714285714,
      "grad_norm": 1.1270016431808472,
      "learning_rate": 1.3904761904761906e-05,
      "loss": 2.6756,
      "step": 19540
    },
    {
      "epoch": 2.7928571428571427,
      "grad_norm": 1.2019932270050049,
      "learning_rate": 1.3809523809523811e-05,
      "loss": 2.5082,
      "step": 19550
    },
    {
      "epoch": 2.7942857142857145,
      "grad_norm": 1.3138960599899292,
      "learning_rate": 1.3714285714285716e-05,
      "loss": 2.4957,
      "step": 19560
    },
    {
      "epoch": 2.795714285714286,
      "grad_norm": 0.8575652241706848,
      "learning_rate": 1.361904761904762e-05,
      "loss": 2.7101,
      "step": 19570
    },
    {
      "epoch": 2.797142857142857,
      "grad_norm": 1.0684236288070679,
      "learning_rate": 1.3523809523809525e-05,
      "loss": 2.6429,
      "step": 19580
    },
    {
      "epoch": 2.7985714285714285,
      "grad_norm": 1.1314276456832886,
      "learning_rate": 1.3428571428571429e-05,
      "loss": 2.653,
      "step": 19590
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.2637358903884888,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.649,
      "step": 19600
    },
    {
      "epoch": 2.8014285714285716,
      "grad_norm": 1.3043208122253418,
      "learning_rate": 1.3238095238095238e-05,
      "loss": 2.6264,
      "step": 19610
    },
    {
      "epoch": 2.802857142857143,
      "grad_norm": 0.9156351089477539,
      "learning_rate": 1.3142857142857143e-05,
      "loss": 2.5956,
      "step": 19620
    },
    {
      "epoch": 2.8042857142857143,
      "grad_norm": 0.9881131052970886,
      "learning_rate": 1.3047619047619048e-05,
      "loss": 2.6773,
      "step": 19630
    },
    {
      "epoch": 2.8057142857142856,
      "grad_norm": 1.281296730041504,
      "learning_rate": 1.2952380952380952e-05,
      "loss": 2.682,
      "step": 19640
    },
    {
      "epoch": 2.807142857142857,
      "grad_norm": 1.219869613647461,
      "learning_rate": 1.2857142857142857e-05,
      "loss": 2.4764,
      "step": 19650
    },
    {
      "epoch": 2.8085714285714287,
      "grad_norm": 1.038651943206787,
      "learning_rate": 1.2761904761904764e-05,
      "loss": 2.7797,
      "step": 19660
    },
    {
      "epoch": 2.81,
      "grad_norm": 0.9219942092895508,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 2.5688,
      "step": 19670
    },
    {
      "epoch": 2.8114285714285714,
      "grad_norm": 1.1617012023925781,
      "learning_rate": 1.2571428571428573e-05,
      "loss": 2.5923,
      "step": 19680
    },
    {
      "epoch": 2.8128571428571427,
      "grad_norm": 1.114499568939209,
      "learning_rate": 1.2476190476190478e-05,
      "loss": 2.6744,
      "step": 19690
    },
    {
      "epoch": 2.814285714285714,
      "grad_norm": 1.0623409748077393,
      "learning_rate": 1.2380952380952381e-05,
      "loss": 2.593,
      "step": 19700
    },
    {
      "epoch": 2.815714285714286,
      "grad_norm": 1.327218770980835,
      "learning_rate": 1.2285714285714286e-05,
      "loss": 2.6915,
      "step": 19710
    },
    {
      "epoch": 2.817142857142857,
      "grad_norm": 1.193121314048767,
      "learning_rate": 1.219047619047619e-05,
      "loss": 2.6344,
      "step": 19720
    },
    {
      "epoch": 2.8185714285714285,
      "grad_norm": 1.3886268138885498,
      "learning_rate": 1.2095238095238096e-05,
      "loss": 2.827,
      "step": 19730
    },
    {
      "epoch": 2.82,
      "grad_norm": 1.3542633056640625,
      "learning_rate": 1.2e-05,
      "loss": 2.622,
      "step": 19740
    },
    {
      "epoch": 2.821428571428571,
      "grad_norm": 1.3765140771865845,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 2.5248,
      "step": 19750
    },
    {
      "epoch": 2.822857142857143,
      "grad_norm": 1.044060468673706,
      "learning_rate": 1.180952380952381e-05,
      "loss": 2.6521,
      "step": 19760
    },
    {
      "epoch": 2.8242857142857143,
      "grad_norm": 1.3040847778320312,
      "learning_rate": 1.1714285714285715e-05,
      "loss": 2.5174,
      "step": 19770
    },
    {
      "epoch": 2.8257142857142856,
      "grad_norm": 1.0169398784637451,
      "learning_rate": 1.161904761904762e-05,
      "loss": 2.9239,
      "step": 19780
    },
    {
      "epoch": 2.8271428571428574,
      "grad_norm": 0.9499984383583069,
      "learning_rate": 1.1523809523809524e-05,
      "loss": 2.5782,
      "step": 19790
    },
    {
      "epoch": 2.8285714285714287,
      "grad_norm": 1.0082197189331055,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 2.6491,
      "step": 19800
    },
    {
      "epoch": 2.83,
      "grad_norm": 1.259599208831787,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 2.6346,
      "step": 19810
    },
    {
      "epoch": 2.8314285714285714,
      "grad_norm": 1.1113455295562744,
      "learning_rate": 1.1238095238095239e-05,
      "loss": 2.6456,
      "step": 19820
    },
    {
      "epoch": 2.8328571428571427,
      "grad_norm": 1.160475254058838,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 2.5144,
      "step": 19830
    },
    {
      "epoch": 2.8342857142857145,
      "grad_norm": 1.043488621711731,
      "learning_rate": 1.1047619047619048e-05,
      "loss": 2.7078,
      "step": 19840
    },
    {
      "epoch": 2.835714285714286,
      "grad_norm": 1.252824306488037,
      "learning_rate": 1.0952380952380953e-05,
      "loss": 2.692,
      "step": 19850
    },
    {
      "epoch": 2.837142857142857,
      "grad_norm": 1.6237990856170654,
      "learning_rate": 1.0857142857142858e-05,
      "loss": 2.6523,
      "step": 19860
    },
    {
      "epoch": 2.8385714285714285,
      "grad_norm": 1.178195595741272,
      "learning_rate": 1.0761904761904763e-05,
      "loss": 2.6748,
      "step": 19870
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.5015778541564941,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 2.556,
      "step": 19880
    },
    {
      "epoch": 2.8414285714285716,
      "grad_norm": 1.1072036027908325,
      "learning_rate": 1.0571428571428572e-05,
      "loss": 2.7038,
      "step": 19890
    },
    {
      "epoch": 2.842857142857143,
      "grad_norm": 1.0928984880447388,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 2.5396,
      "step": 19900
    },
    {
      "epoch": 2.8442857142857143,
      "grad_norm": 1.1803621053695679,
      "learning_rate": 1.0380952380952382e-05,
      "loss": 2.7298,
      "step": 19910
    },
    {
      "epoch": 2.8457142857142856,
      "grad_norm": 1.6132413148880005,
      "learning_rate": 1.0285714285714286e-05,
      "loss": 2.6315,
      "step": 19920
    },
    {
      "epoch": 2.847142857142857,
      "grad_norm": 1.2401182651519775,
      "learning_rate": 1.0190476190476191e-05,
      "loss": 2.4891,
      "step": 19930
    },
    {
      "epoch": 2.8485714285714288,
      "grad_norm": 1.3675593137741089,
      "learning_rate": 1.0095238095238094e-05,
      "loss": 2.6644,
      "step": 19940
    },
    {
      "epoch": 2.85,
      "grad_norm": 1.0221599340438843,
      "learning_rate": 1e-05,
      "loss": 2.5776,
      "step": 19950
    },
    {
      "epoch": 2.8514285714285714,
      "grad_norm": 1.1461663246154785,
      "learning_rate": 9.904761904761906e-06,
      "loss": 2.489,
      "step": 19960
    },
    {
      "epoch": 2.8528571428571428,
      "grad_norm": 1.0670051574707031,
      "learning_rate": 9.80952380952381e-06,
      "loss": 2.658,
      "step": 19970
    },
    {
      "epoch": 2.854285714285714,
      "grad_norm": 0.921972393989563,
      "learning_rate": 9.714285714285715e-06,
      "loss": 2.6011,
      "step": 19980
    },
    {
      "epoch": 2.855714285714286,
      "grad_norm": 1.4625134468078613,
      "learning_rate": 9.61904761904762e-06,
      "loss": 2.6618,
      "step": 19990
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.9806492924690247,
      "learning_rate": 9.523809523809523e-06,
      "loss": 2.6632,
      "step": 20000
    },
    {
      "epoch": 2.8585714285714285,
      "grad_norm": 1.2416656017303467,
      "learning_rate": 9.42857142857143e-06,
      "loss": 2.7444,
      "step": 20010
    },
    {
      "epoch": 2.86,
      "grad_norm": 1.2530620098114014,
      "learning_rate": 9.333333333333334e-06,
      "loss": 2.5444,
      "step": 20020
    },
    {
      "epoch": 2.861428571428571,
      "grad_norm": 0.9576514959335327,
      "learning_rate": 9.238095238095239e-06,
      "loss": 2.4619,
      "step": 20030
    },
    {
      "epoch": 2.862857142857143,
      "grad_norm": 0.94756019115448,
      "learning_rate": 9.142857142857144e-06,
      "loss": 2.6315,
      "step": 20040
    },
    {
      "epoch": 2.8642857142857143,
      "grad_norm": 1.2349052429199219,
      "learning_rate": 9.047619047619047e-06,
      "loss": 2.5528,
      "step": 20050
    },
    {
      "epoch": 2.8657142857142857,
      "grad_norm": 1.396850824356079,
      "learning_rate": 8.952380952380952e-06,
      "loss": 2.6118,
      "step": 20060
    },
    {
      "epoch": 2.867142857142857,
      "grad_norm": 1.2815470695495605,
      "learning_rate": 8.857142857142857e-06,
      "loss": 2.7197,
      "step": 20070
    },
    {
      "epoch": 2.8685714285714283,
      "grad_norm": 1.1897135972976685,
      "learning_rate": 8.761904761904763e-06,
      "loss": 2.8697,
      "step": 20080
    },
    {
      "epoch": 2.87,
      "grad_norm": 1.4337637424468994,
      "learning_rate": 8.666666666666668e-06,
      "loss": 2.5216,
      "step": 20090
    },
    {
      "epoch": 2.8714285714285714,
      "grad_norm": 1.21825110912323,
      "learning_rate": 8.571428571428573e-06,
      "loss": 2.5926,
      "step": 20100
    },
    {
      "epoch": 2.8728571428571428,
      "grad_norm": 1.0512875318527222,
      "learning_rate": 8.476190476190476e-06,
      "loss": 2.6519,
      "step": 20110
    },
    {
      "epoch": 2.8742857142857146,
      "grad_norm": 1.0568609237670898,
      "learning_rate": 8.38095238095238e-06,
      "loss": 2.677,
      "step": 20120
    },
    {
      "epoch": 2.8757142857142854,
      "grad_norm": 1.2431113719940186,
      "learning_rate": 8.285714285714285e-06,
      "loss": 2.617,
      "step": 20130
    },
    {
      "epoch": 2.8771428571428572,
      "grad_norm": 1.3246315717697144,
      "learning_rate": 8.190476190476192e-06,
      "loss": 2.5358,
      "step": 20140
    },
    {
      "epoch": 2.8785714285714286,
      "grad_norm": 1.2845408916473389,
      "learning_rate": 8.095238095238097e-06,
      "loss": 2.6277,
      "step": 20150
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.2216541767120361,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.7191,
      "step": 20160
    },
    {
      "epoch": 2.8814285714285717,
      "grad_norm": 1.5496808290481567,
      "learning_rate": 7.904761904761904e-06,
      "loss": 2.824,
      "step": 20170
    },
    {
      "epoch": 2.882857142857143,
      "grad_norm": 1.5965899229049683,
      "learning_rate": 7.80952380952381e-06,
      "loss": 2.5813,
      "step": 20180
    },
    {
      "epoch": 2.8842857142857143,
      "grad_norm": 1.4389991760253906,
      "learning_rate": 7.714285714285714e-06,
      "loss": 2.66,
      "step": 20190
    },
    {
      "epoch": 2.8857142857142857,
      "grad_norm": 1.4965389966964722,
      "learning_rate": 7.6190476190476205e-06,
      "loss": 2.5786,
      "step": 20200
    },
    {
      "epoch": 2.887142857142857,
      "grad_norm": 0.8754857182502747,
      "learning_rate": 7.523809523809524e-06,
      "loss": 2.7753,
      "step": 20210
    },
    {
      "epoch": 2.888571428571429,
      "grad_norm": 1.051683783531189,
      "learning_rate": 7.428571428571429e-06,
      "loss": 2.5569,
      "step": 20220
    },
    {
      "epoch": 2.89,
      "grad_norm": 0.9531000852584839,
      "learning_rate": 7.333333333333334e-06,
      "loss": 2.6754,
      "step": 20230
    },
    {
      "epoch": 2.8914285714285715,
      "grad_norm": 1.12361741065979,
      "learning_rate": 7.238095238095238e-06,
      "loss": 2.6658,
      "step": 20240
    },
    {
      "epoch": 2.892857142857143,
      "grad_norm": 1.209775447845459,
      "learning_rate": 7.142857142857143e-06,
      "loss": 2.6334,
      "step": 20250
    },
    {
      "epoch": 2.894285714285714,
      "grad_norm": 1.4126684665679932,
      "learning_rate": 7.0476190476190475e-06,
      "loss": 2.676,
      "step": 20260
    },
    {
      "epoch": 2.895714285714286,
      "grad_norm": 1.0660313367843628,
      "learning_rate": 6.952380952380953e-06,
      "loss": 2.5898,
      "step": 20270
    },
    {
      "epoch": 2.8971428571428572,
      "grad_norm": 0.9098018407821655,
      "learning_rate": 6.857142857142858e-06,
      "loss": 2.6791,
      "step": 20280
    },
    {
      "epoch": 2.8985714285714286,
      "grad_norm": 0.9399306774139404,
      "learning_rate": 6.761904761904763e-06,
      "loss": 2.6351,
      "step": 20290
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.877568006515503,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.724,
      "step": 20300
    },
    {
      "epoch": 2.9014285714285712,
      "grad_norm": 1.175238013267517,
      "learning_rate": 6.5714285714285714e-06,
      "loss": 2.6234,
      "step": 20310
    },
    {
      "epoch": 2.902857142857143,
      "grad_norm": 1.1817734241485596,
      "learning_rate": 6.476190476190476e-06,
      "loss": 2.6217,
      "step": 20320
    },
    {
      "epoch": 2.9042857142857144,
      "grad_norm": 1.1820257902145386,
      "learning_rate": 6.380952380952382e-06,
      "loss": 2.5927,
      "step": 20330
    },
    {
      "epoch": 2.9057142857142857,
      "grad_norm": 1.2446147203445435,
      "learning_rate": 6.285714285714287e-06,
      "loss": 2.4848,
      "step": 20340
    },
    {
      "epoch": 2.907142857142857,
      "grad_norm": 1.2024058103561401,
      "learning_rate": 6.190476190476191e-06,
      "loss": 2.6616,
      "step": 20350
    },
    {
      "epoch": 2.9085714285714284,
      "grad_norm": 1.1791574954986572,
      "learning_rate": 6.095238095238095e-06,
      "loss": 2.7721,
      "step": 20360
    },
    {
      "epoch": 2.91,
      "grad_norm": 1.2532473802566528,
      "learning_rate": 6e-06,
      "loss": 2.6613,
      "step": 20370
    },
    {
      "epoch": 2.9114285714285715,
      "grad_norm": 0.9856494069099426,
      "learning_rate": 5.904761904761905e-06,
      "loss": 2.5403,
      "step": 20380
    },
    {
      "epoch": 2.912857142857143,
      "grad_norm": 1.042924165725708,
      "learning_rate": 5.80952380952381e-06,
      "loss": 2.5098,
      "step": 20390
    },
    {
      "epoch": 2.914285714285714,
      "grad_norm": 1.325691819190979,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 2.6549,
      "step": 20400
    },
    {
      "epoch": 2.9157142857142855,
      "grad_norm": 1.2061289548873901,
      "learning_rate": 5.619047619047619e-06,
      "loss": 2.6893,
      "step": 20410
    },
    {
      "epoch": 2.9171428571428573,
      "grad_norm": 0.9443087577819824,
      "learning_rate": 5.523809523809524e-06,
      "loss": 2.6711,
      "step": 20420
    },
    {
      "epoch": 2.9185714285714286,
      "grad_norm": 1.4248311519622803,
      "learning_rate": 5.428571428571429e-06,
      "loss": 2.6162,
      "step": 20430
    },
    {
      "epoch": 2.92,
      "grad_norm": 1.68243408203125,
      "learning_rate": 5.333333333333334e-06,
      "loss": 2.6636,
      "step": 20440
    },
    {
      "epoch": 2.9214285714285713,
      "grad_norm": 1.6380759477615356,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 2.603,
      "step": 20450
    },
    {
      "epoch": 2.9228571428571426,
      "grad_norm": 1.2997092008590698,
      "learning_rate": 5.142857142857143e-06,
      "loss": 2.6711,
      "step": 20460
    },
    {
      "epoch": 2.9242857142857144,
      "grad_norm": 1.3710843324661255,
      "learning_rate": 5.047619047619047e-06,
      "loss": 2.4471,
      "step": 20470
    },
    {
      "epoch": 2.9257142857142857,
      "grad_norm": 1.4197795391082764,
      "learning_rate": 4.952380952380953e-06,
      "loss": 2.6137,
      "step": 20480
    },
    {
      "epoch": 2.927142857142857,
      "grad_norm": 1.105798602104187,
      "learning_rate": 4.857142857142858e-06,
      "loss": 2.5758,
      "step": 20490
    },
    {
      "epoch": 2.928571428571429,
      "grad_norm": 1.427938461303711,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 2.5845,
      "step": 20500
    },
    {
      "epoch": 2.93,
      "grad_norm": 1.0392473936080933,
      "learning_rate": 4.666666666666667e-06,
      "loss": 2.7331,
      "step": 20510
    },
    {
      "epoch": 2.9314285714285715,
      "grad_norm": 1.230602741241455,
      "learning_rate": 4.571428571428572e-06,
      "loss": 2.6182,
      "step": 20520
    },
    {
      "epoch": 2.932857142857143,
      "grad_norm": 1.0387016534805298,
      "learning_rate": 4.476190476190476e-06,
      "loss": 2.6687,
      "step": 20530
    },
    {
      "epoch": 2.934285714285714,
      "grad_norm": 1.3883056640625,
      "learning_rate": 4.3809523809523815e-06,
      "loss": 2.5078,
      "step": 20540
    },
    {
      "epoch": 2.935714285714286,
      "grad_norm": 1.070706844329834,
      "learning_rate": 4.285714285714286e-06,
      "loss": 2.7267,
      "step": 20550
    },
    {
      "epoch": 2.9371428571428573,
      "grad_norm": 1.235435962677002,
      "learning_rate": 4.19047619047619e-06,
      "loss": 2.6682,
      "step": 20560
    },
    {
      "epoch": 2.9385714285714286,
      "grad_norm": 1.1254327297210693,
      "learning_rate": 4.095238095238096e-06,
      "loss": 2.4762,
      "step": 20570
    },
    {
      "epoch": 2.94,
      "grad_norm": 1.2040354013442993,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.6967,
      "step": 20580
    },
    {
      "epoch": 2.9414285714285713,
      "grad_norm": 1.6899827718734741,
      "learning_rate": 3.904761904761905e-06,
      "loss": 2.6637,
      "step": 20590
    },
    {
      "epoch": 2.942857142857143,
      "grad_norm": 1.0946413278579712,
      "learning_rate": 3.8095238095238102e-06,
      "loss": 2.6589,
      "step": 20600
    },
    {
      "epoch": 2.9442857142857144,
      "grad_norm": 1.0939103364944458,
      "learning_rate": 3.7142857142857146e-06,
      "loss": 2.626,
      "step": 20610
    },
    {
      "epoch": 2.9457142857142857,
      "grad_norm": 0.9807326197624207,
      "learning_rate": 3.619047619047619e-06,
      "loss": 2.6687,
      "step": 20620
    },
    {
      "epoch": 2.947142857142857,
      "grad_norm": 1.633365273475647,
      "learning_rate": 3.5238095238095238e-06,
      "loss": 2.6935,
      "step": 20630
    },
    {
      "epoch": 2.9485714285714284,
      "grad_norm": 1.0408508777618408,
      "learning_rate": 3.428571428571429e-06,
      "loss": 2.588,
      "step": 20640
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.1154160499572754,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 2.7051,
      "step": 20650
    },
    {
      "epoch": 2.9514285714285715,
      "grad_norm": 1.3683751821517944,
      "learning_rate": 3.238095238095238e-06,
      "loss": 2.6217,
      "step": 20660
    },
    {
      "epoch": 2.952857142857143,
      "grad_norm": 1.0371299982070923,
      "learning_rate": 3.1428571428571433e-06,
      "loss": 2.6063,
      "step": 20670
    },
    {
      "epoch": 2.954285714285714,
      "grad_norm": 1.4230233430862427,
      "learning_rate": 3.0476190476190477e-06,
      "loss": 2.7093,
      "step": 20680
    },
    {
      "epoch": 2.9557142857142855,
      "grad_norm": 1.250032663345337,
      "learning_rate": 2.9523809523809525e-06,
      "loss": 2.5089,
      "step": 20690
    },
    {
      "epoch": 2.9571428571428573,
      "grad_norm": 1.286258339881897,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 2.6316,
      "step": 20700
    },
    {
      "epoch": 2.9585714285714286,
      "grad_norm": 1.2779312133789062,
      "learning_rate": 2.761904761904762e-06,
      "loss": 2.5766,
      "step": 20710
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.9123525619506836,
      "learning_rate": 2.666666666666667e-06,
      "loss": 2.488,
      "step": 20720
    },
    {
      "epoch": 2.9614285714285713,
      "grad_norm": 1.1366311311721802,
      "learning_rate": 2.5714285714285716e-06,
      "loss": 2.5775,
      "step": 20730
    },
    {
      "epoch": 2.9628571428571426,
      "grad_norm": 1.3451827764511108,
      "learning_rate": 2.4761904761904764e-06,
      "loss": 2.7376,
      "step": 20740
    },
    {
      "epoch": 2.9642857142857144,
      "grad_norm": 1.5146384239196777,
      "learning_rate": 2.3809523809523808e-06,
      "loss": 2.6299,
      "step": 20750
    },
    {
      "epoch": 2.9657142857142857,
      "grad_norm": 1.1691197156906128,
      "learning_rate": 2.285714285714286e-06,
      "loss": 2.461,
      "step": 20760
    },
    {
      "epoch": 2.967142857142857,
      "grad_norm": 1.2291922569274902,
      "learning_rate": 2.1904761904761908e-06,
      "loss": 2.5252,
      "step": 20770
    },
    {
      "epoch": 2.9685714285714284,
      "grad_norm": 1.5231717824935913,
      "learning_rate": 2.095238095238095e-06,
      "loss": 2.447,
      "step": 20780
    },
    {
      "epoch": 2.9699999999999998,
      "grad_norm": 1.2746309041976929,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.627,
      "step": 20790
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 1.2563797235488892,
      "learning_rate": 1.9047619047619051e-06,
      "loss": 2.6343,
      "step": 20800
    },
    {
      "epoch": 2.972857142857143,
      "grad_norm": 1.0460431575775146,
      "learning_rate": 1.8095238095238095e-06,
      "loss": 2.5892,
      "step": 20810
    },
    {
      "epoch": 2.974285714285714,
      "grad_norm": 0.9264086484909058,
      "learning_rate": 1.7142857142857145e-06,
      "loss": 2.6319,
      "step": 20820
    },
    {
      "epoch": 2.975714285714286,
      "grad_norm": 1.3046633005142212,
      "learning_rate": 1.619047619047619e-06,
      "loss": 2.5727,
      "step": 20830
    },
    {
      "epoch": 2.977142857142857,
      "grad_norm": 1.2451523542404175,
      "learning_rate": 1.5238095238095238e-06,
      "loss": 2.6267,
      "step": 20840
    },
    {
      "epoch": 2.9785714285714286,
      "grad_norm": 1.149877905845642,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 2.6455,
      "step": 20850
    },
    {
      "epoch": 2.98,
      "grad_norm": 1.0842204093933105,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 2.6492,
      "step": 20860
    },
    {
      "epoch": 2.9814285714285713,
      "grad_norm": 1.0866954326629639,
      "learning_rate": 1.2380952380952382e-06,
      "loss": 2.6462,
      "step": 20870
    },
    {
      "epoch": 2.982857142857143,
      "grad_norm": 1.0962156057357788,
      "learning_rate": 1.142857142857143e-06,
      "loss": 2.3242,
      "step": 20880
    },
    {
      "epoch": 2.9842857142857144,
      "grad_norm": 1.1226634979248047,
      "learning_rate": 1.0476190476190476e-06,
      "loss": 2.6911,
      "step": 20890
    },
    {
      "epoch": 2.9857142857142858,
      "grad_norm": 1.181947946548462,
      "learning_rate": 9.523809523809526e-07,
      "loss": 2.6559,
      "step": 20900
    },
    {
      "epoch": 2.987142857142857,
      "grad_norm": 1.0433140993118286,
      "learning_rate": 8.571428571428572e-07,
      "loss": 2.7063,
      "step": 20910
    },
    {
      "epoch": 2.9885714285714284,
      "grad_norm": 1.2933894395828247,
      "learning_rate": 7.619047619047619e-07,
      "loss": 2.7741,
      "step": 20920
    },
    {
      "epoch": 2.99,
      "grad_norm": 1.1541413068771362,
      "learning_rate": 6.666666666666667e-07,
      "loss": 2.7318,
      "step": 20930
    },
    {
      "epoch": 2.9914285714285715,
      "grad_norm": 1.3281381130218506,
      "learning_rate": 5.714285714285715e-07,
      "loss": 2.523,
      "step": 20940
    },
    {
      "epoch": 2.992857142857143,
      "grad_norm": 1.1799228191375732,
      "learning_rate": 4.761904761904763e-07,
      "loss": 2.5395,
      "step": 20950
    },
    {
      "epoch": 2.994285714285714,
      "grad_norm": 1.3053970336914062,
      "learning_rate": 3.8095238095238096e-07,
      "loss": 2.5801,
      "step": 20960
    },
    {
      "epoch": 2.9957142857142856,
      "grad_norm": 1.2411102056503296,
      "learning_rate": 2.8571428571428575e-07,
      "loss": 2.7006,
      "step": 20970
    },
    {
      "epoch": 2.9971428571428573,
      "grad_norm": 0.961629331111908,
      "learning_rate": 1.9047619047619048e-07,
      "loss": 2.6916,
      "step": 20980
    },
    {
      "epoch": 2.9985714285714287,
      "grad_norm": 1.0451409816741943,
      "learning_rate": 9.523809523809524e-08,
      "loss": 2.6548,
      "step": 20990
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.1993515491485596,
      "learning_rate": 0.0,
      "loss": 2.4977,
      "step": 21000
    }
  ],
  "logging_steps": 10,
  "max_steps": 21000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4212368909631488e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
